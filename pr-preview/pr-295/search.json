[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The Data Science team at the Strategy Unit comprises the following team members:\n\nChris Beeley\nClaire Welsh\nFrancis Barton\nMatt Dray\nOzayr Mohammed\nRhian Davies\nTom Jemmett\nYiWen Hon\n\nCurrent and previous projects of note include:\n\nWork supporting the New Hospitals Programme, including building a model for predicting the demand and capacity requirements of hospitals in the future, and a tool for mapping the evidence on this topic.\nThe Patient Experience Qualitative Data Categorisation project\nWork supporting the wider analytical community, through events/communities such as NHS-R and HACA."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html",
    "title": "RStudio Tips and Tricks",
    "section": "",
    "text": "In a recent Coffee & Coding session we chatted about tips and tricks for RStudio, the popular and free Integrated Development Environment (IDE) that many Strategy Unit analysts use to write R code.\nRStudio has lots of neat features but many are tucked away in submenus. This session was a chance for the community to uncover and discuss some hidden gems to make our work easier and faster."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#coffee-coding",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#coffee-coding",
    "title": "RStudio Tips and Tricks",
    "section": "",
    "text": "In a recent Coffee & Coding session we chatted about tips and tricks for RStudio, the popular and free Integrated Development Environment (IDE) that many Strategy Unit analysts use to write R code.\nRStudio has lots of neat features but many are tucked away in submenus. This session was a chance for the community to uncover and discuss some hidden gems to make our work easier and faster."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#official-guidance",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#official-guidance",
    "title": "RStudio Tips and Tricks",
    "section": "Official guidance",
    "text": "Official guidance\nPosit is the company who build and maintain RStudio. They host a number of cheatsheets on their website, including one for RStudio. They also have a more in-depth user guide."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#command-palette",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#command-palette",
    "title": "RStudio Tips and Tricks",
    "section": "Command palette",
    "text": "Command palette\nRStudio has a powerful built-in Command Palette, which is a special search box that gives instant access to features and settings without needing to find them in the menus. Many of the tips and tricks we discussed can be found by searching in the Palette. Open it with the keyboard shortcut Ctrl + Shift + P.\n\n\n\nOpening the Command Palette.\n\n\nFor example, let‚Äôs say you forgot how to restart R. If you open the Command Palette and start typing ‚Äòrestart‚Äô, you‚Äôll see the option ‚ÄòRestart R Session‚Äô. Clicking it will do exactly that. Handily, the Palette also displays the keyboard shortcut (Control + Shift + F10 on Windows) as a reminder.\nAs for settings, a search for ‚Äòrainbow‚Äô in the Command Palette will find ‚ÄòUse rainbow parentheses‚Äô, an option to help prevent bracket-mismatch errors by colouring pairs of parentheses. What‚Äôs nice is that the checkbox to toggle the feature appears right there in the palette so you can change it immediately.\nI refer to menu paths and keyboard shortcuts in the rest of this post, but bear in mind that you can use the Command Palette instead."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#options",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#options",
    "title": "RStudio Tips and Tricks",
    "section": "Options",
    "text": "Options\nIn general, most settings can be found under Tools &gt; Global Options‚Ä¶ and many of these are discussed in the rest of this post.\n\n\n\nAdjusting workspace and history settings.\n\n\nBut there‚Äôs a few settings in particular that we recommend you change to help maximise reproducibility and reduce the chance of confusion. Under General &gt; Basic, uncheck ‚ÄòRestore .Rdata into workspace at startup‚Äô and select ‚ÄòNever‚Äô from the dropdown options next to ‚ÄòSave workspace to .Rdata on exit‚Äô. These options mean you start with the ‚Äòblank slate‚Äô of an empty environment when you open a project, allowing you to rebuild objects from scratch1."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#keyboard-shortcuts",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#keyboard-shortcuts",
    "title": "RStudio Tips and Tricks",
    "section": "Keyboard shortcuts",
    "text": "Keyboard shortcuts\nYou can speed up day-to-day coding with keyboard shortcuts instead of clicking buttons in the interface.\nYou can see some available shortcuts in RStudio if you navigate to Help &gt; Keyboard Shortcuts Help, or use the shortcut Alt + Shift + K (how meta). You can go to Help &gt; Modify Keyboard Shortcuts‚Ä¶ to search all shortcuts and change them to what you prefer2.\nWe discussed a number of handy shortcuts that we use frequently3. You can:\n\nre-indent lines to the appropriate depth with Control + I\nreformat code with Control + Shift + A\nturn one or more lines into a comment with Control + Shift + C\ninsert the pipe operator (%&gt;% or |&gt;4) with Control + Shift + M5\ninsert the assignment arrow (&lt;-) with Alt + - (hyphen)\nhighlight a function in the script or console and press F1 to open the function documentation in the ‚ÄòHelp‚Äô pane\nuse ‚ÄòFind in Files‚Äô to search for a particular variable, function or string across all the files in your project, with Control + Shift + F"
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#themes",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#themes",
    "title": "RStudio Tips and Tricks",
    "section": "Themes",
    "text": "Themes\nYou can change a number of settings to alter RStudio‚Äôs theme, colours and fonts to whatever you desire.\nYou can change the default theme in Tools &gt; Global Options‚Ä¶ &gt; Appearance &gt; Editor theme and select one from the pre-installed list. You can upload new themes by clicking the ‚ÄòAdd‚Äô button and selecting a theme from your computer. They typically have the file extension .rsthemes and can be downloaded from the web, or you can create or tweak one yourself. The {rsthemes} package has a number of options and also allows you to switch between themes and automatically switch between light and dark themes depending on the time of day.\n\n\n\nCustomising the appearance and font.\n\n\nIn the same ‚ÄòAppearance‚Äô submenu as the theme settings, you can find an option to change fonts. Monospace fonts, ones where each character takes up the same width, will appear here automatically if you‚Äôve installed them on your computer. One popular font for coding is Fira Code, which has the special property of converting certain sets of characters into ‚Äòligatures‚Äô, which some people find easier to read. For example, the base pipe will appear as a rightward-pointing arrow rather than its constituent vertical-pipe and greater-than symbol (|&gt;)."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#panes",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#panes",
    "title": "RStudio Tips and Tricks",
    "section": "Panes",
    "text": "Panes\n\nLayout\nThe structural layout of RStudio‚Äôs panes can be adjusted. One simple thing you can do is minimise and maximise each pane by clicking the window icons in their upper-right corners. This is useful when you want more screen real-estate for a particular pane.\nYou can move pane loations too. Click the ‚ÄòWorkspace Panes‚Äô button (a square with four more inside it) at the top of the IDE to see a number of settings. For example, you can select ‚ÄòConsole on the right‚Äô to move the R console to the upper-right pane, which you may prefer for maximimsing the vertical space in which code is shown. You could also click Pane Layout‚Ä¶ in this menu to be taken to Tools &gt; Global Options‚Ä¶ &gt; Pane layout, where you can click ‚ÄòAdd Column‚Äô to insert new script panes that allow you to inspect and write multiple files side-by-side.\n\n\nScript navigation\nThe script pane in particular has a nice feature for navigating through sections of your script or Quarto/R Markdown files. Click the ‚ÄòShow Document Outline‚Äô button or use the keyboard shortcut Control + Shift + O to slide open a tray that provides a nice indented list of all the sections and function defintions in your file.\nSection headers are auto-detected in a Quarto or R Markdown document wherever the Markdown header markup has been used: one hashmark (#) for a level 1 header, two for level 2, and so on. To add section headers to an R Script, add at least four hyphens after a commented line that starts with #. Use two or more hashes at the start of the comment to increase the nestedness of that section.\n\n# Header ------------------------------------------------------------------\n\n## Section ----\n\n### Subsection ----\n\nNote that Ctrl + Shift + R will open a dialog box for you to input the name of a section header, which will be inserted and automatically padded to 75 characters to provide a strong visual cue between sections.\nAs well as the document outline, there‚Äôs also a reminder in the lower-left of the script pane that gives the name of the section that your cursor is currently in. A symbol is also shown: a hashmark means it‚Äôs a headed section and an ‚Äòf‚Äô means it‚Äôs a function definition. You can click this to jump to other sections.\n\n\n\nNavigating with headers in the R script pane.\n\n\n\n\nBackground jobs\nPerhaps an under-used pane is ‚ÄòBackground jobs‚Äô. This is where you can run a separate R process that keeps your R console free. Go to Tools &gt; Background Jobs &gt; Start Background Job‚Ä¶ to expose this tab if it isn‚Äôt already listed alongside the R console.\nWhy might you want to do this? As I write this post, there‚Äôs a background process to detect changes to the Quarto document that I‚Äôm writing and then update a preview I have running in the browser. You can do something similar for Shiny apps. You can continue to develop your app and test things in the console and the app preview will update on save. You won‚Äôt need to keep hitting the ‚ÄòRender‚Äô or ‚ÄòRun app‚Äô button every time you make a change."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#magic-wand",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#magic-wand",
    "title": "RStudio Tips and Tricks",
    "section": "Magic wand",
    "text": "Magic wand\nThere‚Äôs a miscellany of useful tools available when you click the ‚Äòmagic wand‚Äô button in the script pane.\n\n\n\nAbracadabra! Casting open the ‚Äòmagic wand‚Äô menu.\n\n\nThis includes:\n\n‚ÄòRename in Scope‚Äô, which is like find-and-replace but you only change instances with the same ‚Äòscope‚Äô, so you could select the variable x, go to Rename in Scope and then you can edit all instances of the variable in the document and change them at the same time (e.g.¬†to rename them)\n‚ÄòReflow Comment‚Äô, which you can click after higlighting a comments block to have the comments automatically line-break at the maximum width\n‚ÄòInsert Roxygen Skeleton‚Äô, which you can click when your cursor is inside the body of a function you‚Äôve written and a {roxygen2} documentation template will be added above your function with the @params argument names pre-filled\n\nAlong with ‚ÄòComment/Uncomment Lines‚Äô, ‚ÄòReindent Lines‚Äô and ‚ÄòReformat Lines‚Äô, mentioned above in the keyboard shortcuts section."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#wrapping-up",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#wrapping-up",
    "title": "RStudio Tips and Tricks",
    "section": "Wrapping up",
    "text": "Wrapping up\nTime was limited in our discussion. There are so many more tips and tricks that we didn‚Äôt get to. Let us know what we missed, or what your favourite shortcuts and settings are."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#footnotes",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#footnotes",
    "title": "RStudio Tips and Tricks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor the same reason it‚Äôs a good idea to restart R on a frequent basis. You may assume that an object x in your environment was made in a certain way and contains certain information, but does it? What if you overwrote it at some point and forgot? Best to wipe the slate clean and rebuild it from scratch. Jenny Bryan has written an explainer.‚Ü©Ô∏é\nYou can ‚Äòsnap focus‚Äô to the script and console panes with the pre-existing shortcuts Control + 1 and Control + 2. My next most-used pane is the terminal, so I‚Äôve re-mapped the shortcut to Control + 3.‚Ü©Ô∏é\nThe classic shortcuts of select-all (Control + A), cut (Control + X), copy Control + C, paste (Control + V), undo (Control + Z) and redo (Control + Shift + Z) are all available when editing.‚Ü©Ô∏é\nNote that you can set the default pipe to the base-R version (|&gt;) by checking the box at Tools &gt; Global Options‚Ä¶ &gt; Code &gt; Use native pipe operator‚Ü©Ô∏é\nProbably ‚ÄòM‚Äô for {magrittr}, the name of the package that contains the %&gt;% incarnation of the operator.‚Ü©Ô∏é"
  },
  {
    "objectID": "blogs/posts/2023-04-26_alternative_remotes/index.html",
    "href": "blogs/posts/2023-04-26_alternative_remotes/index.html",
    "title": "Alternative remote repositories",
    "section": "",
    "text": "It‚Äôs great when someone send‚Äôs you a pull request on GitHub to fix bugs or add new features to your project, but you probably always want to check the other persons work in someway before merging that pull request.\nAll of the steps below are intended to be entered via a terminal.\nLet‚Äôs imagine that we have a GitHub account called example and a repository called test, and we use https rather than ssh.\n$ git remote get-url origin\n# https://github.com/example/test.git\nNow, let‚Äôs say we have someone who has submitted a Pull Request (PR), and their username is friend. We can add a new remote for their fork with\n$ git remote add friend https://github.com/friend/test.git\nHere, I name the remote exactly as per the persons GitHub username for no other reason than making it easier to track things later on. You could name this remote whatever you like, but you will need to make sure that the remote url matches their repository correctly.\nWe are now able to checkout their remote branch. First, we will want to fetch their work:\n# make sure to replace the remote name to what you set it to before\n$ git fetch friend\nNow, hopefully they have commited to a branch with a name that you haven‚Äôt used. Let‚Äôs say they created a branch called my_work. You can then simply run\n$ git switch friend/my_work\nThis should checkout the my_work branch locally for you.\nNow, if they have happened to use a branch name that you are already using, or more likely, directly commited to their own main branch, you will need to do checkout to a new branch:\n# replace friend as above to be the name of the remote, and main to be the branch\n# that they have used\n# replace their_work with whatever you want to call this branch locally\n$ git checkout friend/main -b their_work\nYou are now ready to run their code and check everything is good to merge!\nFinally, If you want to clean up your local repository you can remove the new branch that you checked out and the new remote with the following steps:\n# switch back to one of your branches, e.g. main\n$ git checkout main\n\n# then remove the branch that you created above\n$ git branch -D their_work\n\n# you can remove the remote\n$ git remote remove friend"
  },
  {
    "objectID": "blogs/posts/2024-05-22_storing-data-safely/index.html",
    "href": "blogs/posts/2024-05-22_storing-data-safely/index.html",
    "title": "Storing data safely",
    "section": "",
    "text": "Note\n\n\n\nUPDATED: Please see the Addendum to this blog, added 2025-04-03 regarding accessing data from SharePoint"
  },
  {
    "objectID": "blogs/posts/2024-05-22_storing-data-safely/index.html#coffee-coding",
    "href": "blogs/posts/2024-05-22_storing-data-safely/index.html#coffee-coding",
    "title": "Storing data safely",
    "section": "Coffee & Coding",
    "text": "Coffee & Coding\nIn a recent Coffee & Coding session we chatted about storing data safely for use in Reproducible Analytical Pipelines (RAP), and the slides from the presentation are now available. We discussed the use of Posit Connect Pins and Azure Storage.\nIn order to avoid duplication, this blog post will not cover the pros and cons of each approach, and will instead focus on documenting the code that was used in our live demonstrations. I would recommend that you look through the slides before using the code in this blogpost and have them alongside, as they provide lots of useful context!"
  },
  {
    "objectID": "blogs/posts/2024-05-22_storing-data-safely/index.html#posit-connect-pins",
    "href": "blogs/posts/2024-05-22_storing-data-safely/index.html#posit-connect-pins",
    "title": "Storing data safely",
    "section": "Posit Connect Pins",
    "text": "Posit Connect Pins\n\n# A brief intro to using {pins} to store, version, share and protect a dataset\n# on Posit Connect. Documentation: https://pins.rstudio.com/\n\n\n# Setup -------------------------------------------------------------------\n\n\ninstall.packages(c(\"pins\", \"dplyr\")) # if not yet installed\n\nsuppressPackageStartupMessages({\n  library(pins)\n  library(dplyr) # for wrangling and the 'starwars' demo dataset\n})\n\nboard &lt;- board_connect() # will error if you haven't authenticated before\n# Error in `check_auth()`: ! auth = `auto` has failed to find a way to authenticate:\n# ‚Ä¢ `server` and `key` not provided for `auth = 'manual'`\n# ‚Ä¢ Can't find CONNECT_SERVER and CONNECT_API_KEY envvars for `auth = 'envvar'`\n# ‚Ä¢ rsconnect package not installed for `auth = 'rsconnect'`\n# Run `rlang::last_trace()` to see where the error occurred.\n\n# To authenticate\n# In RStudio: Tools &gt; Global Options &gt; Publishing &gt; Connect... &gt; Posit Connect\n# public URL of the Strategy Unit Posit Connect Server: connect.strategyunitwm.nhs.uk\n# Your browser will open to the Posit Connect web page and you're prompted to\n# for your password. Enter it and you'll be authenticated.\n\n# Once authenticated\nboard &lt;- board_connect()\n# Connecting to Posit Connect 2024.03.0 at\n# &lt;https://connect.strategyunitwm.nhs.uk&gt;\n\nboard |&gt; pin_list() # see all the pins on that board\n\n\n# Create a pin ------------------------------------------------------------\n\n\n# Write a dataset to the board as a pin\nboard |&gt; pin_write(\n  x = starwars,\n  name = \"starwars_demo\"\n)\n# Guessing `type = 'rds'`\n# Writing to pin 'matt.dray/starwars_demo'\n\nboard |&gt; pin_exists(\"starwars_demo\")\n# ! Use a fully specified name including user name: \"matt.dray/starwars_demo\",\n# not \"starwars_demo\".\n# [1] TRUE\n\npin_name &lt;- \"matt.dray/starwars_demo\"\n\nboard |&gt; pin_exists(pin_name) # logical, TRUE/FALSE\nboard |&gt; pin_meta(pin_name) # metadata, see also 'metadata' arg in pin_write()\nboard |&gt; pin_browse(pin_name) # view the pin in the browser\n\n\n# Permissions -------------------------------------------------------------\n\n\n# You can let people see and edit a pin. Log into Posit Connect and select the\n# pin under 'Content'. In the 'Settings' panel on the right-hand side, adjust\n# the 'sharing' options in the 'Access' tab.\n\n\n# Overwrite and version ---------------------------------------------------\n\n\nstarwars_droids &lt;- starwars |&gt;\n  filter(species == \"Droid\") # beep boop\n\nboard |&gt; pin_write(\n  starwars_droids,\n  pin_name,\n  type = \"rds\"\n)\n# Writing to pin 'matt.dray/starwars_demo'\n\nboard |&gt; pin_versions(pin_name) # see version history\nboard |&gt; pin_versions_prune(pin_name, n = 1) # remove history\nboard |&gt; pin_versions(pin_name)\n\n# What if you try to overwrite the data but it hasn't changed?\nboard |&gt; pin_write(\n  starwars_droids,\n  pin_name,\n  type = \"rds\"\n)\n# ! The hash of pin \"matt.dray/starwars_demo\" has not changed.\n# ‚Ä¢ Your pin will not be stored.\n\n\n# Use the pin -------------------------------------------------------------\n\n\n# You can read a pin to your local machine, or access it from a Quarto file\n# or Shiny app hosted on Connect, for example. If the output and the pin are\n# both on Connect, no authentication is required; the board is defaulted to\n# the Posit Connect instance where they're both hosted.\n\nboard |&gt;\n  pin_read(pin_name) |&gt; # like you would use e.g. read_csv\n  with(data = _, plot(mass, height)) # wow!\n\n\n# Delete pin --------------------------------------------------------------\n\n\nboard |&gt; pin_exists(pin_name) # logical, good function for error handling\nboard |&gt; pin_delete(pin_name)\nboard |&gt; pin_exists(pin_name)"
  },
  {
    "objectID": "blogs/posts/2024-05-22_storing-data-safely/index.html#azure-storage-in-r",
    "href": "blogs/posts/2024-05-22_storing-data-safely/index.html#azure-storage-in-r",
    "title": "Storing data safely",
    "section": "Azure Storage in R",
    "text": "Azure Storage in R\nYou will need an .Renviron file with the four environment variables listed below for the code to work. This .Renviron file should be ignored by git. You can share the contents of .Renviron files with other team members via Teams, email, or Sharepoint.\nBelow is a sample .Renviron file\nAZ_STORAGE_EP=https://STORAGEACCOUNT.blob.core.windows.net/\nAZ_STORAGE_CONTAINER=container-name\nAZ_TENANT_ID=long-sequence-of-numbers-and-letters\nAZ_APP_ID=another-long-sequence-of-numbers-and-letters\n\ninstall.packages(c(\"AzureAuth\", \"AzureStor\", \"arrow\")) # if not yet installed\n\n# Load all environment variables\nep_uri &lt;- Sys.getenv(\"AZ_STORAGE_EP\")\napp_id &lt;- Sys.getenv(\"AZ_APP_ID\")\ncontainer_name &lt;- Sys.getenv(\"AZ_STORAGE_CONTAINER\")\ntenant &lt;- Sys.getenv(\"AZ_TENANT_ID\")\n\n# Authenticate\ntoken &lt;- AzureAuth::get_azure_token(\n  \"https://storage.azure.com\",\n  tenant = tenant,\n  app = app_id,\n  auth_type = \"device_code\",\n)\n\n# If you have not authenticated before, you will be taken to an external page to\n# authenticate!Use your mlcsu.nhs.uk account.\n\n# Connect to container\nendpoint &lt;- AzureStor::blob_endpoint(ep_uri, token = token)\ncontainer &lt;- AzureStor::storage_container(endpoint, container_name)\n\n# List files in container\nblob_list &lt;- AzureStor::list_blobs(container)\n\n# If you get a 403 error when trying to interact with the container, you may\n# have to clear your Azure token and re-authenticate using a different browser.\n# Use AzureAuth::clean_token_directory() to clear your token, then repeat the\n# AzureAuth::get_azure_token() step above.\n\n# Upload specific file to container\nAzureStor::storage_upload(container, \"data/ronald.jpeg\", \"newdir/ronald.jpeg\")\n\n# Upload contents of a local directory to container\nAzureStor::storage_multiupload(container, \"data/*\", \"newdir\")\n\n# Check files have uploaded\nblob_list &lt;- AzureStor::list_blobs(container)\n\n# Load file directly from Azure container\ndf_from_azure &lt;- AzureStor::storage_read_csv(\n  container,\n  \"newdir/cats.csv\",\n  show_col_types = FALSE\n)\n\n# Load file directly from Azure container (by temporarily downloading file\n# and storing it in memory)\nparquet_in_memory &lt;- AzureStor::storage_download(\n  container,\n  src = \"newdir/cats.parquet\", dest = NULL\n)\nparq_df &lt;- arrow::read_parquet(parquet_in_memory)\n\n# Delete from Azure container (!!!)\nfor (blobfile in blob_list$name) {\n  AzureStor::delete_storage_file(container, blobfile)\n}"
  },
  {
    "objectID": "blogs/posts/2024-05-22_storing-data-safely/index.html#azure-storage-in-python",
    "href": "blogs/posts/2024-05-22_storing-data-safely/index.html#azure-storage-in-python",
    "title": "Storing data safely",
    "section": "Azure Storage in Python",
    "text": "Azure Storage in Python\nThis will use the same environment variables as the R version, just stored in a .env file instead.\nWe didn‚Äôt cover this in the presentation, so it‚Äôs not in the slides, but the code should be self-explanatory.\n\n\nimport os\nimport io\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom azure.identity import DefaultAzureCredential\nfrom azure.storage.blob import ContainerClient\n\n\n# Load all environment variables\nload_dotenv()\naccount_url = os.getenv('AZ_STORAGE_EP')\ncontainer_name = os.getenv('AZ_STORAGE_CONTAINER')\n\n\n# Authenticate\ndefault_credential = DefaultAzureCredential()\n\nFor the first time, you might need to authenticate via the Azure CLI\nDownload it from https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-windows?tabs=azure-cli\nInstall then run az login in your terminal. Once you have logged in with your browser try the DefaultAzureCredential() again!\n\n# Connect to container\ncontainer_client = ContainerClient(account_url, container_name, default_credential)\n\n\n# List files in container - should be empty\nblob_list = container_client.list_blob_names()\nfor blob in blob_list:\n    if blob.startswith('newdir'):\n        print(blob)\n\nnewdir/cats.parquet\nnewdir/ronald.jpeg\n\n\n\n# Upload file to container\nwith open(file='data/cats.csv', mode=\"rb\") as data:\n    blob_client = container_client.upload_blob(name='newdir/cats.csv', \n                                               data=data, \n                                               overwrite=True)\n\n\n# # Check files have uploaded - List files in container again\nblob_list = container_client.list_blobs()\nfor blob in blob_list:\n    if blob['name'].startswith('newdir'):\n        print(blob['name'])\n\nnewdir/cats.csv\nnewdir/cats.parquet\nnewdir/ronald.jpeg\n\n\n\n# Download file from Azure container to temporary filepath\n\n# Connect to blob\nblob_client = container_client.get_blob_client('newdir/cats.csv')\n\n# Write to local file from blob\ntemp_filepath = os.path.join('temp_data', 'cats.csv')\nwith open(file=temp_filepath, mode=\"wb\") as sample_blob:\n    download_stream = blob_client.download_blob()\n    sample_blob.write(download_stream.readall())\ncat_data = pd.read_csv(temp_filepath)\ncat_data.head()\n\n\n\n\n\n\n\n\nName\nPhysical_characteristics\nBehaviour\n\n\n\n\n0\nRonald\nWhite and ginger\nLazy and greedy but undoubtedly cutest and best\n\n\n1\nKaspie\nSmall calico\nSweet and very shy but adventurous\n\n\n2\nHennimore\nPale orange\nUnhinged and always in a state of panic\n\n\n3\nThug cat\nBlack and white - very large\nLocal bully\n\n\n4\nSon of Stripey\nGrey tabby\nVery vocal\n\n\n\n\n\n\n\n\n# Load directly from Azure - no local copy\n\ndownload_stream = blob_client.download_blob()\nstream_object = io.BytesIO(download_stream.readall())\ncat_data = pd.read_csv(stream_object)\ncat_data\n\n\n\n\n\n\n\n\nName\nPhysical_characteristics\nBehaviour\n\n\n\n\n0\nRonald\nWhite and ginger\nLazy and greedy but undoubtedly cutest and best\n\n\n1\nKaspie\nSmall calico\nSweet and very shy but adventurous\n\n\n2\nHennimore\nPale orange\nUnhinged and always in a state of panic\n\n\n3\nThug cat\nBlack and white - very large\nLocal bully\n\n\n4\nSon of Stripey\nGrey tabby\nVery vocal\n\n\n\n\n\n\n\n\n# !!!!!!!!! Delete from Azure container !!!!!!!!!\nblob_client = container_client.get_blob_client('newdir/cats.csv')\nblob_client.delete_blob()\n\n\nblob_list = container_client.list_blobs()\nfor blob in blob_list:\n    if blob['name'].startswith('newdir'):\n        print(blob['name'])\n\nnewdir/cats.parquet\nnewdir/ronald.jpeg"
  },
  {
    "objectID": "blogs/posts/2024-05-22_storing-data-safely/index.html#addendum-accessing-data-from-sharepoint",
    "href": "blogs/posts/2024-05-22_storing-data-safely/index.html#addendum-accessing-data-from-sharepoint",
    "title": "Storing data safely",
    "section": "ADDENDUM: Accessing data from SharePoint",
    "text": "ADDENDUM: Accessing data from SharePoint\nSharePoint is a Microsoft product, which is a content/knowledge management tool. Many teams across the NHS use SharePoint for all sorts of file types that need to be preserved or shared within and between teams, but also need to be kept secure.\nAccessing SharePoint requires user authentication, which you‚Äôll be prompted for in the browser when you try to access SharePoint from R. Note that you must ‚Äòfollow‚Äô a SharePoint site before you can fetch its content.\nTo access data on SharePoint, follow these steps:\n\nNavigate to the SharePoint page that has the file of interest in it, using your browser.\nClick the small star in the top right corner of the window, labelled ‚ÄòFollow‚Äô.\n\n\n\nOpen (or create) your project‚Äôs .Renviron file, used for storing environmental variables. You can do this by running usethis::edit_r_environ() from the R console.\nSave two new environment variables to the .Renviron file:\n\nSP_SITE_NAME: the name of the site (i.e.¬†the name at the top of the browser screen)\nSP_FILE_PATH: the full file path from below the Documents/ folder to the file itself, including the file type extension.\n\nSave and close the .Renviron file then either restart your R session (CTRL + Shift + F10 on Windows machines) or run readRenviron(\".Renviron\") to make the new variables available in your session.\nUse the code below to read in an Excel file (xlsx) into R memory, or adapt it to read other file types.\n\n\n# Read in the securely saved path and site variables\nsharepoint_site &lt;- Sys.getenv(\"SP_SITE_NAME\")\ntemplate_path &lt;- Sys.getenv(\"SP_FILE_PATH\")\n\n# access the dataset and save it into a temporary file\nsite &lt;- Microsoft365R::get_sharepoint_site(sharepoint_site)\ndrv &lt;- site$get_drive()\ntmp_file &lt;- tempfile(fileext = \".xlsx\")\ndrv$download_file(template_path, dest = tmp_file)\n\ndata &lt;- readxl::read_xlsx(tmp_file)\n\n#tidy up\nunlink(tmp_file)"
  },
  {
    "objectID": "blogs/posts/2023-03-24_hotfix-with-git/index.html",
    "href": "blogs/posts/2023-03-24_hotfix-with-git/index.html",
    "title": "Creating a hotfix with git",
    "section": "",
    "text": "I recently discovered a bug in a code-base which needed to be fixed and deployed back to production A.S.A.P., but since the last release the code has moved on significantly. The history looks something a bit like:\nThat is, we have a tag which is the code that is currently in production (which we need to patch), a number of commits after that tag to main (which were separate branches merged via pull requests), and a current development branch.\nI need to somehow: 1. go back to the tagged release, 2. check that code out, 3. patch that code, 4. commit this change, but insert the commit before all of the new commits after the tag\nThere are at least two ways that I know to do this, one would be with an interactive rebase, but I used a slightly longer method, but one I feel is a little less likely to get wrong.\nBelow are the step‚Äôs that I took. One thing I should note is this worked well for my particular issue because the change didn‚Äôt cause any merge conflicts later on."
  },
  {
    "objectID": "blogs/posts/2023-03-24_hotfix-with-git/index.html#fixing-my-codebase",
    "href": "blogs/posts/2023-03-24_hotfix-with-git/index.html#fixing-my-codebase",
    "title": "Creating a hotfix with git",
    "section": "Fixing my codebase",
    "text": "Fixing my codebase\nFirst, we need to checkout the tag\ngit checkout -b hotfix v0.2.0\nThis creates a new branch called hotfix off of the tag v0.2.0.\nNow that I have the code base checked out at the point I need to fix, I can make the change that is needed, and commit the change\ngit add [FILENAME]\ngit commit -m \"fixes the code\"\n(Obviously, I used the actual file name and gave a better commit message. I Promise üòù)\nNow my code is fixed, I create a new tag for this ‚Äúrelease‚Äù, as well as push the code to production (this step is omitted here)\ngit tag v0.2.1 -m \"version 0.2.0\"\nAt this point, our history looks something like\n\n\n\n\n\n\n\n\n\nWhat we want to do is break the link between main and v0.2.0, instead attaching tov0.2.1. First though, I want to make sure that if I make a mistake, I‚Äôm not making it on the main branch.\ngit checkout main\ngit checkout -b apply-hotfix\nThen we can fix our history using the rebase command\ngit rebase hotfix\nWhat this does is it rolls back to the point where the branch that we are rebasing (apply-hotfix) and the hotfix branch both share a common commit (v0.2.0 tag). It then applies the commits in the hotfix branch, before reapplying the commits from apply-hotfix (a.k.a. the main branch).\nOne thing to note, if you have any merge conflicts created by your fix, then the rebase will stop and ask you to fix the merge conflicts. There is some information in the GitHub doc‚Äôs for [resolving merge conflicts after a Git rebase][2].\n[2]: https://docs.github.com/en/get-started/using-git/resolving-merge-conflicts-after-a-git-rebase\nAt this point, we can check that the commit history looks correct\ngit log v0.2.0..HEAD\nIf we are happy, then we can apply this to the main branch. I do this by renaming the apply-hotfix branch as main. First, you have to delete the main branch to allow us to rename the branch.\ngit branch -D main\ngit branch -m main\nWe also need to update the other branches to use the new main branch\ngit checkout branch\ngit rebase main\nNow, we should have a history like"
  },
  {
    "objectID": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html",
    "href": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html",
    "title": "Using GitHub to plan and organise Coffee & Coding",
    "section": "",
    "text": "Coffee & Coding is a fortnightly hour-long session organised by the Data Science team, open to all members of the Strategy Unit with an interest in coding. It‚Äôs been well received and is a valued source of professional development and general geekery in the team.\nWe‚Äôve been experimenting with using GitHub as an organisational tool for our team‚Äôs work, and are testing the same approach for Coffee & Coding sessions as well. Previously, future Coffee & Coding sessions were haphazardly listed in a Google Doc that was only accessible to members of the Data Science team, and we wanted a more open approach. We also didn‚Äôt have a good record of topics that were previously covered.\nYou‚Äôll need a GitHub account to enjoy the full functionality of the planner. If you need help setting this up, get in touch with any member of the Data Science team.\nAny feedback on this new system for organising and planning Coffee & Coding is very welcome! Hope you enjoy using it."
  },
  {
    "objectID": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html#coffee-coding",
    "href": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html#coffee-coding",
    "title": "Using GitHub to plan and organise Coffee & Coding",
    "section": "",
    "text": "Coffee & Coding is a fortnightly hour-long session organised by the Data Science team, open to all members of the Strategy Unit with an interest in coding. It‚Äôs been well received and is a valued source of professional development and general geekery in the team.\nWe‚Äôve been experimenting with using GitHub as an organisational tool for our team‚Äôs work, and are testing the same approach for Coffee & Coding sessions as well. Previously, future Coffee & Coding sessions were haphazardly listed in a Google Doc that was only accessible to members of the Data Science team, and we wanted a more open approach. We also didn‚Äôt have a good record of topics that were previously covered.\nYou‚Äôll need a GitHub account to enjoy the full functionality of the planner. If you need help setting this up, get in touch with any member of the Data Science team.\nAny feedback on this new system for organising and planning Coffee & Coding is very welcome! Hope you enjoy using it."
  },
  {
    "objectID": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html#viewing-upcoming-sessions",
    "href": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html#viewing-upcoming-sessions",
    "title": "Using GitHub to plan and organise Coffee & Coding",
    "section": "Viewing upcoming sessions",
    "text": "Viewing upcoming sessions\nWe have created a fully open GitHub project for tracking Coffee & Coding sessions. Any sessions with scheduled dates can be seen in the ‚ÄúUpcoming sessions‚Äù view. Clicking on a session title brings up more information, including a brief overview of the session and the people running it. Users with GitHub accounts can make comments or post emoji reactions.\n\n\n\nViewing upcoming session details"
  },
  {
    "objectID": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html#adding-session-ideas",
    "href": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html#adding-session-ideas",
    "title": "Using GitHub to plan and organise Coffee & Coding",
    "section": "Adding session ideas",
    "text": "Adding session ideas\nTo add a session idea:\n\nCreate a new issue on the data_science repository. Provide a useful title and description for the session.\nGive your new issue the label C&C‚òï\nIf you would like to run or contribute to the session, assign yourself to it.\nClick ‚ÄúCreate‚Äù to save your session idea as a GitHub issue. You should then be able to see it listed as a ‚ÄúPotential session‚Äù on the planner, and others will be able to view, vote for, and comment on your session idea.\n\n\n\n\nAdding a session idea"
  },
  {
    "objectID": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html#voting-for-session-ideas",
    "href": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html#voting-for-session-ideas",
    "title": "Using GitHub to plan and organise Coffee & Coding",
    "section": "Voting for session ideas",
    "text": "Voting for session ideas\nWe will use thumbs up (üëç) emoji reactions to suggested sessions as a voting system to help us with planning and scheduling.\nIf you see any potential sessions that you are interested in, react to them with a thumbs up emoji. You can see all planned sessions, in order of votes received, listed here.\n\n\n\nVoting for a session idea"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "This post is comprised of the Jupyter notebook that was used during a Coffee & Coding session providing an overview of text vectorization, a key concept in Natural Language Processing.\nLet‚Äôs take as our first example a dataset of reviews from IMDB. The aim is to try and classify if the review had a positive or negative score, based on the words in the text.\n\nimport pandas as pd\npd.set_option('display.max_colwidth', 400)\n# Dataset from  https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download\ndata = pd.read_csv('IMDB Dataset.csv')\ndata\n\n\n\n\n\n\n\n\nreview\nsentiment\n\n\n\n\n0\nOne of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.&lt;br /&gt;&lt;br /&gt;The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regard...\npositive\n\n\n1\nA wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. &lt;br /&gt;&lt;br /&gt;The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the ref...\npositive\n\n\n2\nI thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof...\npositive\n\n\n3\nBasically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.&lt;br /&gt;&lt;br /&gt;This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.&lt;br /&gt;&lt;br /&gt;OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Paren...\nnegative\n\n\n4\nPetter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. &lt;br /&gt;&lt;br /&gt;This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action t...\npositive\n\n\n...\n...\n...\n\n\n49995\nI thought this movie did a down right good job. It wasn't as creative or original as the first, but who was expecting it to be. It was a whole lotta fun. the more i think about it the more i like it, and when it comes out on DVD I'm going to pay the money for it very proudly, every last cent. Sharon Stone is great, she always is, even if her movie is horrible(Catwoman), but this movie isn't, t...\npositive\n\n\n49996\nBad plot, bad dialogue, bad acting, idiotic directing, the annoying porn groove soundtrack that ran continually over the overacted script, and a crappy copy of the VHS cannot be redeemed by consuming liquor. Trust me, because I stuck this turkey out to the end. It was so pathetically bad all over that I had to figure it was a fourth-rate spoof of Springtime for Hitler.&lt;br /&gt;&lt;br /&gt;The girl who ...\nnegative\n\n\n49997\nI am a Catholic taught in parochial elementary schools by nuns, taught by Jesuit priests in high school & college. I am still a practicing Catholic but would not be considered a \"good Catholic\" in the church's eyes because I don't believe certain things or act certain ways just because the church tells me to.&lt;br /&gt;&lt;br /&gt;So back to the movie...its bad because two people are killed by this nun w...\nnegative\n\n\n49998\nI'm going to have to disagree with the previous comment and side with Maltin on this one. This is a second rate, excessively vicious Western that creaks and groans trying to put across its central theme of the Wild West being tamed and kicked aside by the steady march of time. It would like to be in the tradition of \"Butch Cassidy and the Sundance Kid\", but lacks that film's poignancy and char...\nnegative\n\n\n49999\nNo one expects the Star Trek movies to be high art, but the fans do expect a movie that is as good as some of the best episodes. Unfortunately, this movie had a muddled, implausible plot that just left me cringing - this is by far the worst of the nine (so far) movies. Even the chance to watch the well known characters interact in another movie can't save this movie - including the goofy scene...\nnegative\n\n\n\n\n50000 rows √ó 2 columns\n\n\n\n\n# Our dataset is quite balanced, with an equal number of positive and negative reviews.\ndata['sentiment'].value_counts()\n\nsentiment\npositive    25000\nnegative    25000\nName: count, dtype: int64\n\n\n\n# In this cell, we are trying to use a very basic machine learning model (Multinomial Naive Bayes) \n# to predict the sentiment of the text (whether it was positive or negative) based on the text.\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_validate\n\nnaivebayes = MultinomialNB()\n\nX = data.review\n\ncv_nb = cross_validate(\n    naivebayes,\n    X,\n    data.sentiment,\n    scoring = \"accuracy\"\n)\n\nround(cv_nb['test_score'].mean(),2)\n\n# ‚ö†Ô∏è Uh oh!! we're getting an error... let's decode it together\n# ValueError: could not convert string to float (it doesn't like the text being as a string!)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[3], line 11\n      7 naivebayes = MultinomialNB()\n      9 X = data.review\n---&gt; 11 cv_nb = cross_validate(\n     12     naivebayes,\n     13     X,\n     14     data.sentiment,\n     15     scoring = \"accuracy\"\n     16 )\n     18 round(cv_nb['test_score'].mean(),2)\n\nFile c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211, in validate_params.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs)\n    205 try:\n    206     with config_context(\n    207         skip_parameter_validation=(\n    208             prefer_skip_nested_validation or global_skip_validation\n    209         )\n    210     ):\n--&gt; 211         return func(*args, **kwargs)\n    212 except InvalidParameterError as e:\n    213     # When the function is just a wrapper around an estimator, we allow\n    214     # the function to delegate validation to the estimator, but we replace\n    215     # the name of the estimator by the name of the function in the error\n    216     # message to avoid confusion.\n    217     msg = re.sub(\n    218         r\"parameter of \\w+ must be\",\n    219         f\"parameter of {func.__qualname__} must be\",\n    220         str(e),\n    221     )\n\nFile c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:328, in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\n    308 parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n    309 results = parallel(\n    310     delayed(_fit_and_score)(\n    311         clone(estimator),\n   (...)\n    325     for train, test in indices\n    326 )\n--&gt; 328 _warn_or_raise_about_fit_failures(results, error_score)\n    330 # For callable scoring, the return type is only know after calling. If the\n    331 # return type is a dictionary, the error scores can now be inserted with\n    332 # the correct key.\n    333 if callable(scoring):\n\nFile c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:414, in _warn_or_raise_about_fit_failures(results, error_score)\n    407 if num_failed_fits == num_fits:\n    408     all_fits_failed_message = (\n    409         f\"\\nAll the {num_fits} fits failed.\\n\"\n    410         \"It is very likely that your model is misconfigured.\\n\"\n    411         \"You can try to debug the error by setting error_score='raise'.\\n\\n\"\n    412         f\"Below are more details about the failures:\\n{fit_errors_summary}\"\n    413     )\n--&gt; 414     raise ValueError(all_fits_failed_message)\n    416 else:\n    417     some_fits_failed_message = (\n    418         f\"\\n{num_failed_fits} fits failed out of a total of {num_fits}.\\n\"\n    419         \"The score on these train-test partitions for these parameters\"\n   (...)\n    423         f\"Below are more details about the failures:\\n{fit_errors_summary}\"\n    424     )\n\nValueError: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 745, in fit\n    X, y = self._check_X_y(X, y)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 578, in _check_X_y\n    return self._validate_data(X, y, accept_sparse=\"csr\", reset=reset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1147, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\pandas\\core\\series.py\", line 1022, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: '\"Sorte Nula\" is the #1 Box Office Portuguese movie of 2004. This extreme low budget production (estimated USD$150,000) opened during Christmas opposite American Blockbusters like National Treasure, Polar Express, The Incredibles and Alexander but rapidly caught the adulation of the Portuguese moviegoers. Despite the harsh competition, the small film did surprisingly well, topping all other Portuguese films of the past two years in its first weeks. The film is a mystery/murder with a humorous tone cleverly written and directed by Fernando Fragata who has become a solid reference in the European independent film arena. Did I like the film? Oh, yes!'\n\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 745, in fit\n    X, y = self._check_X_y(X, y)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 578, in _check_X_y\n    return self._validate_data(X, y, accept_sparse=\"csr\", reset=reset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1147, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\pandas\\core\\series.py\", line 1022, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: \"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.&lt;br /&gt;&lt;br /&gt;The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.&lt;br /&gt;&lt;br /&gt;It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.&lt;br /&gt;&lt;br /&gt;I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\"\n\n\n\n\n\n\nWhen working with text data, computers need to convert the words into numbers first before being able to work with them. Hence vectorisation - the process of converting numbers into words. There are a few different approaches and concepts which we‚Äôll explore today\n\nTokenization\nBag of words\nTF-IDF\nn-grams\nWord2Vec embeddings\n\nFinally we‚Äôll look at (at a very very high level!) how Transformer/attention-based approaches to word vectorisation have transformed NLP\n\n\n\nBreaking up texts into their individual components, or tokens\n\nfrom nltk.tokenize import word_tokenize\n\ntext = \"Had a slight weapons malfunction but, uh everything's perfectly all right now. We're fine. We're all fine here now. Thank you. How are you?\"\n\n# Document before tokenization\nprint(text)\n\nHad a slight weapons malfunction but, uh everything's perfectly all right now. We're fine. We're all fine here now. Thank you. How are you?\n\n\n\nword_tokens = word_tokenize(text)\n\n# Document after tokenization - each word is separated out. Compound words like \"everything's\" are now two words: \"everything\" and \"'s\"\nprint(word_tokens)\n\n['Had', 'a', 'slight', 'weapons', 'malfunction', 'but', ',', 'uh', 'everything', \"'s\", 'perfectly', 'all', 'right', 'now', '.', 'We', \"'re\", 'fine', '.', 'We', \"'re\", 'all', 'fine', 'here', 'now', '.', 'Thank', 'you', '.', 'How', 'are', 'you', '?']\n\n\n\n\n\n\nTokens: how we‚Äôve broken down the text into smaller units\nDocument: the unit of text we‚Äôre analysing. Could be sentences, could be paragraphs, could be a whole book. Different breakdowns for different purposes\nCorpus: The collection of documents being analysed\n\n\n\n\n\ntexts = [\n    'I love to run',\n    'the cat does not eat fruit',\n    'run to the cat',\n    'I love to eat fruit. fruit fruit fruit fruit'\n]\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer = CountVectorizer()\nX = count_vectorizer.fit_transform(texts)\nX.toarray()\n\narray([[0, 0, 0, 0, 1, 0, 1, 0, 1],\n       [1, 1, 1, 1, 0, 1, 0, 1, 0],\n       [1, 0, 0, 0, 0, 0, 1, 1, 1],\n       [0, 0, 1, 5, 1, 0, 0, 0, 1]], dtype=int64)\n\n\nEach column is a different word, and the count vectorizer simply counts how many appearances of each word are in each sentence.\nü§î Can you guess which column represents which word?\nIt‚Äôs column 4: the word ‚Äúfruit‚Äù appears 5 times in the last sentence.\n\n# Visualising what the vectorizer has done\n\nvectorized_texts = pd.DataFrame(\n    X.toarray(),\n    columns = count_vectorizer.get_feature_names_out(),\n    index = texts\n)\n\nvectorized_texts\n\n\n\n\n\n\n\n\ncat\ndoes\neat\nfruit\nlove\nnot\nrun\nthe\nto\n\n\n\n\nI love to run\n0\n0\n0\n0\n1\n0\n1\n0\n1\n\n\nthe cat does not eat fruit\n1\n1\n1\n1\n0\n1\n0\n1\n0\n\n\nrun to the cat\n1\n0\n0\n0\n0\n0\n1\n1\n1\n\n\nI love to eat fruit. fruit fruit fruit fruit\n0\n0\n1\n5\n1\n0\n0\n0\n1\n\n\n\n\n\n\n\nWe will try the same code from above - this time on the vectorised text instead of the raw text! This time we shouldn‚Äôt get any errors.\n\nnaivebayes = MultinomialNB()\ncount_vectorizer = CountVectorizer()\n\nX = count_vectorizer.fit_transform(data.review)\n\ncv_nb = cross_validate(\n    naivebayes,\n    X,\n    data.sentiment,\n    scoring = \"accuracy\"\n)\n\nround(cv_nb['test_score'].mean(),2)\n\n0.85\n\n\nOur accuracy score is 85% which isn‚Äôt too bad\nWhat are the limitations of this approach?\n\nNo context\nWord order not available\nAll words treated the same\nVery simplistic approach!\n\n\n\n\nTERM FREQUENCY (TF)\nThe more often a word appears in a document relative to others, the more likely it is that it will be important to this document\nExample: if a word appears relatively frequently in a document, it is obvious that this word is important to the overall meaning of the document.\n\n\n\nimage.png\n\n\n\ntexts = [\n    'I love to run',\n    'the cat does not eat the fruit',\n    'run to the cat',\n    'I love to eat the fruit. fruit fruit fruit fruit'\n]\n\n\n# In document 4, the Term Frequency (TF) of the word FRUIT is?\n# The word fruit appears 5 times\n# There are 10 words in the document\n\n5/10\n\n0.5\n\n\nDOCUMENT FREQUENCY (DF)\nIf a word appears in many documents of a corpus, it‚Äôs not important to understand a particular document.\nExample: on eurosport.com/football, the word ‚Äúfootball‚Äù appears in every article, hence why the word football on this website is an unimportant word!\n\n\n\nimage.png\n\n\nFor the word ‚Äúfootball‚Äù on Eurosport, we would expect this formula to be close to 1 since the number of docs containing the word ‚Äúfootball‚Äù will probably only be slightly less than the total number of docs (out of 100 maybe only 5 don‚Äôt have the word ‚Äúfootball‚Äù, so we get 95/100).\nif the word ‚Äúfootball‚Äù appears in all the articles it is not very useful for helping us identify between two articles, but if only a few documents contain words like ‚Äúconcussion‚Äù or ‚Äúwellbeing‚Äù, (e.g.¬†they appear in 2/100 articles) it will be much more useful in determining the topic of that article (they are probably specifically about player wellfare).\nüí° Thus the intuition of the term frequency - inverse document frequency approach is to give a high weight to any term which appears frequently in a single document, but not in too many documents of the corpus.\n\n## ?? Which words appear frequently in our small corpus \n# and might not be useful for deriving meaning?\n\ntexts = [\n    'I love to run',\n    'the cat does not eat the fruit',\n    'run to the cat',\n    'I love to eat the fruit. fruit fruit fruit fruit'\n]\n\n# the\n# to\n\n\n\n\nimage.png\n\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Instantiating the TfidfVectorizer\ntf_idf_vectorizer = TfidfVectorizer()\n\n# Training it on the texts\nweighted_words = pd.DataFrame(tf_idf_vectorizer.fit_transform(texts).toarray(),\n                 columns = tf_idf_vectorizer.get_feature_names_out(),\n                index = texts)\n\nweighted_words\n\n\n\n\n\n\n\n\ncat\ndoes\neat\nfruit\nlove\nnot\nrun\nthe\nto\n\n\n\n\nI love to run\n0.000000\n0.000000\n0.000000\n0.000000\n0.613667\n0.000000\n0.613667\n0.000000\n0.496816\n\n\nthe cat does not eat the fruit\n0.336350\n0.426618\n0.336350\n0.336350\n0.000000\n0.426618\n0.000000\n0.544609\n0.000000\n\n\nrun to the cat\n0.549578\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.549578\n0.444931\n0.444931\n\n\nI love to eat the fruit. fruit fruit fruit fruit\n0.000000\n0.000000\n0.187942\n0.939709\n0.187942\n0.000000\n0.000000\n0.152155\n0.152155\n\n\n\n\n\n\n\nWeaknesses of this approach?\n\nword order still missing\nrelationship between words still missing\n\n\n\n\n\n# The two following sentences have the exact same representation in bag of words/ TFIDF approaches\n# However, they have very different meanings!\n\nsentences = [\n    \"I like cats but not dogs\",\n    \"I like dogs but not cats\"\n]\n\n\n# Vectorize the sentences\ncount_vectorizer = CountVectorizer()\nsentences_vectorized = count_vectorizer.fit_transform(sentences)\n\n# Show the representations in a nice DataFrame\nsentences_vectorized = pd.DataFrame(\n    sentences_vectorized.toarray(),\n    columns = count_vectorizer.get_feature_names_out(),\n    index = sentences\n)\n\n# Show the vectorized words\nsentences_vectorized\n\n\n\n\n\n\n\n\nbut\ncats\ndogs\nlike\nnot\n\n\n\n\nI like cats but not dogs\n1\n1\n1\n1\n1\n\n\nI like dogs but not cats\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\nüßëüèª‚Äçüè´ When using a bag-of-words representation, an efficient way to capture context is to consider:\n\nthe count of single tokens (unigrams)\nthe count of pairs (bigrams), triplets (trigrams), and more generally sequences of n words, also known as n-grams\n\n 4)\nüò• With a unigram vectorization, we couldn‚Äôt distinguish two sentences with the same words, despite their meaning being quite different\n\nsentences_vectorized\n\n\n\n\n\n\n\n\nbut\ncats\ndogs\nlike\nnot\n\n\n\n\nI like cats but not dogs\n1\n1\n1\n1\n1\n\n\nI like dogs but not cats\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\nüë©üèª‚Äçüî¨ What about a bigram vectorization?\n\n# Vectorize the sentences\ncount_vectorizer_n_gram = CountVectorizer(ngram_range = (1,2)) # BI-GRAMS\nsentences_vectorized_n_gram = count_vectorizer_n_gram.fit_transform(sentences)\n\n# Show the representations in a nice DataFrame\nsentences_vectorized_n_gram = pd.DataFrame(\n    sentences_vectorized_n_gram.toarray(),\n    columns = count_vectorizer_n_gram.get_feature_names_out(),\n    index = sentences\n)\n\n# Show the vectorized movies with bigrams (pairs of words)\nsentences_vectorized_n_gram\n\n\n\n\n\n\n\n\nbut\nbut not\ncats\ncats but\ndogs\ndogs but\nlike\nlike cats\nlike dogs\nnot\nnot cats\nnot dogs\n\n\n\n\nI like cats but not dogs\n1\n1\n1\n1\n1\n0\n1\n1\n0\n1\n0\n1\n\n\nI like dogs but not cats\n1\n1\n1\n0\n1\n1\n1\n0\n1\n1\n1\n0\n\n\n\n\n\n\n\n\n\n\nAttempting to capture semantic meaning of words in numerical format\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\nimport gensim.downloader\n\n# Lots of different pretrained embeddings we can use for free!\nprint(list(gensim.downloader.info()['models'].keys()))\n\n['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n\n\n\n# We will use a vectoriser trained on Wikipedia today\nmodel_wiki = gensim.downloader.load('glove-wiki-gigaword-50')\n\n\n# Vectors based on 2B tweets, 27B tokens, 1.2M vocab!\n# 50 dimensions\n# N.B. Words not in glove-wiki-gigaword-50 will not have vectors computed. For example, if there was a niche word or acronym like \"NHS-R\" there would not be a vector for this word.\n\nmodel_wiki[\"cat\"]\n\narray([ 0.45281 , -0.50108 , -0.53714 , -0.015697,  0.22191 ,  0.54602 ,\n       -0.67301 , -0.6891  ,  0.63493 , -0.19726 ,  0.33685 ,  0.7735  ,\n        0.90094 ,  0.38488 ,  0.38367 ,  0.2657  , -0.08057 ,  0.61089 ,\n       -1.2894  , -0.22313 , -0.61578 ,  0.21697 ,  0.35614 ,  0.44499 ,\n        0.60885 , -1.1633  , -1.1579  ,  0.36118 ,  0.10466 , -0.78325 ,\n        1.4352  ,  0.18629 , -0.26112 ,  0.83275 , -0.23123 ,  0.32481 ,\n        0.14485 , -0.44552 ,  0.33497 , -0.95946 , -0.097479,  0.48138 ,\n       -0.43352 ,  0.69455 ,  0.91043 , -0.28173 ,  0.41637 , -1.2609  ,\n        0.71278 ,  0.23782 ], dtype=float32)\n\n\n\n# King is to Queen as Man is to ...\n\nexample_1 = model_wiki[\"queen\"] - model_wiki[\"king\"] + model_wiki[\"man\"]\nmodel_wiki.most_similar(example_1)[0]\n\n('woman', 0.8903914093971252)\n\n\n\n# Similar words to cat\n\nmodel_wiki.most_similar(model_wiki[\"cat\"])\n\n[('cat', 1.0),\n ('dog', 0.9218006134033203),\n ('rabbit', 0.8487820625305176),\n ('monkey', 0.804108202457428),\n ('rat', 0.7891963124275208),\n ('cats', 0.7865270972251892),\n ('snake', 0.7798910140991211),\n ('dogs', 0.7795815467834473),\n ('pet', 0.7792249917984009),\n ('mouse', 0.7731667160987854)]\n\n\n\n# Opposite of cold...?\n\nexample_2 = model_wiki[\"good\"] - model_wiki[\"evil\"] + model_wiki[\"cold\"]\nmodel_wiki.most_similar(example_2)[0]\n\n('warm', 0.7870427966117859)\n\n\n\n\n\nThe basis of transformer-based neural networks like ChatGPT! The paper that started it all: Attention is all you need\n\nEach token (word) embedding gets projected ‚û°Ô∏è into 3 further vectors: the query, key and value vectors (usually 768 dimensions each)!!\nWe compute a scaled dot-product üî¥ on the query and key vectors to work out how much each word relates to those around it\nTake these scores and normalize with softmax ‚§µÔ∏è\nMultiply by our value vectors ‚ùé, sum and pass to our dense neural network\n\n‚ö†Ô∏è TLDR: The main point is that each word is now represented by 768 * 3 numbers! This is partly what makes LLMs so powerful (and resource-hungry) ‚ö†Ô∏è\nIn the simple bag-of-words and TFIDF approaches, each word was represented by only 1 number each\nIn more complex word embeddings each word was represented by between 50 to 300 numbers each\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\n\nR text mining book https://www.tidytextmining.com/\nHuggingface tutorials (python) https://huggingface.co/learn/nlp-course/chapter1/1\nGreat video on attention https://www.youtube.com/watch?v=zxQyTK8quyY"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#key-learning",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#key-learning",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "When working with text data, computers need to convert the words into numbers first before being able to work with them. Hence vectorisation - the process of converting numbers into words. There are a few different approaches and concepts which we‚Äôll explore today\n\nTokenization\nBag of words\nTF-IDF\nn-grams\nWord2Vec embeddings\n\nFinally we‚Äôll look at (at a very very high level!) how Transformer/attention-based approaches to word vectorisation have transformed NLP"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#tokenization",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#tokenization",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "Breaking up texts into their individual components, or tokens\n\nfrom nltk.tokenize import word_tokenize\n\ntext = \"Had a slight weapons malfunction but, uh everything's perfectly all right now. We're fine. We're all fine here now. Thank you. How are you?\"\n\n# Document before tokenization\nprint(text)\n\nHad a slight weapons malfunction but, uh everything's perfectly all right now. We're fine. We're all fine here now. Thank you. How are you?\n\n\n\nword_tokens = word_tokenize(text)\n\n# Document after tokenization - each word is separated out. Compound words like \"everything's\" are now two words: \"everything\" and \"'s\"\nprint(word_tokens)\n\n['Had', 'a', 'slight', 'weapons', 'malfunction', 'but', ',', 'uh', 'everything', \"'s\", 'perfectly', 'all', 'right', 'now', '.', 'We', \"'re\", 'fine', '.', 'We', \"'re\", 'all', 'fine', 'here', 'now', '.', 'Thank', 'you', '.', 'How', 'are', 'you', '?']"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#some-terminology",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#some-terminology",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "Tokens: how we‚Äôve broken down the text into smaller units\nDocument: the unit of text we‚Äôre analysing. Could be sentences, could be paragraphs, could be a whole book. Different breakdowns for different purposes\nCorpus: The collection of documents being analysed"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#bag-of-words",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#bag-of-words",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "texts = [\n    'I love to run',\n    'the cat does not eat fruit',\n    'run to the cat',\n    'I love to eat fruit. fruit fruit fruit fruit'\n]\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer = CountVectorizer()\nX = count_vectorizer.fit_transform(texts)\nX.toarray()\n\narray([[0, 0, 0, 0, 1, 0, 1, 0, 1],\n       [1, 1, 1, 1, 0, 1, 0, 1, 0],\n       [1, 0, 0, 0, 0, 0, 1, 1, 1],\n       [0, 0, 1, 5, 1, 0, 0, 0, 1]], dtype=int64)\n\n\nEach column is a different word, and the count vectorizer simply counts how many appearances of each word are in each sentence.\nü§î Can you guess which column represents which word?\nIt‚Äôs column 4: the word ‚Äúfruit‚Äù appears 5 times in the last sentence.\n\n# Visualising what the vectorizer has done\n\nvectorized_texts = pd.DataFrame(\n    X.toarray(),\n    columns = count_vectorizer.get_feature_names_out(),\n    index = texts\n)\n\nvectorized_texts\n\n\n\n\n\n\n\n\ncat\ndoes\neat\nfruit\nlove\nnot\nrun\nthe\nto\n\n\n\n\nI love to run\n0\n0\n0\n0\n1\n0\n1\n0\n1\n\n\nthe cat does not eat fruit\n1\n1\n1\n1\n0\n1\n0\n1\n0\n\n\nrun to the cat\n1\n0\n0\n0\n0\n0\n1\n1\n1\n\n\nI love to eat fruit. fruit fruit fruit fruit\n0\n0\n1\n5\n1\n0\n0\n0\n1\n\n\n\n\n\n\n\nWe will try the same code from above - this time on the vectorised text instead of the raw text! This time we shouldn‚Äôt get any errors.\n\nnaivebayes = MultinomialNB()\ncount_vectorizer = CountVectorizer()\n\nX = count_vectorizer.fit_transform(data.review)\n\ncv_nb = cross_validate(\n    naivebayes,\n    X,\n    data.sentiment,\n    scoring = \"accuracy\"\n)\n\nround(cv_nb['test_score'].mean(),2)\n\n0.85\n\n\nOur accuracy score is 85% which isn‚Äôt too bad\nWhat are the limitations of this approach?\n\nNo context\nWord order not available\nAll words treated the same\nVery simplistic approach!"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#tf-idf",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#tf-idf",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "TERM FREQUENCY (TF)\nThe more often a word appears in a document relative to others, the more likely it is that it will be important to this document\nExample: if a word appears relatively frequently in a document, it is obvious that this word is important to the overall meaning of the document.\n\n\n\nimage.png\n\n\n\ntexts = [\n    'I love to run',\n    'the cat does not eat the fruit',\n    'run to the cat',\n    'I love to eat the fruit. fruit fruit fruit fruit'\n]\n\n\n# In document 4, the Term Frequency (TF) of the word FRUIT is?\n# The word fruit appears 5 times\n# There are 10 words in the document\n\n5/10\n\n0.5\n\n\nDOCUMENT FREQUENCY (DF)\nIf a word appears in many documents of a corpus, it‚Äôs not important to understand a particular document.\nExample: on eurosport.com/football, the word ‚Äúfootball‚Äù appears in every article, hence why the word football on this website is an unimportant word!\n\n\n\nimage.png\n\n\nFor the word ‚Äúfootball‚Äù on Eurosport, we would expect this formula to be close to 1 since the number of docs containing the word ‚Äúfootball‚Äù will probably only be slightly less than the total number of docs (out of 100 maybe only 5 don‚Äôt have the word ‚Äúfootball‚Äù, so we get 95/100).\nif the word ‚Äúfootball‚Äù appears in all the articles it is not very useful for helping us identify between two articles, but if only a few documents contain words like ‚Äúconcussion‚Äù or ‚Äúwellbeing‚Äù, (e.g.¬†they appear in 2/100 articles) it will be much more useful in determining the topic of that article (they are probably specifically about player wellfare).\nüí° Thus the intuition of the term frequency - inverse document frequency approach is to give a high weight to any term which appears frequently in a single document, but not in too many documents of the corpus.\n\n## ?? Which words appear frequently in our small corpus \n# and might not be useful for deriving meaning?\n\ntexts = [\n    'I love to run',\n    'the cat does not eat the fruit',\n    'run to the cat',\n    'I love to eat the fruit. fruit fruit fruit fruit'\n]\n\n# the\n# to\n\n\n\n\nimage.png\n\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Instantiating the TfidfVectorizer\ntf_idf_vectorizer = TfidfVectorizer()\n\n# Training it on the texts\nweighted_words = pd.DataFrame(tf_idf_vectorizer.fit_transform(texts).toarray(),\n                 columns = tf_idf_vectorizer.get_feature_names_out(),\n                index = texts)\n\nweighted_words\n\n\n\n\n\n\n\n\ncat\ndoes\neat\nfruit\nlove\nnot\nrun\nthe\nto\n\n\n\n\nI love to run\n0.000000\n0.000000\n0.000000\n0.000000\n0.613667\n0.000000\n0.613667\n0.000000\n0.496816\n\n\nthe cat does not eat the fruit\n0.336350\n0.426618\n0.336350\n0.336350\n0.000000\n0.426618\n0.000000\n0.544609\n0.000000\n\n\nrun to the cat\n0.549578\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.549578\n0.444931\n0.444931\n\n\nI love to eat the fruit. fruit fruit fruit fruit\n0.000000\n0.000000\n0.187942\n0.939709\n0.187942\n0.000000\n0.000000\n0.152155\n0.152155\n\n\n\n\n\n\n\nWeaknesses of this approach?\n\nword order still missing\nrelationship between words still missing"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#n-grams",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#n-grams",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "# The two following sentences have the exact same representation in bag of words/ TFIDF approaches\n# However, they have very different meanings!\n\nsentences = [\n    \"I like cats but not dogs\",\n    \"I like dogs but not cats\"\n]\n\n\n# Vectorize the sentences\ncount_vectorizer = CountVectorizer()\nsentences_vectorized = count_vectorizer.fit_transform(sentences)\n\n# Show the representations in a nice DataFrame\nsentences_vectorized = pd.DataFrame(\n    sentences_vectorized.toarray(),\n    columns = count_vectorizer.get_feature_names_out(),\n    index = sentences\n)\n\n# Show the vectorized words\nsentences_vectorized\n\n\n\n\n\n\n\n\nbut\ncats\ndogs\nlike\nnot\n\n\n\n\nI like cats but not dogs\n1\n1\n1\n1\n1\n\n\nI like dogs but not cats\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\nüßëüèª‚Äçüè´ When using a bag-of-words representation, an efficient way to capture context is to consider:\n\nthe count of single tokens (unigrams)\nthe count of pairs (bigrams), triplets (trigrams), and more generally sequences of n words, also known as n-grams\n\n 4)\nüò• With a unigram vectorization, we couldn‚Äôt distinguish two sentences with the same words, despite their meaning being quite different\n\nsentences_vectorized\n\n\n\n\n\n\n\n\nbut\ncats\ndogs\nlike\nnot\n\n\n\n\nI like cats but not dogs\n1\n1\n1\n1\n1\n\n\nI like dogs but not cats\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\nüë©üèª‚Äçüî¨ What about a bigram vectorization?\n\n# Vectorize the sentences\ncount_vectorizer_n_gram = CountVectorizer(ngram_range = (1,2)) # BI-GRAMS\nsentences_vectorized_n_gram = count_vectorizer_n_gram.fit_transform(sentences)\n\n# Show the representations in a nice DataFrame\nsentences_vectorized_n_gram = pd.DataFrame(\n    sentences_vectorized_n_gram.toarray(),\n    columns = count_vectorizer_n_gram.get_feature_names_out(),\n    index = sentences\n)\n\n# Show the vectorized movies with bigrams (pairs of words)\nsentences_vectorized_n_gram\n\n\n\n\n\n\n\n\nbut\nbut not\ncats\ncats but\ndogs\ndogs but\nlike\nlike cats\nlike dogs\nnot\nnot cats\nnot dogs\n\n\n\n\nI like cats but not dogs\n1\n1\n1\n1\n1\n0\n1\n1\n0\n1\n0\n1\n\n\nI like dogs but not cats\n1\n1\n1\n0\n1\n1\n1\n0\n1\n1\n1\n0"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#word2vec-embeddings",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#word2vec-embeddings",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "Attempting to capture semantic meaning of words in numerical format\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\nimport gensim.downloader\n\n# Lots of different pretrained embeddings we can use for free!\nprint(list(gensim.downloader.info()['models'].keys()))\n\n['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n\n\n\n# We will use a vectoriser trained on Wikipedia today\nmodel_wiki = gensim.downloader.load('glove-wiki-gigaword-50')\n\n\n# Vectors based on 2B tweets, 27B tokens, 1.2M vocab!\n# 50 dimensions\n# N.B. Words not in glove-wiki-gigaword-50 will not have vectors computed. For example, if there was a niche word or acronym like \"NHS-R\" there would not be a vector for this word.\n\nmodel_wiki[\"cat\"]\n\narray([ 0.45281 , -0.50108 , -0.53714 , -0.015697,  0.22191 ,  0.54602 ,\n       -0.67301 , -0.6891  ,  0.63493 , -0.19726 ,  0.33685 ,  0.7735  ,\n        0.90094 ,  0.38488 ,  0.38367 ,  0.2657  , -0.08057 ,  0.61089 ,\n       -1.2894  , -0.22313 , -0.61578 ,  0.21697 ,  0.35614 ,  0.44499 ,\n        0.60885 , -1.1633  , -1.1579  ,  0.36118 ,  0.10466 , -0.78325 ,\n        1.4352  ,  0.18629 , -0.26112 ,  0.83275 , -0.23123 ,  0.32481 ,\n        0.14485 , -0.44552 ,  0.33497 , -0.95946 , -0.097479,  0.48138 ,\n       -0.43352 ,  0.69455 ,  0.91043 , -0.28173 ,  0.41637 , -1.2609  ,\n        0.71278 ,  0.23782 ], dtype=float32)\n\n\n\n# King is to Queen as Man is to ...\n\nexample_1 = model_wiki[\"queen\"] - model_wiki[\"king\"] + model_wiki[\"man\"]\nmodel_wiki.most_similar(example_1)[0]\n\n('woman', 0.8903914093971252)\n\n\n\n# Similar words to cat\n\nmodel_wiki.most_similar(model_wiki[\"cat\"])\n\n[('cat', 1.0),\n ('dog', 0.9218006134033203),\n ('rabbit', 0.8487820625305176),\n ('monkey', 0.804108202457428),\n ('rat', 0.7891963124275208),\n ('cats', 0.7865270972251892),\n ('snake', 0.7798910140991211),\n ('dogs', 0.7795815467834473),\n ('pet', 0.7792249917984009),\n ('mouse', 0.7731667160987854)]\n\n\n\n# Opposite of cold...?\n\nexample_2 = model_wiki[\"good\"] - model_wiki[\"evil\"] + model_wiki[\"cold\"]\nmodel_wiki.most_similar(example_2)[0]\n\n('warm', 0.7870427966117859)"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#attention-mechanism",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#attention-mechanism",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "The basis of transformer-based neural networks like ChatGPT! The paper that started it all: Attention is all you need\n\nEach token (word) embedding gets projected ‚û°Ô∏è into 3 further vectors: the query, key and value vectors (usually 768 dimensions each)!!\nWe compute a scaled dot-product üî¥ on the query and key vectors to work out how much each word relates to those around it\nTake these scores and normalize with softmax ‚§µÔ∏è\nMultiply by our value vectors ‚ùé, sum and pass to our dense neural network\n\n‚ö†Ô∏è TLDR: The main point is that each word is now represented by 768 * 3 numbers! This is partly what makes LLMs so powerful (and resource-hungry) ‚ö†Ô∏è\nIn the simple bag-of-words and TFIDF approaches, each word was represented by only 1 number each\nIn more complex word embeddings each word was represented by between 50 to 300 numbers each\n\n\n\nimage.png\n\n\n\n\n\nimage.png"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#resources",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#resources",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "R text mining book https://www.tidytextmining.com/\nHuggingface tutorials (python) https://huggingface.co/learn/nlp-course/chapter1/1\nGreat video on attention https://www.youtube.com/watch?v=zxQyTK8quyY"
  },
  {
    "objectID": "blogs/posts/2024-02-28_sankey_plot/index.html",
    "href": "blogs/posts/2024-02-28_sankey_plot/index.html",
    "title": "Visualising participant recruitment in R using Sankey plots",
    "section": "",
    "text": "Sankey diagrams are great tools to visualise flows through a system. They show connections between the steps of a process where the width of the arrows is proportional to the flow.\nI‚Äôm working on an evaluation of a risk screening process for people aged between 55-74 years and a history of smoking. In this Targeted Lung Health Check (TLHC) programme1 eligible people are invited to attend a free lung check where those assessed at high risk of lung cancer are then offered low-dose CT screening scans.\n1¬†Please visit the NHS England site for for more background.We used Sankey diagrams to visualise how people have engaged with the programme, from recruitment, attendance at appointments, their outcome from risk assessment, attendance at CT scans and will eventually be extended to cover the impact of the screening on early detection of those diagnosed with lung cancer.\nThis blog post is about the technical process of preparing record-level data for visualisation in a Sankey plot using R and customising it to enhance look and feel. Here is how the finished product will look:"
  },
  {
    "objectID": "blogs/posts/2024-02-28_sankey_plot/index.html#get-the-data",
    "href": "blogs/posts/2024-02-28_sankey_plot/index.html#get-the-data",
    "title": "Visualising participant recruitment in R using Sankey plots",
    "section": "Get the data",
    "text": "Get the data\nIn this example we will work with a simplified set of data focused on invitations.\nThe invites table holds details of when people were sent a letter or message inviting them to take part, how many times they were invited and how the person responded.\nThe people eligible for the programme are identified up-front and are represented by a unique ID with one row per person. Let‚Äôs assume each person receives at least one invitation to take part, they can have one of three outcomes:\n\nThey accept the invitation and agree to take part,\nThey decline the invitation,\nThey do not respond to the invitation.\n\nIf the person doesn‚Äôt respond to the first invitation they may be sent a second invitation and could be offered a third invitation if they didn‚Äôt respond to the second.\nHere is the specification for our simplified invites table:\n\nInvites specification\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nParticipant ID\nInteger\nA unique identifier for each person.\n\n\nInvite date 1\nDate\nThe date the person was first invited to participate.\nEvery person will have a date in this field.\n\n\nInvite date 2\nDate\nThe date a second invitation was sent.\n\n\nInvite date 3\nDate\nThe date a third invitation was sent.\n\n\nInvite outcome\nText\nThe outcome from the invite, one of either ‚ÄòAccepted‚Äô, ‚ÄòDeclined‚Äô or ‚ÄòNo response‚Äô.\n\n\n\nEveryone receives at least one invite. Assuming a third of these respond (to either accept or decline) then two-thirds receive a follow-up invite. Of these, we assume half respond, meaning the remaining participants receive a third invite.\nHere we generate 100 rows of example data to populate our table.\n\n\nCode\n# set a randomisation seed for reproducibility\nset.seed(seed = 1234)\n\n# define some parameters\nstart_date &lt;- as.Date(\"2019-01-01\")\nend_date &lt;- as.Date(\"2021-01-01\")\nrows &lt;- 100\n\ndf_invites_1 &lt;- tibble(\n  # create a unique id for each participant\n  participant_id = 1:rows,\n\n  # create a random initial invite date between our start and end dates\n  invite_1_date = sample(\n    seq(start_date, end_date, by = \"day\"),\n    size = rows, replace = T\n  ),\n\n  # create a random outcome for this participant\n  invite_outcome = sample(\n    x = c(\"Accepted\", \"Declined\", \"No response\"),\n    size = rows, replace = T\n  )\n)\n\n# take a sample of participants and allocate them a second invite date\ndf_invites_2 &lt;- df_invites_1 |&gt;\n  # sample two thirds of participants to get a second invite\n  slice_sample(prop = 2 / 3) |&gt;\n  # allocate a date between 10 and 30 days following the first\n  mutate(\n    invite_2_date = invite_1_date + sample(10:30, size = n(), replace = T)\n  ) |&gt;\n  # keep just id and second date\n  select(participant_id, invite_2_date)\n\n\n# take a sample of those with a second invite and allocate them a third invite date\ndf_invites_3 &lt;- df_invites_2 |&gt;\n  # sample half of these to get a third invite\n  slice_sample(prop = 1 / 2) |&gt;\n  # allocate a date between 10 to 30 days following the second\n  mutate(\n    invite_3_date = invite_2_date + sample(10:30, size = n(), replace = T)\n  ) |&gt;\n  # keep just id and second date\n  select(participant_id, invite_3_date)\n\n# combine the 2nd and 3rd invites with the first table\ndf_invites &lt;- df_invites_1 |&gt;\n  left_join(\n    y = df_invites_2,\n    by = \"participant_id\"\n  ) |&gt;\n  left_join(\n    y = df_invites_3,\n    by = \"participant_id\"\n  ) |&gt;\n  # move the outcome field after the third invite\n  relocate(invite_outcome, .after = invite_3_date)\n\n# housekeeping\nrm(df_invites_1, df_invites_2, df_invites_3, start_date, end_date, rows)\n\n# view our data\ndf_invites |&gt;\n  reactable(defaultPageSize = 5)\n\n\n\n\nGenerated invite table"
  },
  {
    "objectID": "blogs/posts/2024-02-28_sankey_plot/index.html#determine-milestone-outcomes",
    "href": "blogs/posts/2024-02-28_sankey_plot/index.html#determine-milestone-outcomes",
    "title": "Visualising participant recruitment in R using Sankey plots",
    "section": "Determine milestone outcomes",
    "text": "Determine milestone outcomes\nThe next step is to take our source table and convert the data into a series of milestones (and associated outcomes) that represents how our invited participants moved through the pathway.\nIn our example we have five milestones to represent in our Sankey plot:\n\nOur eligible population (everyone in our invites table),\nThe result from the first invitation,\nThe result from the second invitation,\nThe result from the third invitation,\nThe overall invite outcome.\n\nAside from the eligible population, where everyone starts with the same value, participants will have one of several outcomes at each milestone. This step is about naming these milestones and the outcomes.\nIt is important that each milestone-outcome has unique values. An outcome of ‚ÄòNo response‚Äô can be recorded against the first, second and third invite, and we wish to see these outcomes separately represented on the Sankey (rather than just one ‚ÄòNo response‚Äô), so each outcome must be made unique. In this example we prefix the outcome from each invite with the number of the invite, e.g.¬†‚ÄòInvite 1 No response‚Äô.\nThe reason for this will become clearer when we come to plot the Sankey, but for now we produce these milestone-outcomes from our invites table.\n\n\nCode\ndf_milestones &lt;- df_invites |&gt;\n  mutate(\n    # everyone starts in the eligible population\n    start_population = \"Eligible population\",\n\n    # work out what happened following the first invite\n    invite_1_outcome = case_when(\n      # if a second invite was sent we assume there was no outcome from the first\n      !is.na(invite_2_date) ~ \"Invitation 1 No response\",\n      # otherwise the overall outcome resulted from the first invite\n      .default = glue(\"Invitation 1 {invite_outcome}\")\n    ),\n\n    # work out what happened following the second invite\n    invite_2_outcome = case_when(\n      # if a third invite was sent we assume there was no outcome from the second\n      !is.na(invite_3_date) ~ \"Invitation 2 No response\",\n      # if a second invite was sent but no third then\n      !is.na(invite_2_date) ~ glue(\"Invitation 2 {invite_outcome}\"),\n      # default to NA if neither of the above are true\n      .default = NA\n    ),\n\n    # work out what happened following the third invite\n    invite_3_outcome = case_when(\n      # if a third invite was sent then the outcome is the overall outcome\n      !is.na(invite_3_date) ~ glue(\"Invitation 3 {invite_outcome}\"),\n      # otherwise mark as NA\n      .default = NA\n    )\n  ) |&gt;\n  # exclude the dates as they are no longer needed\n  select(-contains(\"_date\")) |&gt;\n  # move the overall invite outcome to the end\n  relocate(invite_outcome, .after = invite_3_outcome)\n\n# view our data\ndf_milestones |&gt;\n  reactable(defaultPageSize = 5)\n\n\n\n\nMilestone-outcomes for participants"
  },
  {
    "objectID": "blogs/posts/2024-02-28_sankey_plot/index.html#calculate-flows",
    "href": "blogs/posts/2024-02-28_sankey_plot/index.html#calculate-flows",
    "title": "Visualising participant recruitment in R using Sankey plots",
    "section": "Calculate flows",
    "text": "Calculate flows\nNext we take pairs of milestone-outcomes and calculate the number of participants that moved between them.\nHere we utilise the power of dplyr::summarise with an argument .by to group by our data before counting the number of unique participants who move between our start and end groups.\nFor invites 2 and 3 we perform two sets of summaries:\n\nThe first where the values in the to and from fields contain details.\nThe second to capture cases where the to destination is NULL. This is because the participant responded at the previous invite so there was no subsequent invite. In these cases we flow the participant to the overall invite outcome.2\n\n2¬†If you are thinking there is a lot of repetition here, you‚Äôre right. In practice I abstracted both steps to a function and passed in the parameters for the from and to variables and simplified my workflow a little, however, I‚Äôm showing it in plain form here for simplification.\n\nCode\ndf_flows &lt;- bind_rows(\n\n  # flow from population to invite 1\n  df_milestones |&gt;\n    filter(!is.na(start_population) & !is.na(invite_1_outcome)) |&gt;\n    rename(from = start_population, to = invite_1_outcome) |&gt;\n    summarise(\n      flow = n_distinct(participant_id, na.rm = T),\n      .by = c(from, to)\n    ),\n\n  # flow from invite 1 to invite 2 (where not NA)\n  df_milestones |&gt;\n    filter(!is.na(invite_1_outcome) & !is.na(invite_2_outcome)) |&gt;\n    rename(from = invite_1_outcome, to = invite_2_outcome) |&gt;\n    summarise(\n      flow = n_distinct(participant_id, na.rm = T),\n      .by = c(from, to)\n    ),\n\n  # flow from invite 1 to overall invite outcome (where invite 2 is NA)\n  df_milestones |&gt;\n    filter(!is.na(invite_1_outcome) & is.na(invite_2_outcome)) |&gt;\n    rename(from = invite_1_outcome, to = invite_outcome) |&gt;\n    summarise(\n      flow = n_distinct(participant_id, na.rm = T),\n      .by = c(from, to)\n    ),\n\n  # flow from invite 2 to invite 3 (where not NA)\n  df_milestones |&gt;\n    filter(!is.na(invite_2_outcome) & !is.na(invite_3_outcome)) |&gt;\n    rename(from = invite_2_outcome, to = invite_3_outcome) |&gt;\n    summarise(\n      flow = n_distinct(participant_id, na.rm = T),\n      .by = c(from, to)\n    ),\n\n  # flow from invite 2 to overall invite outcome (where invite 3 is NA)\n  df_milestones |&gt;\n    filter(!is.na(invite_2_outcome) & is.na(invite_3_outcome)) |&gt;\n    rename(from = invite_2_outcome, to = invite_outcome) |&gt;\n    summarise(\n      flow = n_distinct(participant_id, na.rm = T),\n      .by = c(from, to)\n    ),\n\n  # final flow - invite 3 to overall outcome (where both are not NA)\n  df_milestones |&gt;\n    filter(!is.na(invite_3_outcome) & !is.na(invite_outcome)) |&gt;\n    rename(from = invite_3_outcome, to = invite_outcome) |&gt;\n    summarise(\n      flow = n_distinct(participant_id, na.rm = T),\n      .by = c(from, to)\n    )\n)\n\n# view our data\ndf_flows |&gt;\n  reactable(defaultPageSize = 5)\n\n\n\n\nFlows of participants between milestones"
  },
  {
    "objectID": "blogs/posts/2024-02-28_sankey_plot/index.html#preparing-for-plotly",
    "href": "blogs/posts/2024-02-28_sankey_plot/index.html#preparing-for-plotly",
    "title": "Visualising participant recruitment in R using Sankey plots",
    "section": "Preparing for plotly",
    "text": "Preparing for plotly\nPlotly expects to be fed two sets of data:\n\nNodes - these are the milestones we have in our from and to fields,\nEdges - these are the flows that occur between nodes, the flow in our table.\n\nIt is possible to extract this data by hand but I found using the tidygraph package was much easier and more convenient.\n\ndf_sankey &lt;- df_flows |&gt;\n  # convert our flows data to a tidy graph object\n  as_tbl_graph()\n\nThe tidygraph package splits our data into nodes and edges. We can selectively work on each by ‚Äòactivating‚Äô them - here is the nodes list:\n\ndf_sankey |&gt;\n  activate(what = \"nodes\") |&gt;\n  as_tibble() |&gt;\n  reactable(defaultPageSize = 5)\n\n\n\n\n\nYou can see each unique node name listed. The row numbers for these nodes are used as reference IDs in the edges object:\n\ndf_sankey |&gt;\n  activate(what = \"edges\") |&gt;\n  as_tibble() |&gt;\n  reactable(defaultPageSize = 5)\n\n\n\n\n\nWe now have enough information to generate our Sankey.\nFirst we extract our nodes and edges to separate data frames then convert the ID values to be zero-based (starts at 0) as this is what plotly is expecting. To do this is as simple as subtracting 1 from the value of the IDs.\nFinally we pass these two dataframes to plotly‚Äôs node and link function inputs to generate the plot.\n\n\nCode\n# extract the nodes to a dataframe\nnodes &lt;- df_sankey |&gt;\n  activate(nodes) |&gt;\n  data.frame() |&gt;\n  mutate(\n    id = row_number() - 1\n  )\n\n# extract the edges to a dataframe\nedges &lt;- df_sankey |&gt;\n  activate(edges) |&gt;\n  data.frame() |&gt;\n  mutate(\n    from = from - 1,\n    to = to - 1\n  )\n\n# plot our sankey\nplot_ly(\n  # setup\n  type = \"sankey\",\n  orientation = \"h\",\n  arrangement = \"snap\",\n\n  # use our node data\n  node = list(\n    label = nodes$name\n  ),\n\n  # use our link data\n  link = list(\n    source = edges$from,\n    target = edges$to,\n    value = edges$flow\n  )\n)\n\n\n\n\nOur first sankey\n\n\nNot bad!\nWe can see the structure of our Sankey now. Can you see the relative proportions of participants who did or didn‚Äôt respond to our first invite? Marvel at how those who responded to the first invite flow into our final outcome. How about those who didn‚Äôt respond to the first invitation go on to receive a second invite?\nPlotly‚Äôs charts are interactive. Try hovering your cursor over the nodes and edges to highlight them and a pop-up box will appear giving you additional details. You can reorder the vertical position of the nodes by dragging them above or below an adjacent node.\nThis looks functional."
  },
  {
    "objectID": "blogs/posts/2024-02-28_sankey_plot/index.html#styling-our-sankey",
    "href": "blogs/posts/2024-02-28_sankey_plot/index.html#styling-our-sankey",
    "title": "Visualising participant recruitment in R using Sankey plots",
    "section": "Styling our Sankey",
    "text": "Styling our Sankey\nNow we have the foundations of our Sankey I‚Äôd like to move on to its presentation. Specifically I‚Äôd like to:\n\nuse colour coding to clearly group those who accept or decline the invite,\nimprove the readability of the node titles,\nadd additional information to the pop-up boxes when you hover over nodes and edges, and\ncontrol the positioning of the nodes in the plot.\n\nAs our nodes and edges objects are dataframes it is straightforward to add this styling information directly to them.\nFor the nodes object we define colours based on the name of each node and manually position them in the plot\n\n\nCode\n# get the eligible population as a single value\n# NB, will be used to work out % amounts in each node and edge\ntemp_eligible_pop &lt;- df_flows |&gt;\n  filter(from == \"Eligible population\") |&gt;\n  summarise(total = sum(flow, na.rm = T)) |&gt;\n  pull(total)\n\n# style our nodes object\nnodes &lt;- nodes |&gt;\n  mutate(\n    # colour ----\n    # add colour definitions, green for accepted, red for declined\n    colour = case_when(\n      str_detect(name, \"Accepted\") ~ \"#44bd32\",\n      str_detect(name, \"Declined\") ~ \"#c23616\",\n      str_detect(name, \"No response\") ~ \"#7f8fa6\",\n      str_detect(name, \"Eligible population\") ~ \"#7f8fa6\"\n    ),\n\n    # add a semi-transparent colour for the edges based on node colours\n    colour_fade = col2hcl(colour = colour, alpha = 0.3),\n\n    # positioning ----\n    # NB, I found that to position nodes you need to supply both\n    # horizontal and vertical positions\n    # NNB, it was a bit of trial and error to get the these positions just\n    # right\n\n    # horizontal positions (0 = left, 1 = right)\n    x = case_when(\n      str_detect(name, \"Eligible population\") ~ 1,\n      str_detect(name, \"Invitation 1\") ~ 2,\n      str_detect(name, \"Invitation 2\") ~ 3,\n      str_detect(name, \"Invitation 3\") ~ 4,\n      .default = 5\n    ) |&gt; rescale(to = c(0.001, 0.9)),\n\n    # vertical position (1 = bottom, 0 = top)\n    y = case_when(\n      str_detect(name, \"Eligible population\") ~ 5,\n      # invite 1\n      str_detect(name, \"Invitation 1 Accepted\") ~ 1,\n      str_detect(name, \"Invitation 1 No response\") ~ 5,\n      str_detect(name, \"Invitation 1 Declined\") ~ 8.5,\n      # invite 2\n      str_detect(name, \"Invitation 2 Accepted\") ~ 2,\n      str_detect(name, \"Invitation 2 No response\") ~ 5,\n      str_detect(name, \"Invitation 2 Declined\") ~ 7.8,\n      # invite 3\n      str_detect(name, \"Invitation 3 Accepted\") ~ 2.7,\n      str_detect(name, \"Invitation 3 No response\") ~ 5.8,\n      str_detect(name, \"Invitation 3 Declined\") ~ 7.2,\n      # final outcomes\n      str_detect(name, \"Accepted\") ~ 1,\n      str_detect(name, \"No response\") ~ 5,\n      str_detect(name, \"Declined\") ~ 8,\n      .default = 5\n    ) |&gt; rescale(to = c(0.001, 0.999))\n  ) |&gt;\n  # add in a custom field to show the percentage flow\n  left_join(\n    y = df_flows |&gt;\n      group_by(to) |&gt;\n      summarise(\n        flow = sum(flow, na.rm = T),\n        flow_perc = percent(flow / temp_eligible_pop, accuracy = 0.1),\n      ) |&gt;\n      select(name = to, flow_perc),\n    by = \"name\"\n  )\n\n# view our nodes data\nnodes |&gt;\n  reactable(defaultPageSize = 5)\n\n\n\n\nStyling the nodes dataframe\n\n\nNext we move to styling the edges, which is a much simpler prospect:\n\n\nCode\nedges &lt;- edges |&gt;\n  mutate(\n    # add a label for each flow to tell us how many people are in each\n    label = number(flow, big.mark = \",\"),\n    # add a percentage flow figure\n    flow_perc = percent(flow / temp_eligible_pop, accuracy = 0.1)\n  ) |&gt;\n  # add the faded colour from our nodes object to match the destinations\n  left_join(\n    y = nodes |&gt; select(to = id, colour_fade),\n    by = \"to\"\n  )\n\n# view our edges data\nedges |&gt;\n  reactable(defaultPageSize = 5)\n\n\n\n\nStyling the edges dataframe\n\n\nWe now have stylised node and edge tables ready and can bring it all together. Note the use of customdata and hovertemplate help to bring in additional information and styling to the pop-up boxes that appear when you hover over each flow and node.\n\n\nCode\n# plot our stylised sankey\nplot_ly(\n  # setup\n  type = \"sankey\",\n  orientation = \"h\",\n  arrangement = \"snap\",\n\n  # use our node data\n  node = list(\n    label = nodes$name,\n    color = nodes$colour,\n    x = nodes$x,\n    y = nodes$y,\n    customdata = nodes$flow_perc,\n    hovertemplate = \"%{label}&lt;br /&gt;&lt;b&gt;%{value}&lt;/b&gt; participants&lt;br /&gt;&lt;b&gt;%{customdata}&lt;/b&gt; of eligible population\"\n  ),\n\n  # use our edge data\n  link = list(\n    source = edges$from,\n    target = edges$to,\n    value = edges$flow,\n    label = edges$label,\n    color = edges$colour_fade,\n    customdata = edges$flow_perc,\n    hovertemplate = \"%{source.label} ‚Üí %{target.label}&lt;br /&gt;&lt;b&gt;%{value}&lt;/b&gt; participants&lt;br /&gt;&lt;b&gt;%{customdata}&lt;/b&gt; of eligible population\"\n  )\n) |&gt;\n  layout(\n    font = list(\n      family = \"Arial, Helvetica, sans-serif\",\n      size = 12\n    ),\n    # make the background transparent (also removes the text shadow)\n    paper_bgcolor = \"rgba(0,0,0,0)\"\n  ) |&gt;\n  config(responsive = T)\n\n\n\n\nA stylish Sankey"
  },
  {
    "objectID": "blogs/posts/2024-05-13_one-year-coffee-code/index.html",
    "href": "blogs/posts/2024-05-13_one-year-coffee-code/index.html",
    "title": "One year of coffee & coding",
    "section": "",
    "text": "The data science team have been running coffee & coding sessions for just over a year now. When I joined that Strategy Unit, I was really pleased to see these sessions running as I think making time to discuss and share technical knowledge is highly valuable, especially as an organisation grows.\nCoffee and coding sessions run every two weeks and usually take the form of a short presentation, followed by a discussion. Although we have had a variety of different sessions including live coding demos and show and tell for projects.\nWe figured it would be a good idea to do a quick survey of attendees to make sure that the sessions were beneficial and see if there were any suggestions for future sessions. We had 11 responses, all of which were really positive, with 90% agreeing that the sessions are interesting, and over 80% saying that they learn new things. Respondents said that the sessions were well varied across the technical spectrum and that they ‚Äúalmost always learn something useful‚Äù.\nThe two main themes of the results were that sessions were inclusive and sparked collaboration. ‚ú®\n\nI like that everyone can contribute\n\n\nIt‚Äôs great seeing what else people are doing\n\n\nI get more ideas for future projects\n\nSome of the main suggestions included more content for newer programmers and encouraging the wider analytical team to share real project examples.\nSo with that, why not consider presenting? The sessions are informal and everyone is welcome to contribute. If you‚Äôve got something to share, please let a member of the data science team know.\nAs a reminder, materials for our previous sessions are available under Presentations."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#what-is-ai",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#what-is-ai",
    "title": "What is AI?",
    "section": "What is AI?",
    "text": "What is AI?\n\nThis is quite obviously a very large question\nIn a meeting of 5 data scientists there are 6 opinions\nI‚Äôm going to start overinclusive\nThen I‚Äôm going to give a quite narrow ‚ÄúWot I think‚Äù\nConcepts and vocabulary to reason about (and use, procure, produce‚Ä¶) AI"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#start-with-the-dictionary",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#start-with-the-dictionary",
    "title": "What is AI?",
    "section": "Start with the dictionary",
    "text": "Start with the dictionary\n\nMerriam Webster: Software designed to imitate intelligent aspects of human behavio[u]r\n\n\nWikipedia: [AI] is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#modern-ai",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#modern-ai",
    "title": "What is AI?",
    "section": "Modern AI",
    "text": "Modern AI\n\nWeb search engines\nRecommendation systems\nInteracting via human speech\nAutonomous vehicles\nGenerative AI\nStrategy games (e.g.¬†chess, go)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#early-history-of-ai",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#early-history-of-ai",
    "title": "What is AI?",
    "section": "Early history of AI",
    "text": "Early history of AI\n\n1930-1950\n\nThe Turing test\nManipulation of symbolic representations analogy for thought\nPsychology &lt;-&gt; Artifical intelligence"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#s-1960s",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#s-1960s",
    "title": "What is AI?",
    "section": "1950s & 1960s",
    "text": "1950s & 1960s\n\n1965, H. A. Simon: ‚Äúmachines will be capable, within twenty years, of doing any work a man can do.‚Äù\n\n\n1970, Marvin Minsky (in Life Magazine): ‚ÄúIn from three to eight years we will have a machine with the general intelligence of an average human being.‚Äù\n\n\nWork with perceptrons (single layer neural networks)\nCompeting with symbolic representation work"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#the-first-ai-winter",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#the-first-ai-winter",
    "title": "What is AI?",
    "section": "The first AI winter",
    "text": "The first AI winter\n\nEarly successes didn‚Äôt continue\nLimited computer power- many examples were ‚Äútoys‚Äù\nCombinatorial explosions- many problems could only be solved in exponential time\nRepresenting common sense reason and knowledge requires immense amounts of data"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#moravecs-paradox",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#moravecs-paradox",
    "title": "What is AI?",
    "section": "Moravec‚Äôs paradox",
    "text": "Moravec‚Äôs paradox\n\nEncoded in the large, highly evolved sensory and motor portions of the human brain is a billion years of experience about the nature of the world and how to survive in it‚Ä¶ We are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy. Abstract thought, though, is a new trick, perhaps less than 100 thousand years old. We have not yet mastered it. It is not all that intrinsically difficult; it just seems so when we do it"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#the-ai-effect",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#the-ai-effect",
    "title": "What is AI?",
    "section": "The ‚ÄúAI effect‚Äù",
    "text": "The ‚ÄúAI effect‚Äù\n\n‚ÄúThe AI effect‚Äù refers to a phenomenon where either the definition of AI or the concept of intelligence is adjusted to exclude capabilities that AI systems have mastered. This often manifests as tasks that AI can now perform successfully no longer being considered part of AI, or as the notion of intelligence itself being redefined to exclude AI achievements (wikipedia)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#the-coffee-test",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#the-coffee-test",
    "title": "What is AI?",
    "section": "The coffee test",
    "text": "The coffee test\n‚ÄúA machine is required to enter an average American home and figure out how to make coffee: find the coffee machine, find the coffee, add water, find a mug, and brew the coffee by pushing the proper buttons.‚Äù (source)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#a-narrower-definition-of-ai-for-us-to-use",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#a-narrower-definition-of-ai-for-us-to-use",
    "title": "What is AI?",
    "section": "A narrower definition of ‚ÄúAI‚Äù for us to use",
    "text": "A narrower definition of ‚ÄúAI‚Äù for us to use\n\nSearching the possibility space of the game of chess is not AI- because it doesn‚Äôt scale\nA robot coming to your house and making a cup of coffee is AI but it‚Äôs way more sophisticated than we need\nUseful AI systems today are basically something inbetween"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#machine-learning",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#machine-learning",
    "title": "What is AI?",
    "section": "Machine learning",
    "text": "Machine learning"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#right-so-really-what-is-ai",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#right-so-really-what-is-ai",
    "title": "What is AI?",
    "section": "Right, so, really, what is AI?",
    "text": "Right, so, really, what is AI?\n\nClassification is often called ‚ÄúAI‚Äù\nVarious types of classification problems:\n\nGiven a set of observations will this patient require hospitalisation?\nGiven a piece of patient feedback which of these 10 categories is it about?\nGiven an X ray is it probable that this patient has cancer?"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#no-really-tell-me-what-is-ai",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#no-really-tell-me-what-is-ai",
    "title": "What is AI?",
    "section": "No, really, tell me, what is AI?",
    "text": "No, really, tell me, what is AI?\n\nThere are two elements of a classification problem that can be ‚Äúdifficult‚Äù\n\nInput\nComputation\n\nI‚Äôd really like to avoid calling anything AI if one or both of those things is not complex"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#risk-score",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#risk-score",
    "title": "What is AI?",
    "section": "Risk score",
    "text": "Risk score\n\nSometimes you see any predictive algorithm as ‚ÄúAI‚Äù"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#deep-learning",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#deep-learning",
    "title": "What is AI?",
    "section": "Deep learning",
    "text": "Deep learning\n\nDeep learning is literally artificial intelligence\n\n\n(source)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#deep-learning-predictions",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#deep-learning-predictions",
    "title": "What is AI?",
    "section": "Deep learning predictions",
    "text": "Deep learning predictions\n\nDeep learning models meet a reasonable standard for ‚ÄúAI‚Äù\nThey model complex non linear relationships without any prior knowledge, rules, or models (a little bit like a human baby, in fact)\nDeep learning models for prediction can absorb large numbers of variables and produce predictions based on highly complex relationships within the data, merely by being trained appropriately"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#classifying-patient-experience-feedback",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#classifying-patient-experience-feedback",
    "title": "What is AI?",
    "section": "Classifying patient experience feedback",
    "text": "Classifying patient experience feedback\n\nComplex input\nFor comment theme- non complex computation (logistic regression)\nFor sentiment- transfer learning\nTheme does not require intelligence- linear models of word counts suffice\nSentiment does require pretrained deep learning models of language\nWe can see intuitively that detecting sentiment is ‚Äúharder‚Äù than theme"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#large-language-models",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#large-language-models",
    "title": "What is AI?",
    "section": "Large language models",
    "text": "Large language models\n\nPasses the Turing test (e.g.¬†ChatGPT)\nCan not make you coffee\nThe fundamental training is done by ‚Äúmasking‚Äù words and asking the model to predict them\nChatGPT has been called ‚Äúa stochastic parrot‚Äù\nDespite appearances, ChatGPT has no understanding other than a deep knowledge of the probabilistic structure of words in language"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#summary",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#summary",
    "title": "What is AI?",
    "section": "Summary",
    "text": "Summary\n\nAI is lots of things and has been called lots of things\nEven some apparently ‚Äúintelligent‚Äù tasks are not really so intelligent when you know how they work\nMy own view is that a useful definition of AI includes:\n\nModelling complex nonlinear systems without an explicit model\nDeep learning predictive algorithms meet this criterion, as do LLMs"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#intro",
    "href": "presentations/2024-12-04_what-the-heck/index.html#intro",
    "title": "What the heck do I do all day?",
    "section": "Intro",
    "text": "Intro\n\nFor analysts, there is a culture of ‚ÄúI want to progress without being a manager‚Äù\nMany analysts don‚Äôt want to and wouldn‚Äôt enjoy being a manager- and they should progress without being one\nBut many would, and I want to speak today to those who are thinking about being a manager, or already are"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#my-story",
    "href": "presentations/2024-12-04_what-the-heck/index.html#my-story",
    "title": "What the heck do I do all day?",
    "section": "My story",
    "text": "My story\n\nThe best lesson I ever learned\nSpent 15 years being a nerd and letting others lead\nI have never in my life had an analyst for a manager\nI fell into being a manager because I wanted to effect change at my previous employer"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#the-downside",
    "href": "presentations/2024-12-04_what-the-heck/index.html#the-downside",
    "title": "What the heck do I do all day?",
    "section": "The Downside",
    "text": "The Downside\n\nI don‚Äôt always understand everything that‚Äôs happening\n\nStill don‚Äôt know what databricks is üòÇ\n\nSome of the things I do are honestly kind of boring\nSome days I just ping between meetings all day and never catch myself up"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#what-the-heck-do-i-do-all-day",
    "href": "presentations/2024-12-04_what-the-heck/index.html#what-the-heck-do-i-do-all-day",
    "title": "What the heck do I do all day?",
    "section": "What the heck do I do all day",
    "text": "What the heck do I do all day\n\nProduct owner\nTeam leader\nLine manager\nMember of the SU leadership group\nHead of data science\nNumber one learning- be different people at different times"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#product-owner",
    "href": "presentations/2024-12-04_what-the-heck/index.html#product-owner",
    "title": "What the heck do I do all day?",
    "section": "Product owner",
    "text": "Product owner\n\nWe do kind of scrum\nI decide what‚Äôs in and what‚Äôs out\nNumber one learning- stay out of the way (a familiar lesson from others)\nLove: clarity; building a product people actually want; giving the team purpose and shaping what we do\nHate: I‚Äôm on the outside- don‚Äôt always understand stuff"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#team-leader",
    "href": "presentations/2024-12-04_what-the-heck/index.html#team-leader",
    "title": "What the heck do I do all day?",
    "section": "Team leader",
    "text": "Team leader\n\nRecruitment\nWhat do we do- where are we going\nCulture &gt; Strategy (#recruitforvalues)"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#line-manager",
    "href": "presentations/2024-12-04_what-the-heck/index.html#line-manager",
    "title": "What the heck do I do all day?",
    "section": "Line manager",
    "text": "Line manager\n\nThe research says line managers are really important\nI ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è hearing about what my staff are doing and where they want to be in five years\nI want to live in a world where they prosper, because they‚Äôre all awesome\n\n(remember #recruitforvalues)"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#member-of-the-su-leadership-group",
    "href": "presentations/2024-12-04_what-the-heck/index.html#member-of-the-su-leadership-group",
    "title": "What the heck do I do all day?",
    "section": "Member of the SU leadership group",
    "text": "Member of the SU leadership group\n\nHonestly I didn‚Äôt think I would be all that bothered (finance reports!)\nIt turns out I am bothered\nI love the Strategy Unit and I want to help it however I can"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#head-of-data-science",
    "href": "presentations/2024-12-04_what-the-heck/index.html#head-of-data-science",
    "title": "What the heck do I do all day?",
    "section": "Head of data science",
    "text": "Head of data science\n\nA bit like team leader\nBut actually data science needs a voice in the Unit\nI‚Äôm not the only voice but it‚Äôs my job to lead"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#why-im-a-terrible-manager",
    "href": "presentations/2024-12-04_what-the-heck/index.html#why-im-a-terrible-manager",
    "title": "What the heck do I do all day?",
    "section": "Why I‚Äôm a terrible manager",
    "text": "Why I‚Äôm a terrible manager\n\nI‚Äôm introverted\nI‚Äôm disorganised\nI have an opinion about everything and can‚Äôt keep my mouth shut\nI say what‚Äôs on my mind; I can‚Äôt help it"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#why-im-a-good-manager",
    "href": "presentations/2024-12-04_what-the-heck/index.html#why-im-a-good-manager",
    "title": "What the heck do I do all day?",
    "section": "Why I‚Äôm a good manager",
    "text": "Why I‚Äôm a good manager\n\nI‚Äôm introverted\nI‚Äôm disorganised\nI have an opinion about everything and can‚Äôt keep my mouth shut\nI say what‚Äôs on my mind; I can‚Äôt help it"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#the-upside",
    "href": "presentations/2024-12-04_what-the-heck/index.html#the-upside",
    "title": "What the heck do I do all day?",
    "section": "The upside",
    "text": "The upside\n\nPinging around hearing about different stuff is fun\nShaping the team and its projects is fun\nMostly for me this job is about making a difference\n\nRecruiting is miserable- hiring great staff is wonderful\nFaffing around with budgets is miserable- growing the team is wonderful\nSitting typing emails all day is meh- connecting people and projects is wonderful"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#its-a-unique-role",
    "href": "presentations/2024-12-04_what-the-heck/index.html#its-a-unique-role",
    "title": "What the heck do I do all day?",
    "section": "It‚Äôs a unique role",
    "text": "It‚Äôs a unique role\n\nEvery day I wake up and don‚Äôt help the team with the code for the sprint\nEvery day I think about what I can bring to the team\nEmbrace the uniqueness and give it a try"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#how-did-we-get-here",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#how-did-we-get-here",
    "title": "Agile and scrum working",
    "section": "How did we get here?",
    "text": "How did we get here?\n\nWaterfall approaches were used in the early days of software development\n\nRequirements; Design; Development; Integration; Testing; Deployment\n\nYou only move to the next stage when the first one is complete\n(although actually it turns out you kind of don‚Äôt‚Ä¶)"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#the-road-to-agile",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#the-road-to-agile",
    "title": "Agile and scrum working",
    "section": "The road to agile",
    "text": "The road to agile\n\nSome of the ideas for agile floated around in the 20th century\nShewart‚Äôs Plan-Do-Study-Act cycle\nThe New New Product Development Game in 1986\nScrum (which we‚Äôll return to) was proposed in 1993\nIn 2001 the Manifesto for Agile Software Development was published"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#the-agile-manifesto",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#the-agile-manifesto",
    "title": "Agile and scrum working",
    "section": "The agile manifesto",
    "text": "The agile manifesto\n\nCopyright ¬© 2001 Kent Beck, Mike Beedle, Arie van Bennekum, Alistair Cockburn, Ward Cunningham, Martin Fowler, James Grenning, Jim Highsmith, Andrew Hunt, Ron Jeffries, Jon Kern, Brian Marick\nRobert C. Martin, Steve Mellor, Ken Schwaber, Jeff Sutherland, Dave Thomas\nthis declaration may be freely copied in any form, but only in its entirety through this notice."
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#agile-principles--software-and-the-mvp",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#agile-principles--software-and-the-mvp",
    "title": "Agile and scrum working",
    "section": "Agile principles- software and the MVP",
    "text": "Agile principles- software and the MVP\n\nOur highest priority is to satisfy the customer through early and continuous delivery of valuable software.\nDeliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale.\nWorking software is the primary measure of progress.\n\n(these principles and those on following slides copyright Ibid.)"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#agile-principles--working-with-customers",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#agile-principles--working-with-customers",
    "title": "Agile and scrum working",
    "section": "Agile principles- working with customers",
    "text": "Agile principles- working with customers\n\nWelcome changing requirements, even late in development. Agile processes harness change for the customer‚Äôs competitive advantage.\nBusiness people and developers must work together daily throughout the project."
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#agile-principles--teamwork",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#agile-principles--teamwork",
    "title": "Agile and scrum working",
    "section": "Agile principles- teamwork",
    "text": "Agile principles- teamwork\n\nBuild projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done.\nThe most efficient and effective method of conveying information to and within a development team is face-to-face conversation.\nThe best architectures, requirements, and designs emerge from self-organizing teams.\nAt regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly."
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#agile-principles--project-management",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#agile-principles--project-management",
    "title": "Agile and scrum working",
    "section": "Agile principles- project management",
    "text": "Agile principles- project management\n\nAgile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely.\nContinuous attention to technical excellence and good design enhances agility.\nSimplicity‚Äìthe art of maximizing the amount of work not done‚Äìis essential."
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#the-agile-advantage",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#the-agile-advantage",
    "title": "Agile and scrum working",
    "section": "The agile advantage",
    "text": "The agile advantage\n\nBetter use of fixed resources to deliver an unknown outcome, rather than unknown resources to deliver a fixed outcome\nContinuous delivery"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#feature-creep",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#feature-creep",
    "title": "Agile and scrum working",
    "section": "Feature creep",
    "text": "Feature creep\n\nUsers ask for: everything they need, everything they think they may need, everything they want, everything they think they may want\n\n‚Äúevery program attempts to expand until it can read mail. Those programs which cannot so expand are replaced by ones which can‚Äù\n\nZawinski‚Äôs Law- Source"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#regular-stakeholder-feedback",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#regular-stakeholder-feedback",
    "title": "Agile and scrum working",
    "section": "Regular stakeholder feedback",
    "text": "Regular stakeholder feedback\n\nAgile teams are very responsive to product feedback\nThe project we‚Äôre curently working on is very agile whether we like it or not\nOur customers never know what they want until we show them something they don‚Äôt want"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#more-agile-advantages",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#more-agile-advantages",
    "title": "Agile and scrum working",
    "section": "More agile advantages",
    "text": "More agile advantages\n\nEarly and cheap failure\nContinuous testing and QA\nReduction in unproductive work\nTeam can improve regularly, not just the product"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#agile-methods",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#agile-methods",
    "title": "Agile and scrum working",
    "section": "Agile methods",
    "text": "Agile methods\n\nThere are lots of agile methodologies\nI‚Äôm not going to embarrass myself by pretending to understand them\nExamples include Lean, Crystal, and Extreme Programming"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#scrum",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#scrum",
    "title": "Agile and scrum working",
    "section": "Scrum",
    "text": "Scrum\n\nScrum is the agile methodology we have adopted\nDespite dire warnings to the contrary we have not adopted it wholesale but most of its principles\nThe fundamental organising principle of work in scrum is a sprint lasting 1-4 weeks\nEach sprint finishes with a defined and useful piece of software that can be shown to/ used by customers"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#product-owner",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#product-owner",
    "title": "Agile and scrum working",
    "section": "Product owner",
    "text": "Product owner\n\nThis person is responsible for the backlog- what goes in to the sprint\nThe backlog should be inclusive of all of the things that customers want or might want\nThe backlog should be prioritised\nThe product owner does this through deep and frequent conversations with customers"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#scrum-master-helps-the-scrum-team",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#scrum-master-helps-the-scrum-team",
    "title": "Agile and scrum working",
    "section": "Scrum master helps the scrum team",
    "text": "Scrum master helps the scrum team\n\n‚ÄúBy coaching the team members in self-management and cross-functionality\nFocus on creating high-value Increments that meet the Definition of Done\nInfluence the removal of impediments to the Scrum Team‚Äôs progress\nEnsure that all Scrum events take place and are positive, productive, and kept within the timebox.‚Äù\n\nSource"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#the-backlog",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#the-backlog",
    "title": "Agile and scrum working",
    "section": "The backlog",
    "text": "The backlog\n\nHaving an accurate and well prioritised backlog is key\nDon‚Äôt estimate the backlog in hours- use ‚ÄúT shirt sizes‚Äù or ‚Äúpoints‚Äù\nPeople are terrible at estimating how long things take- particularly in software\nEverything in the backlog needs a defined ‚ÄúDone‚Äù state"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#sprint-planning",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#sprint-planning",
    "title": "Agile and scrum working",
    "section": "Sprint planning",
    "text": "Sprint planning\n\nThe team, the product owner, and the scrum master plan the sprint\nSprints should be a fixed length of time less than one month\nThe sprint cannot be changed or added to (we break this rule)\nThe team works autonomously in the sprint- nobody decides who does what except the team\nCan take three hours and should if it needs to"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#standup",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#standup",
    "title": "Agile and scrum working",
    "section": "Standup",
    "text": "Standup\n\nEvery day, for no more than 15 minutes (teams often stand up to reinforce this rule) team and scrum master meet\nEach person answers three questions\n\nWhat did you do yesterday to help the team finish the sprint?\nWhat will you do today to help the team finish the sprint?\nIs there an obstacle blocking you or the team from achieveing the sprint goal"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#sprint-retro",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#sprint-retro",
    "title": "Agile and scrum working",
    "section": "Sprint retro",
    "text": "Sprint retro\n\nWhat went well, what could have gone better, and what to improve next time\nLooking at process, not blaming individuals\nRequires maturity and trust to bring up issues, and to respond to them in a constructive way\nShould agree at the end on one process improvement which goes in the next sprint\nWe‚Äôve had some really, really good retros and I think it‚Äôs a really important process for a team"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#team-perspective",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#team-perspective",
    "title": "Agile and scrum working",
    "section": "Team perspective",
    "text": "Team perspective\n\nProduct owner- that‚Äôs me\n\nFocus, clarity and transparency, team delivery, clear and appropriate responsibilities\n\nScrum master- YiWen\nTeam member- Matt\nTeam member- Rhian"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#scrum-values",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#scrum-values",
    "title": "Agile and scrum working",
    "section": "Scrum values",
    "text": "Scrum values\n\nCourage\nFocus\nCommitment\nRespect\nOpenness"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#using-agile-outside-of-software",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#using-agile-outside-of-software",
    "title": "Agile and scrum working",
    "section": "Using agile outside of software",
    "text": "Using agile outside of software\n\nData science is outside of software (IMHO)\n\nWe don‚Äôt have daily standups and some of our processes run longer than in software development\n\nYou can build cars with Agile\nMarketing and UX design"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#packages-we-are-using-today",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#packages-we-are-using-today",
    "title": "Coffee and Coding",
    "section": "Packages we are using today",
    "text": "Packages we are using today\n\nlibrary(tidyverse)\n\nlibrary(sf)\n\nlibrary(tidygeocoder)\nlibrary(PostcodesioR)\n\nlibrary(osrm)\n\nlibrary(leaflet)"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#getting-boundary-data",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#getting-boundary-data",
    "title": "Coffee and Coding",
    "section": "Getting boundary data",
    "text": "Getting boundary data\nWe can use the ONS‚Äôs Geoportal we can grab boundary data to generate maps\n\n\n\nicb_url &lt;- paste0(\n  \"https://services1.arcgis.com\",\n  \"/ESMARspQHYMw9BZ9/arcgis\",\n  \"/rest/services\",\n  \"/Integrated_Care_Boards_April_2023_EN_BGC\",\n  \"/FeatureServer/0/query\",\n  \"?outFields=*&where=1%3D1&f=geojson\"\n)\nicb_boundaries &lt;- read_sf(icb_url)\n\nicb_boundaries |&gt;\n  ggplot() +\n  geom_sf() +\n  theme_void()"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-is-the-icb_boundaries-data",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-is-the-icb_boundaries-data",
    "title": "Coffee and Coding",
    "section": "What is the icb_boundaries data?",
    "text": "What is the icb_boundaries data?\n\nicb_boundaries |&gt;\n  select(ICB23CD, ICB23NM)\n\nSimple feature collection with 42 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -6.418667 ymin: 49.86479 xmax: 1.763706 ymax: 55.81112\nGeodetic CRS:  WGS 84\n# A tibble: 42 √ó 3\n   ICB23CD   ICB23NM                                                    geometry\n   &lt;chr&gt;     &lt;chr&gt;                                            &lt;MULTIPOLYGON [¬∞]&gt;\n 1 E54000008 NHS Cheshire and Merseyside Integrated C‚Ä¶ (((-3.083264 53.2559, -3‚Ä¶\n 2 E54000010 NHS Staffordshire and Stoke-on-Trent Int‚Ä¶ (((-1.950489 53.21188, -‚Ä¶\n 3 E54000011 NHS Shropshire, Telford and Wrekin Integ‚Ä¶ (((-2.380794 52.99841, -‚Ä¶\n 4 E54000013 NHS Lincolnshire Integrated Care Board    (((0.2687853 52.81584, 0‚Ä¶\n 5 E54000015 NHS Leicester, Leicestershire and Rutlan‚Ä¶ (((-0.7875237 52.97762, ‚Ä¶\n 6 E54000018 NHS Coventry and Warwickshire Integrated‚Ä¶ (((-1.577608 52.67858, -‚Ä¶\n 7 E54000019 NHS Herefordshire and Worcestershire Int‚Ä¶ (((-2.272042 52.43972, -‚Ä¶\n 8 E54000022 NHS Norfolk and Waveney Integrated Care ‚Ä¶ (((1.666741 52.31366, 1.‚Ä¶\n 9 E54000023 NHS Suffolk and North East Essex Integra‚Ä¶ (((0.8997023 51.7732, 0.‚Ä¶\n10 E54000024 NHS Bedfordshire, Luton and Milton Keyne‚Ä¶ (((-0.4577115 52.32009, ‚Ä¶\n# ‚Ñπ 32 more rows"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-dataframes",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-dataframes",
    "title": "Coffee and Coding",
    "section": "Working with geospatial dataframes",
    "text": "Working with geospatial dataframes\nWe can simply join sf data frames and ‚Äúregular‚Äù data frames together\n\n\n\nicb_metrics &lt;- icb_boundaries |&gt;\n  st_drop_geometry() |&gt;\n  select(ICB23CD) |&gt;\n  mutate(admissions = rpois(n(), 1000000))\n\nicb_boundaries |&gt;\n  inner_join(icb_metrics, by = \"ICB23CD\") |&gt;\n  ggplot() +\n  geom_sf(aes(fill = admissions)) +\n  scale_fill_viridis_c() +\n  theme_void()"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-data-frames",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-data-frames",
    "title": "Coffee and Coding",
    "section": "Working with geospatial data frames",
    "text": "Working with geospatial data frames\nWe can manipulate sf objects like other data frames\n\n\n\nlondon_icbs &lt;- icb_boundaries |&gt;\n  filter(ICB23NM |&gt; stringr::str_detect(\"London\"))\n\nggplot() +\n  geom_sf(data = london_icbs) +\n  geom_sf(data = st_centroid(london_icbs)) +\n  theme_void()"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-data-frames-1",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-data-frames-1",
    "title": "Coffee and Coding",
    "section": "Working with geospatial data frames",
    "text": "Working with geospatial data frames\nSummarising the data will combine the geometries.\n\nlondon_icbs |&gt;\n  summarise(area = sum(Shape__Area)) |&gt;\n  # and use geospatial functions to create calculations using the geometry\n  mutate(new_area = st_area(geometry), .before = \"geometry\")\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -0.5102803 ymin: 51.28676 xmax: 0.3340241 ymax: 51.69188\nGeodetic CRS:  WGS 84\n# A tibble: 1 √ó 3\n         area    new_area                                               geometry\n*       &lt;dbl&gt;       [m^2]                                     &lt;MULTIPOLYGON [¬∞]&gt;\n1 1573336388. 1567995610. (((-0.3314819 51.43935, -0.3306676 51.43889, -0.33118‚Ä¶\n\n\n Why the difference in area?\n\n We are using a simplified geometry, so calculating the area will be slightly inaccurate. The original area was calculated on the non-simplified geometries."
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#creating-our-own-geospatial-data",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#creating-our-own-geospatial-data",
    "title": "Coffee and Coding",
    "section": "Creating our own geospatial data",
    "text": "Creating our own geospatial data\n\nlocation_raw &lt;- postcode_lookup(\"B2 4BJ\")\nglimpse(location_raw)\n\nRows: 1\nColumns: 40\n$ postcode                             &lt;chr&gt; \"B2 4BJ\"\n$ quality                              &lt;int&gt; 1\n$ eastings                             &lt;int&gt; 406866\n$ northings                            &lt;int&gt; 286775\n$ country                              &lt;chr&gt; \"England\"\n$ nhs_ha                               &lt;chr&gt; \"West Midlands\"\n$ longitude                            &lt;dbl&gt; -1.90033\n$ latitude                             &lt;dbl&gt; 52.47887\n$ european_electoral_region            &lt;chr&gt; \"West Midlands\"\n$ primary_care_trust                   &lt;chr&gt; \"Heart of Birmingham Teaching\"\n$ region                               &lt;chr&gt; \"West Midlands\"\n$ lsoa                                 &lt;chr&gt; \"Birmingham 138A\"\n$ msoa                                 &lt;chr&gt; \"Birmingham 138\"\n$ incode                               &lt;chr&gt; \"4BJ\"\n$ outcode                              &lt;chr&gt; \"B2\"\n$ parliamentary_constituency           &lt;chr&gt; \"Birmingham Ladywood\"\n$ parliamentary_constituency_2024      &lt;chr&gt; \"Birmingham Ladywood\"\n$ admin_district                       &lt;chr&gt; \"Birmingham\"\n$ parish                               &lt;chr&gt; \"Birmingham, unparished area\"\n$ admin_county                         &lt;lgl&gt; NA\n$ date_of_introduction                 &lt;chr&gt; \"198001\"\n$ admin_ward                           &lt;chr&gt; \"Ladywood\"\n$ ced                                  &lt;lgl&gt; NA\n$ ccg                                  &lt;chr&gt; \"NHS Birmingham and Solihull\"\n$ nuts                                 &lt;chr&gt; \"Birmingham\"\n$ pfa                                  &lt;chr&gt; \"West Midlands\"\n$ admin_district_code                  &lt;chr&gt; \"E08000025\"\n$ admin_county_code                    &lt;chr&gt; \"E99999999\"\n$ admin_ward_code                      &lt;chr&gt; \"E05011151\"\n$ parish_code                          &lt;chr&gt; \"E43000250\"\n$ parliamentary_constituency_code      &lt;chr&gt; \"E14001096\"\n$ parliamentary_constituency_2024_code &lt;chr&gt; \"E14001096\"\n$ ccg_code                             &lt;chr&gt; \"E38000258\"\n$ ccg_id_code                          &lt;chr&gt; \"15E\"\n$ ced_code                             &lt;chr&gt; \"E99999999\"\n$ nuts_code                            &lt;chr&gt; \"TLG31\"\n$ lsoa_code                            &lt;chr&gt; \"E01033620\"\n$ msoa_code                            &lt;chr&gt; \"E02006899\"\n$ lau2_code                            &lt;chr&gt; \"E08000025\"\n$ pfa_code                             &lt;chr&gt; \"E23000014\"\n\n\n\n\n\nlocation &lt;- location_raw |&gt;\n  st_as_sf(coords = c(\"eastings\", \"northings\"), crs = 27700) |&gt;\n  select(postcode, ccg) |&gt;\n  st_transform(crs = 4326)\n\nlocation\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -1.900335 ymin: 52.47886 xmax: -1.900335 ymax: 52.47886\nGeodetic CRS:  WGS 84\n  postcode                         ccg                   geometry\n1   B2 4BJ NHS Birmingham and Solihull POINT (-1.900335 52.47886)"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#creating-a-geospatial-data-frame-for-all-nhs-trusts",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#creating-a-geospatial-data-frame-for-all-nhs-trusts",
    "title": "Coffee and Coding",
    "section": "Creating a geospatial data frame for all NHS Trusts",
    "text": "Creating a geospatial data frame for all NHS Trusts\n\n\n\n# using the NHSRtools package\n# remotes::install_github(\"NHS-R-Community/NHSRtools\")\ntrusts &lt;- ods_get_trusts() |&gt;\n  filter(status == \"Active\") |&gt;\n  select(name, org_id, post_code) |&gt;\n  geocode(postalcode = \"post_code\") |&gt;\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326)\n\n\ntrusts |&gt;\n  leaflet() |&gt;\n  addProviderTiles(\"Stamen.TonerLite\") |&gt;\n  addMarkers(popup = ~name)"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-are-the-nearest-trusts-to-our-location",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-are-the-nearest-trusts-to-our-location",
    "title": "Coffee and Coding",
    "section": "What are the nearest trusts to our location?",
    "text": "What are the nearest trusts to our location?\n\nnearest_trusts &lt;- trusts |&gt;\n  mutate(\n    distance = st_distance(geometry, location)[, 1]\n  ) |&gt;\n  arrange(distance) |&gt;\n  head(5)\n\nnearest_trusts\n\nSimple feature collection with 5 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -1.9384 ymin: 52.4533 xmax: -1.886282 ymax: 52.48764\nGeodetic CRS:  WGS 84\n# A tibble: 5 √ó 5\n  name                       org_id post_code             geometry distance\n  &lt;chr&gt;                      &lt;chr&gt;  &lt;chr&gt;              &lt;POINT [¬∞]&gt;      [m]\n1 BIRMINGHAM WOMEN'S AND CH‚Ä¶ RQ3    B4 6NH     (-1.894241 52.4849)     789.\n2 BIRMINGHAM AND SOLIHULL M‚Ä¶ RXT    B1 3RB    (-1.917663 52.48416)    1313.\n3 BIRMINGHAM COMMUNITY HEAL‚Ä¶ RYW    B7 4BN    (-1.886282 52.48754)    1356.\n4 SANDWELL AND WEST BIRMING‚Ä¶ RXK    B18 7QH   (-1.930203 52.48764)    2246.\n5 UNIVERSITY HOSPITALS BIRM‚Ä¶ RRK    B15 2GW      (-1.9384 52.4533)    3838."
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#lets-find-driving-routes-to-these-trusts",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#lets-find-driving-routes-to-these-trusts",
    "title": "Coffee and Coding",
    "section": "Let‚Äôs find driving routes to these trusts",
    "text": "Let‚Äôs find driving routes to these trusts\n\nroutes &lt;- nearest_trusts |&gt;\n  mutate(\n    route = map(geometry, ~ osrmRoute(location, st_coordinates(.x)))\n  ) |&gt;\n  st_drop_geometry() |&gt;\n  rename(straight_line_distance = distance) |&gt;\n  unnest(route) |&gt;\n  st_as_sf()\n\nroutes\n\nSimple feature collection with 5 features and 8 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -1.93846 ymin: 52.45316 xmax: -1.88527 ymax: 52.49279\nGeodetic CRS:  WGS 84\n# A tibble: 5 √ó 9\n  name     org_id post_code straight_line_distance src   dst   duration distance\n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;                        [m] &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 BIRMING‚Ä¶ RQ3    B4 6NH                      789. 1     dst       5.81     3.09\n2 BIRMING‚Ä¶ RXT    B1 3RB                     1313. 1     dst       6.87     4.14\n3 BIRMING‚Ä¶ RYW    B7 4BN                     1356. 1     dst       7.63     4.29\n4 SANDWEL‚Ä¶ RXK    B18 7QH                    2246. 1     dst       8.81     4.95\n5 UNIVERS‚Ä¶ RRK    B15 2GW                    3838. 1     dst      10.6      4.85\n# ‚Ñπ 1 more variable: geometry &lt;LINESTRING [¬∞]&gt;"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#lets-show-the-routes",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#lets-show-the-routes",
    "title": "Coffee and Coding",
    "section": "Let‚Äôs show the routes",
    "text": "Let‚Äôs show the routes\n\nleaflet(routes) |&gt;\n  addTiles() |&gt;\n  addMarkers(data = location) |&gt;\n  addPolylines(color = \"black\", weight = 3, opacity = 1) |&gt;\n  addCircleMarkers(data = nearest_trusts, radius = 4, opacity = 1, fillOpacity = 1)"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#we-can-use-osrm-to-calculate-isochrones",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#we-can-use-osrm-to-calculate-isochrones",
    "title": "Coffee and Coding",
    "section": "We can use {osrm} to calculate isochrones",
    "text": "We can use {osrm} to calculate isochrones\n\n\n\niso &lt;- osrmIsochrone(location, breaks = seq(0, 60, 15), res = 10)\n\nisochrone_ids &lt;- unique(iso$id)\n\npal &lt;- colorFactor(\n  viridisLite::viridis(length(isochrone_ids)),\n  isochrone_ids\n)\n\nleaflet(location) |&gt;\n  addProviderTiles(\"Stamen.TonerLite\") |&gt;\n  addMarkers() |&gt;\n  addPolygons(\n    data = iso,\n    fillColor = ~ pal(id),\n    color = \"#000000\",\n    weight = 1\n  )"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones",
    "title": "Coffee and Coding",
    "section": "What trusts are in the isochrones?",
    "text": "What trusts are in the isochrones?\nThe summarise() function will ‚Äúunion‚Äù the geometry\n\nsummarise(iso)\n\nSimple feature collection with 1 feature and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -2.913575 ymin: 51.98094 xmax: -0.8505441 ymax: 53.10806\nGeodetic CRS:  WGS 84\n                        geometry\n1 POLYGON ((-1.541014 52.9691..."
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones-1",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones-1",
    "title": "Coffee and Coding",
    "section": "What trusts are in the isochrones?",
    "text": "What trusts are in the isochrones?\nWe can use this with a geo-filter to find the trusts in the isochrone\n\n# also works\ntrusts_in_iso &lt;- trusts |&gt;\n  st_filter(\n    summarise(iso),\n    .predicate = st_within\n  )\n\ntrusts_in_iso\n\nSimple feature collection with 31 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -2.793386 ymin: 52.19205 xmax: -1.10302 ymax: 53.01015\nGeodetic CRS:  WGS 84\n# A tibble: 31 √ó 4\n   name                               org_id post_code             geometry\n * &lt;chr&gt;                              &lt;chr&gt;  &lt;chr&gt;              &lt;POINT [¬∞]&gt;\n 1 BIRMINGHAM AND SOLIHULL MENTAL HE‚Ä¶ RXT    B1 3RB    (-1.917663 52.48416)\n 2 BIRMINGHAM COMMUNITY HEALTHCARE N‚Ä¶ RYW    B7 4BN    (-1.886282 52.48754)\n 3 BIRMINGHAM WOMEN'S AND CHILDREN'S‚Ä¶ RQ3    B4 6NH     (-1.894241 52.4849)\n 4 BIRMINGHAM WOMEN'S NHS FOUNDATION‚Ä¶ RLU    B15 2TG   (-1.942861 52.45325)\n 5 BURTON HOSPITALS NHS FOUNDATION T‚Ä¶ RJF    DE13 0RB  (-1.656667 52.81774)\n 6 COVENTRY AND WARWICKSHIRE PARTNER‚Ä¶ RYG    CV6 6NY    (-1.48692 52.45659)\n 7 DERBYSHIRE HEALTHCARE NHS FOUNDAT‚Ä¶ RXM    DE22 3LZ  (-1.512896 52.91831)\n 8 DUDLEY INTEGRATED HEALTH AND CARE‚Ä¶ RYK    DY5 1RU    (-2.11786 52.48176)\n 9 GEORGE ELIOT HOSPITAL NHS TRUST    RLT    CV10 7DJ   (-1.47844 52.51258)\n10 HEART OF ENGLAND NHS FOUNDATION T‚Ä¶ RR1    B9 5ST     (-1.828759 52.4781)\n# ‚Ñπ 21 more rows"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones-2",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones-2",
    "title": "Coffee and Coding",
    "section": "What trusts are in the isochrones?",
    "text": "What trusts are in the isochrones?\n\n\n\nleaflet(trusts_in_iso) |&gt;\n  addProviderTiles(\"Stamen.TonerLite\") |&gt;\n  addMarkers() |&gt;\n  addPolygons(\n    data = iso,\n    fillColor = ~pal(id),\n    color = \"#000000\",\n    weight = 1\n  )"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#doing-the-same-but-within-a-radius",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#doing-the-same-but-within-a-radius",
    "title": "Coffee and Coding",
    "section": "Doing the same but within a radius",
    "text": "Doing the same but within a radius\n\n\n\nr &lt;- 25000\n\ntrusts_in_radius &lt;- trusts |&gt;\n  st_filter(\n    location,\n    .predicate = st_is_within_distance,\n    dist = r\n  )\n\n# transforming gives us a pretty smooth circle\nradius &lt;- location |&gt;\n  st_transform(crs = 27700) |&gt;\n  st_buffer(dist = r) |&gt;\n  st_transform(crs = 4326)\n\nleaflet(trusts_in_radius) |&gt;\n  addProviderTiles(\"Stamen.TonerLite\") |&gt;\n  addMarkers() |&gt;\n  addPolygons(\n    data = radius,\n    color = \"#000000\",\n    weight = 1\n  )"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#further-reading",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#further-reading",
    "title": "Coffee and Coding",
    "section": "Further reading",
    "text": "Further reading\n\nGeocomputation with R\nr-spatial\n{sf} documentation\nLeaflet documentation\nTidy Geospatial Networks in R"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#generative-ai",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#generative-ai",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Generative AI ‚ú®",
    "text": "Generative AI ‚ú®\n\nCreates new content\nTrained on lots of examples\nCan mimic creativity\n\n\n\nGenerative AI uses patterns from data (like text, images, or sound) to create new, similar content.\nIt‚Äôs trained on large datasets and then generates things based on what it has learned.\nCan write stories, make art, create music, or even simulate human conversations."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#modalities",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#modalities",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Modalities üñºÔ∏è",
    "text": "Modalities üñºÔ∏è\n\n\n\nImages (e.g.¬†DALL-E)\nAudio/video (e.g.¬†inVideo AI)\nText (e.g.¬†ChatGPT)\n\n\n\n\n\n\nWritten content like stories, code or conversations.\nVisuals, from drawings to photorealistic images.\nMusic, voices, or even entire videos with sound and motion.\nThese slides are about that last one, text, via Large Language Models (LLMs)."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#text",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#text",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Text üî§",
    "text": "Text üî§\n\nSpam filters\nSentiment\nTopic detection\nWord prediction\n‚Ä¶\n\n\n\nBefore we get into LLMs, it‚Äôs worth recognising how many tools we use that involve some kind of text processing (perhaps AI driven).\nPattern detection, comparison to previous spam, keyword detection.\nSummarising customer reviews as positive or negative (sentiment analysis).\nAutomatically identify key themes or topics within large collections of unstructured text (e.g.¬†topic modelling)."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#large-language-models-llms",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#large-language-models-llms",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Large Language Models (LLMs) ü¶ú",
    "text": "Large Language Models (LLMs) ü¶ú\n\nA (fancy?) parrot\nLearns from lots of text\nPredicts next word\n\n\n\n\nBreaks down text: the model processes input text by breaking it into smaller units (like words or subwords).\nFinds patterns: it learns relationships between the units.\nGenerates predictions: based on the input, it predicts the most likely next word or phrase to generate coherent responses or text."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#in-healthcare",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#in-healthcare",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "In healthcare üè•",
    "text": "In healthcare üè•\n\nOrganise medical documents\nDrug discovery research\nChatbots\n‚Ä¶\n\n\n\nAutomatically generate, organise, summarise patient notes.\nAnalyse medical literature and data to help identify new potential treatments or drug interactions.\nProvide 24/7 support by answering common medical questions and guiding patients through symptom checks."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#irl-models",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#irl-models",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "IRL models ü§ñ",
    "text": "IRL models ü§ñ\n\nHealthcare-focused models\nGoogle‚Äôs MedLM\nNature: LLMs in medicine and the future landscape\n\n\n\nSpecifically-developed LLMs for healthcare applications already exist, many are openly available.\nGoogle‚Äôs MedLM has a price tag.\nThere‚Äôs a couple of Nature articles on current applications and potential future landscape."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#case-study",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#case-study",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Case study üß†",
    "text": "Case study üß†\n\nNHS: chatbot for mental health referrals\nUsed Limbic ‚ÄòAccess‚Äô ‚Äòe-triage‚Äô chatbot\nAccording to Limbic:\n\n\n‚ÄòNearly 40% of NHS Talking Therapies already trust Limbic to improve their services‚Äô\n\n\n\nThere‚Äôs an admin burden.\nTook place in the Surrey and Borders Partnership NHS Foundation Trust"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#case-study-1",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#case-study-1",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Case study üß†",
    "text": "Case study üß†\n\n\nPatient\n\n\nClinician\n\n\n\nFrom the NHS-E Transformation Directorate write-up:\n~99% of patients that left feedback said that Limbic was helpful\nthe service has seen a 30% increase in referrals and initial evidence indicates that Limbic improved out of hours access\non a pro-rata basis, a saving of 3000 hours (4 psychological wellbeing practitioners)\nnearly 20% of referrals were identified as ineligible and signposted to a more appropriate service"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#but",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#but",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "But‚Ä¶ ‚ö†Ô∏è",
    "text": "But‚Ä¶ ‚ö†Ô∏è\nThe effect of using a large language model to respond to patient messages (The Lancet)\n\n‚Ä¶raises the question of the extent to which LLM assistance is decision support versus LLM-based decision making\n\n\n‚Ä¶a minority of LLM drafts, if left unedited, could lead to severe harm or death\n\n\n\nBrigham and Women‚Äôs Hospital, USA.\nInvestigated how ‚ÄòLLM [GPT-4] assistance for electronic patient portal messaging in electronic health record systems (ie, using an LLM to draft a response for a clinician to edit) might impact subjective efficiency, clinical recommendations, and potential harms‚Äô for cancer patients.\nFound safety errors, and in one instance the advice given to a patient could have been fatal."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#pros",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#pros",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Pros ‚ûï",
    "text": "Pros ‚ûï\n\nFor providers: could reduce pressure\nFor users: increases service accessibility\nCan be trained for domain specificity\n\n\n\nCould ease pressure by providing info/assistance to users anytime, making services like customer support available 24/7.\nHealthcare is very broad, but we can train on specific subsets of information to tailor the outcome. However, we do need plenty of data."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#cons",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#cons",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Cons ‚ûñ",
    "text": "Cons ‚ûñ\n\nEthical issues, like:\n\nbias\ncomputational cost\ndata origins\nprivacy\n\nNot human\nIt lies\n\n\n\nSometimes gives wrong or confusing information, especially on complex topics. May provide inaccurate medical advice or information, leading to potential misdiagnoses or harmful decisions.\n‚ÄòHallucination‚Äô is perhaps a weasel word.\nCan reflect biases found in the data it was trained on, leading to unfair responses that could be plain wrong for the target audience.\nLack of accountability: it can be unclear who is responsible, complicating patient care and trust issues.\nRequires significant computational power and resources, which can be expensive and environmentally taxing.\nDoesn‚Äôt understand emotions, context, or nuance, which are all important in clinical settings."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#consider",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#consider",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Consider ü§î",
    "text": "Consider ü§î\n\nAre there legal issues?\nHave you considered user needs?\nShould you follow policies (e.g.¬†HM gov, NHS, trust)?\n\n\n\nCould your product technically be a medical device, for example?\nHave you performed a data protection impact assessment?\nAre there more classic, better understood AI approaches you could use? NLP?\nTest the limits of the system; where is it likely to fail or give bad information?\nProvide an alternative to users in certain cases (e.g.¬†non-chatbot given the chance of ‚Äòconversation loops‚Äô).\nAre you thinking about the humans at the end of the process? The users of your service?"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#to-ponder",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#to-ponder",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "To ponder ‚ùì",
    "text": "To ponder ‚ùì\n\nIs this AI?\nAre LLMs an appropriate tool in healthcare?\nHow might you feel interacting with an LLM-driven service?\nHow can we protect patient privacy?\nHow do we deal with LLMs as tools for decision support vs decision making?\nWho is responsible for errors, or even death?"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#further-reading",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#further-reading",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Further reading üìö",
    "text": "Further reading üìö\n\nNHS Knowledge and Library Services: AI\n3 Blue 1 Brown (YouTube)\nComputerphile (YouTube)\nGOV.UK chatbot experiment"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#intro",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#intro",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Intro",
    "text": "Intro\n\nDesign of the application\nDeployment and use\nUpdate and maintenance\nSome slides reused from Tom Jemmett‚Äôs HACA presentation"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#model-overview",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#model-overview",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Model Overview",
    "text": "Model Overview\n\nThe baseline data is a year worth of a provider‚Äôs HES data\nEach row in the baseline data is run through a series of steps\nEach step creates a factor that says how many times (on average) to sample that row\nThe factors are multiplied together and used to create a random Poisson value\nWe resample the rows using this random values\nEfficiencies are then applied, e.g.¬†LoS reductions, type conversions"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#monte-carlo-simulation",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#monte-carlo-simulation",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Monte Carlo Simulation",
    "text": "Monte Carlo Simulation\n\nWe run the model N times, varying the input parameters each time slightly to handle the uncertainty.\nThe results of the model are aggregated at the end of each model run\nThe aggregated results are combined at the end into a single file"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#prequisites",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#prequisites",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Prequisites",
    "text": "Prequisites\n\nRobust to updated model releases\nReproducibility- across multiple versions\nSpeed- for the model and the reporting\nInterpretation of complex output"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#data-sources",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#data-sources",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Data sources",
    "text": "Data sources\n\nThe usual rule is that 80% of the work is on data\nNot a problem for us because we‚Äôre using trusted, curated data (HES)\nSo a mere 80% of our work is on data\nWait, what?"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#data-pipelines",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#data-pipelines",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Data pipelines",
    "text": "Data pipelines\n\nRAP is obviously crucial\n\nReproducibility is essential\nUndocumented mistakes severe (there are always bugs üôÇ)\nSpeed and sharing within the project team\n\nThe original data pipelines were SQL\n\nUpdates took 4 days with frequent hangs and crashes\n\nThey now execute in 30 minutes on databricks"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#the-model-itself",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#the-model-itself",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "The model itself",
    "text": "The model itself\n\nWritten in Python for speed\nAn API spins up a Docker instance to run the model on demand (~ 5 minutes)\nUses .parquet, also for speed\nThe model outputs row level data conceptually, if not actually"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#outputs",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#outputs",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Outputs",
    "text": "Outputs\n\n‚ÄúFinal report‚Äù Word document, bespoke to current context\n‚ÄúOutputs dashboard‚Äù - containing high level summaries\nDetailed model results - we offer a range of outputs but can always make more"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#overview",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#overview",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Overview",
    "text": "Overview"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#uses",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#uses",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Uses",
    "text": "Uses\n\nSizing hospitals\nSizing left shift\nPredicting, comparing, and monitoring activity mitigation"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#the-process",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#the-process",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "The process",
    "text": "The process\n\nJanuary 2023: Development phase\nApril-ish: Thinking about deployment\nOctober-ish: Model is on its way to full production and much remains to do"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#operational-mode",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#operational-mode",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Operational mode",
    "text": "Operational mode\n\nThe team was growing in November\nThere were two priorities\n\nIncrease bus factor (which I‚Äôll come back to)\nDeploy the first production ready version"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#the-first-deployed-version",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#the-first-deployed-version",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "The first deployed version",
    "text": "The first deployed version\n\nDecember-ish: Many needs that were not anticipated\nThis first release kicked off lots of other work\nThe second release kicked off lots of other work\nIt was very hard to do any long term planning"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#scrum",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#scrum",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Scrum",
    "text": "Scrum\n\nWe are not doing ‚Äúproper‚Äù scrum\nProduct owner, scrum master, everyone else\nFive week sprints with a one week recovery run between each one\nSprint planning, sprint catchup, sprint retro"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#sprint-retro",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#sprint-retro",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Sprint retro",
    "text": "Sprint retro\n\nWhat went well, what could have gone better, and what to improve next time\nLooking at process, not blaming individuals\nRequires maturity and trust to bring up issues, and to respond to them in a constructive way\nShould agree at the end on one process improvement which goes in the next sprint\nWe‚Äôve had some really, really good retros and I think it‚Äôs a really important process for a team"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#what-did-scrum-give-the-team",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#what-did-scrum-give-the-team",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "What did scrum give the team?",
    "text": "What did scrum give the team?\n\nSimultaneous releases of linked repos\nThe team works autonomously in the sprint\nBetter conversations about ‚Äúno‚Äù\nThe planning and retro process improves the team‚Äôs processes, not just the code"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#product-owner",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#product-owner",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Product owner",
    "text": "Product owner\n\nMy lessson- get out the way\nA better connection between high level and low level planning\nClear release dates and responsibilities\nClear what I should be doing"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#agility",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#agility",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Agility",
    "text": "Agility\n\nThis project was agile whether we liked it or not!\nMy 2022 agile definition:\n\nCustomers can‚Äôt make up their minds\nIt‚Äôs hard to design software all at once\nContinuous delivery keeps customers happy"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#some-highlights-from-the-agile-manifesto",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#some-highlights-from-the-agile-manifesto",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Some highlights from The agile manifesto",
    "text": "Some highlights from The agile manifesto\n\n‚ÄúWelcome changing requirements, even late in development‚Äù\n‚ÄúAt regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior‚Äù\n‚ÄúContinuous attention to technical excellence and good design enhances agility‚Äù\n‚ÄúSimplicity- the art of maximizing the amount of work not done- is essential‚Äù"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#how-scrum-helped",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#how-scrum-helped",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "How scrum helped",
    "text": "How scrum helped\n\nThose shaping the project wanted to be able to make quick changes- and see the long term plan\nAgility is a mindset, a mode of practice\nIf anything we were actually too agile\nBeing agile is all about being able to review and make decisions frequently\nBut it isn‚Äôt about changing what you‚Äôre doing all the time\nGood code and good teams are ready to change direction- whether they change or not"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#current-work-and-future",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#current-work-and-future",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Current work and future",
    "text": "Current work and future\n\nWork on a ‚Äúpopulation based‚Äù model\nProviding support to elicit parameters for local and national use\nMore work understanding and building on the activity avoidance parameters"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#the-team",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#the-team",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "The team",
    "text": "The team"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#a-hospital-is-a-place-where-you-can-find-people",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#a-hospital-is-a-place-where-you-can-find-people",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "A hospital is a place where you can find people‚Ä¶",
    "text": "A hospital is a place where you can find people‚Ä¶\n\n\nhaving the best day of their life,\nthe worst day of their life,\nthe first day of their life,\nand the last day of their life."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#planning-is-hard",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#planning-is-hard",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Planning is hard",
    "text": "Planning is hard\n\n\n\n\n\nbuilt with enough capacity to replace the existing school\nfailed to take into account a new housing estate\nlikely needs double the number of spaces within the next decade\n\nBBC article"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#review-of-existing-models",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#review-of-existing-models",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Review of existing models",
    "text": "Review of existing models\n\nSteven Wyatt - NHS-R 2022"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#review-of-existing-models-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#review-of-existing-models-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Review of existing models",
    "text": "Review of existing models\n\nlots of models\nlots of external consultancies\nlots of similarities\n\n\n\nlots of repetition/duplication\nsufficiently different that comparing results is difficult\nmethodological progress slow\nno base to build from\n\n\n\nconsultancies don‚Äôt tend to offer products, but services\ndifficult to compare different models to understand if differences are methodological or due to assumptions\nsame issues seen 20/30 years ago\nlearning and expertise gathered tends to be trapped within trusts, or kept secret by consultancies"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#common-issues",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#common-issues",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Common issues",
    "text": "Common issues\n\nhandling uncertainty\nunnecessary/early aggregation\npoor coverage of some changes\nlack of ownership & auditability of assumptions\nconflating demand forecasting with affordability\n\n\n\nmost models handle changes like demographic changes and the impact of changes in occupancy rates\nbut few try to handle addressing inequities, health status adjustment"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#our-model",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#our-model",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Our model",
    "text": "Our model\n\nopen source (not quite yet‚Ä¶)\nuses standard, well-known datasets (e.g.¬†HES, ONS population projections)\ncurrently handles Inpatient admissions, Outpatient attendances, and A&E arrivals\nextensible and adaptable\ncovering all of the change factors\nstochastic Monte-Carlo model to handle uncertainty"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#project-structure",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#project-structure",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Project Structure",
    "text": "Project Structure\n\n\n\nData Extraction (R + {targets} & Sql)\nInputs App (R + {shiny})\nOutputs App (R + {shiny})\nModel Engine (Python & Docker)\nAzure Infrastructure (VM/ACR/ACI/Storage Accounts)\nAll of the code is stored on GitHub (currently, private repos üòî)\n\n\n\n\n\n\n\nflowchart TB\n  classDef orange fill:#f9bf07,stroke:#2c2825,color:#2c2825;\n  classDef lightslate fill:#b2b7b9,stroke:#2c2825,color:#2c2825;\n\n  A[Data Extraction]\n  B[Inputs App]\n  C[Model]\n  D[Outputs App]\n\n\n  SB[(input app data)]\n  SC[(model data)]\n  SD[(results data)]\n\n  A ---&gt; SB\n  A ---&gt; SC\n  \n  SB ---&gt; B\n  SC ---&gt; C\n\n  B ---&gt; C\n\n  C ---&gt; SD\n  SD ---&gt; D\n\n  B -.-&gt; D\n\n  class A,B,C,D orange\n  class SB,SC,SD lightslate"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-overview",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-overview",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Overview",
    "text": "Model Overview\n\n\nthe baseline data is a year worth of a provider‚Äôs HES data\neach row in the baseline data is run through a series of steps\neach step creates a factor that says how many times (on average) to sample that row\nthe factors are multiplied together and used to create a random Poisson value\nwe resample the rows using this random values\nefficiencies are then applied, e.g.¬†LoS reductions, type conversions\n\n\n\n\nIP/OP/A&E data\ncomplex, but not complicated"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-diagram",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-diagram",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Diagram",
    "text": "Model Diagram\n\n\n\n\n\nflowchart TB\n    classDef blue fill:#5881c1,stroke:#2c2825,color:#2c2825;\n    classDef orange fill:#f9bf07,stroke:#2c2825,color:#2c2825;\n    classDef red fill:#ec6555,stroke:#2c2825,color:#2c2825;\n    classDef lightslate fill:#b2b7b9,stroke:#2c2825,color:#2c2825;\n    classDef slate fill:#e0e2e3,stroke:#2c2825,color:#2c2825;\n\n    S[Baseline Activity]\n    T[Future Activity]\n\n    class S,T red\n\n    subgraph rr[Row Resampling]\n        direction LR\n\n        subgraph pop[Population Changes]\n            direction TB\n            pop_p[Population Growth]\n            pop_a[Age/Sex Structure]\n            pop_h[Population Specific Health Status]\n\n            class pop_p,pop_a,pop_h orange\n\n            pop_p --- pop_a --- pop_h\n        end\n\n        subgraph dsi[Demand Supply Imbalances]\n            direction TB\n            dsi_w[Waiting List Adjustment]\n            dsi_r[Repatriation/Expatriation]\n            dsi_p[Private Healthcare Dynamics]\n\n            class dsi_w,dsi_r,dsi_p orange\n\n            dsi_w --- dsi_r --- dsi_p\n        end\n\n        subgraph nsi[Need Supply Imbalances]\n            direction TB\n            nsi_g[Gaps in Care]\n            nsi_i[Inequalities]\n            nsi_t[Threshold Imbalances]\n\n            class nsi_g,nsi_i,nsi_t orange\n\n            nsi_g --- nsi_i --- nsi_t\n        end\n\n        subgraph nda [Non-Demographic Adjustment]\n            direction TB\n            nda_m[Medical Interventions]\n            nda_c[Changes to National Standards]\n            nda_p[Patient Expectations]\n\n            class nda_m,nda_c,nda_p orange\n\n            nda_m --- nda_c --- nda_p\n        end\n\n        subgraph mit[Activity Mitigators]\n            direction TB\n            mit_a[Activity Avoidance]\n            mit_t[Type Conversion]\n            mit_e[Efficiencies]\n            \n            class mit_a,mit_t,mit_e orange\n\n            mit_a --- mit_t --- mit_e\n        end\n\n        pop --- dsi --- nsi --- nda --- mit\n\n        class dsi,nsi,pop,nda,mit lightslate\n    end\n\n    class rr slate\n    \n    S --&gt; rr --&gt; T\n\n\n\n\n\n\n\n\nuses either patient-level data, or minimal aggregation\nrow resampling grouped into 5 broad groups\n\npopulation changes address the changes to the structure of the population and health status over the medium term\ndemand supply imbalances: hospitals are currently struggling to keep pace with demand, so we correct for this to not carry forwards these into the future\nneed supply imbalance: addressing gaps in care that currently exist\nnon-demographic: such as the development of new medical technologies\nactivity mitigators: strategies trusts adopt for reducing activity, or delivering activity more efficiently\n\nsome assumptions set nationally, such as population growth via ONS population projections\nother assumptions set locally, with support from a Shiny app"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-diagram-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-diagram-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Diagram",
    "text": "Model Diagram\n\n\n\n\n\nflowchart TB\n    classDef blue fill:#5881c1,stroke:#2c2825,color:#2c2825;\n    classDef orange fill:#f9bf07,stroke:#2c2825,color:#2c2825;\n    classDef red fill:#ec6555,stroke:#2c2825,color:#2c2825;\n    classDef lightslate fill:#b2b7b9,stroke:#2c2825,color:#2c2825;\n    classDef slate fill:#e0e2e3,stroke:#2c2825,color:#2c2825;\n\n    S[Baseline Activity]\n    T[Future Activity]\n\n    ORANGE[Implemented]\n    BLUE[Not yet implemented]\n\n    class ORANGE orange\n    class BLUE blue\n\n    class S,T red\n\n    subgraph rr[Row Resampling]\n        direction LR\n\n        subgraph pop[Population Changes]\n            direction TB\n            pop_p[Population Growth]\n            pop_a[Age/Sex Structure]\n            pop_h[Population Specific Health Status]\n\n            class pop_p,pop_a,pop_h orange\n\n            pop_p --- pop_a --- pop_h\n        end\n\n        subgraph dsi[Demand Supply Imbalances]\n            direction TB\n            dsi_w[Waiting List Adjustment]\n            dsi_r[Repatriation/Expatriation]\n            dsi_p[Private Healthcare Dynamics]\n\n            class dsi_w,dsi_r orange\n            class dsi_p blue\n\n            dsi_w --- dsi_r --- dsi_p\n        end\n\n        subgraph nsi[Need Supply Imbalances]\n            direction TB\n            nsi_g[Gaps in Care]\n            nsi_i[Inequalities]\n            nsi_t[Threshold Imbalances]\n\n            class nsi_g,nsi_i,nsi_t blue\n\n            nsi_g --- nsi_i --- nsi_t\n        end\n\n        subgraph nda [Non-Demographic Adjustment]\n            direction TB\n            nda_m[Medical Interventions]\n            nda_c[Changes to National Standards]\n            nda_p[Patient Expectations]\n\n            class nda_m,nda_c,nda_p blue\n\n            nda_m --- nda_c --- nda_p\n        end\n\n        subgraph mit[Activity Mitigators]\n            direction TB\n            mit_a[Activity Avoidance]\n            mit_t[Type Conversion]\n            mit_e[Efficiencies]\n            \n            class mit_a,mit_t,mit_e orange\n\n            mit_a --- mit_t --- mit_e\n        end\n\n        pop --- dsi --- nsi --- nda --- mit\n\n        class dsi,nsi,pop,nda,mit lightslate\n    end\n\n    class rr slate\n    \n    S --&gt; rr --&gt; T"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#monte-carlo-simulation",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#monte-carlo-simulation",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Monte Carlo Simulation",
    "text": "Monte Carlo Simulation\n\n\n\nWe run the model N times, varying the input parameters each time slightly to handle the uncertainty.\nThe results of the model are aggregated at the end of each model run\nThe aggregated results are combined at the end into a single file\n\n\n\n\n\n\n\nflowchart LR\n  classDef orange fill:#f9bf07,stroke:#2c2825,color:#2c2825;\n  classDef red fill:#ec6555,stroke:#2c2825,color:#2c2825;\n  \n  A[Baseline Activity]\n  Ba[Model Run 0]\n  Bb[Model Run 1]\n  Bc[Model Run 2]\n  Bd[Model Run 3]\n  Bn[Model Run n]\n  C[Results]\n\n  A ---&gt; Ba ---&gt; C\n  A ---&gt; Bb ---&gt; C\n  A ---&gt; Bc ---&gt; C\n  A ---&gt; Bd ---&gt; C\n  A ---&gt; Bn ---&gt; C\n  \n  class A,C red\n  class Ba,Bb,Bc,Bd,Bn orange\n  \n\n\n\n\n\n\n\nInspired by\n\nMapReduce (Google, 2004)\nSplit, Apply, Combine (H. Wickham, 2011)"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Parameters",
    "text": "Model Parameters\n\nWe ask users to provide parameters in the form of 90% confidence intervals\nWe can then convert these confidence intervals into distributions\nDuring the model we sample values from these distributions for each model parameter\nAll of the parameters represent the average rate to sample a row of data from the baseline"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Parameters",
    "text": "Model Parameters\n\n‚ÄúWe expect in the future to see between a 25% reduction and a 25% increase in this activity‚Äù\n\n\n\n\ngrey highlighted section: 90% confidence intervals\nblack line: confidence intervals into distributions\nyellow points: sampled parameter for a model run"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-2",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-2",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Parameters",
    "text": "Model Parameters\n\n‚ÄúWe expect in the future to see between a 20% reduction and a 90% reduction in this activity‚Äù\n\n\n\n\ngrey highlighted section: 90% confidence intervals\nblack line: confidence intervals into distributions\nyellow points: sampled parameter for a model run"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-3",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-3",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Parameters",
    "text": "Model Parameters\n\n‚ÄúWe expect in the future to see between a 2% reduction and an 18% reduction in this activity‚Äù\n\n\n\n\ngrey highlighted section: 90% confidence intervals\nblack line: confidence intervals into distributions\nyellow points: sampled parameter for a model run"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Run Example (1)",
    "text": "Model Run Example (1)\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\nf\n\n\n\n\n1\n50\nm\n100\n4\n1.00\n\n\n2\n50\nm\n110\n3\n1.00\n\n\n3\n51\nm\n120\n5\n1.00\n\n\n4\n50\nf\n100\n1\n1.00\n\n\n5\n50\nf\n110\n2\n1.00\n\n\n6\n52\nf\n120\n0\n1.00\n\n\n\n\n\n\n\n\n\nStart with baseline data - we are going to sample each row exactly once (column f)."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-2",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-2",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Run Example (2)",
    "text": "Model Run Example (2)\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\nf\n\n\n\n\n1\n50\nm\n100\n4\n1.00\n\n\n2\n50\nm\n110\n3\n1.00\n\n\n3\n51\nm\n120\n5\n1.00\n\n\n4\n50\nf\n100\n1\n1.00\n\n\n5\n50\nf\n110\n2\n1.00\n\n\n6\n52\nf\n120\n0\n1.00\n\n\n\n\n\n\n\nage\nsex\nf\n\n\n\n\n50\nm\n0.90\n\n\n51\nm\n1.10\n\n\n52\nm\n1.20\n\n\n50\nf\n0.80\n\n\n51\nf\n0.70\n\n\n52\nf\n1.30\n\n\n\n\n\n\n\nf\n\n\n\n\n1.00 √ó 0.90 = 0.90\n\n\n1.00 √ó 0.90 = 0.90\n\n\n1.00 √ó 1.10 = 1.10\n\n\n1.00 √ó 0.80 = 0.80\n\n\n1.00 √ó 0.80 = 0.80\n\n\n1.00 √ó 1.30 = 1.30\n\n\n\n\n\nWe perform a step where we join based on age and sex, then update the f column."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-3",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-3",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Run Example (3)",
    "text": "Model Run Example (3)\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\nf\n\n\n\n\n1\n50\nm\n100\n4\n0.90\n\n\n2\n50\nm\n110\n3\n0.90\n\n\n3\n51\nm\n120\n5\n1.10\n\n\n4\n50\nf\n100\n1\n0.80\n\n\n5\n50\nf\n110\n2\n0.80\n\n\n6\n52\nf\n120\n0\n1.30\n\n\n\n\n\n\n\nspecialty\nf\n\n\n\n\n100\n0.90\n\n\n110\n1.10\n\n\n\n\n\n\n\nf\n\n\n\n\n0.90 √ó 0.90 = 0.81\n\n\n0.90 √ó 1.10 = 0.99\n\n\n1.10 √ó 1.00 = 1.10\n\n\n0.80 √ó 0.90 = 0.72\n\n\n0.80 √ó 1.10 = 0.88\n\n\n1.30 √ó 1.00 = 1.30\n\n\n\n\n\nThe next step joins on the specialty column, again updating f. Note, if there is no value to join on, then we multiply by 1."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-4",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-4",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Run Example (4)",
    "text": "Model Run Example (4)\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\nf\nn\n\n\n\n\n1\n50\nm\n100\n4\n0.90\n1\n\n\n2\n50\nm\n110\n3\n0.90\n0\n\n\n3\n51\nm\n120\n5\n1.10\n2\n\n\n4\n50\nf\n100\n1\n0.80\n1\n\n\n5\n50\nf\n110\n2\n0.80\n0\n\n\n6\n52\nf\n120\n0\n1.30\n3\n\n\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\n\n\n\n\n1\n50\nm\n100\n4\n\n\n3\n51\nm\n120\n5\n\n\n3\n51\nm\n120\n5\n\n\n4\n50\nf\n100\n1\n\n\n6\n52\nf\n120\n0\n\n\n6\n52\nf\n120\n0\n\n\n6\n52\nf\n120\n0\n\n\n\n\n\nOnce all of the steps are performed, sample a random value n from a Poisson distribution with Œª=f, then we select each row n times."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-5",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-5",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Run Example (5)",
    "text": "Model Run Example (5)\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\ng\n\n\n\n\n1\n50\nm\n100\n4\n0.75\n\n\n3\n51\nm\n120\n5\n0.50\n\n\n3\n51\nm\n120\n5\n1.00\n\n\n4\n50\nf\n100\n1\n0.90\n\n\n6\n52\nf\n120\n0\n0.80\n\n\n6\n52\nf\n120\n0\n0.80\n\n\n6\n52\nf\n120\n0\n0.80\n\n\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\n\n\n\n\n1\n50\nm\n100\n2\n\n\n3\n51\nm\n120\n1\n\n\n3\n51\nm\n120\n5\n\n\n4\n50\nf\n100\n0\n\n\n6\n52\nf\n120\n0\n\n\n6\n52\nf\n120\n0\n\n\n6\n52\nf\n120\n0\n\n\n\n\n\nAfter resampling, we apply efficiency steps. E.g., similar joins are used to create column g, which is then used to sample a new LOS from a binomial distribution."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-built",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-built",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "How the model is built",
    "text": "How the model is built\n\nThe model is built in Python and can be run on any machine you can install Python on\nUses various packages, such as numpy and pandas\nReads data in .parquet format for efficiency\nReturns aggregated results as a .json file\nCould also output full row level results if needed"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-built-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-built-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "How the model is built",
    "text": "How the model is built\n\nCode is built in a modular approach\nEach activity type (Inpatients/Outpatients/A&E) has its own model code\nCode is reused where possible (e.g.¬†all three models share the code for demographic adjustment)"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-deployed",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-deployed",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "How the model is deployed",
    "text": "How the model is deployed\n\nDeployed as a Docker Container\nRuns in Azure Container Instances\nEach model run creates a new container, and the container is destroyed when the model run completes"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#data-extraction",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#data-extraction",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Data Extraction",
    "text": "Data Extraction\n\nUses principles of RAP, using R + {targets} and Sql\nAll of the data required to run the model\nData is extracted from various sources\n\nSql Datawarehouse (HES data)\nONS population projections + life expectancy tables\nCentral returns, e.g.¬†KH03\nODS data (organisation names, successors)\n\nExtracted data is uploaded to Azure storage containers"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#inputs-app",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#inputs-app",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Inputs App",
    "text": "Inputs App\nA {shiny} app that allows the user to set parameters, and submit as a job to run the model with those values."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#inputs-app-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#inputs-app-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Inputs App",
    "text": "Inputs App"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#outputs-app",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#outputs-app",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Outputs App",
    "text": "Outputs App\nA {shiny} app that allows the user to view the results of model runs."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#outputs-app-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#outputs-app-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Outputs App",
    "text": "Outputs App"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#questions",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#questions",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Questions?",
    "text": "Questions?\n\nContact The Strategy Unit\n\n\n strategy.unit@nhs.net\n The-Strategy-Unit\n\n\nContact Me\n\n\n thomas.jemmett@nhs.net\n tomjemmett"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#targets-for-analysts",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#targets-for-analysts",
    "title": "Coffee and Coding",
    "section": "{targets} for analysts",
    "text": "{targets} for analysts\n\n\n\nTom previously presented about {targets} at a coffee and coding last March and you can revisit his presentation and learn about the reasons why you should use the package to manage your pipeline and see a simple demonstration of how to use the package.\nMatt has presented previously about {targets} and making your workflows (pipelines) reproducible.\nSo‚Ä¶.. if you aren‚Äôt really even sure why your pipeline needs managing as an analyst or whether you actually have one (you do) then links to their presentations are at the end"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#aims",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#aims",
    "title": "Coffee and Coding",
    "section": "Aims",
    "text": "Aims\n\nIn this presentation we aim to demonstrate the real-world use of {targets} in an analysis project, but first a brief explanation\n\n\nWithout {targets} we\n\n\nWrite a script\nExecute script\nMake changes\nGo to step 2\n\n\nWith {targets} we will\n\n\nlearn how the various stages of our analysis fit together\nsave time by only running necessary stages as we cycle through the process\nhelp future you and colleagues re-visiting the analysis - Matt says ‚Äúits like a time-capsule‚Äù\nmake Reproducible Analytical Pipelines\n\n\nsource: The {targets} R package user manual"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#explain-the-live-project",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#explain-the-live-project",
    "title": "Coffee and Coding",
    "section": "Explain the live project",
    "text": "Explain the live project\n\noriginal project had 30+ metrics\nmultiple inter-related processing steps\neach time a metric changed or a process was altered it impacted across the project\nthere was potential for mistakes, duplication, lots of wasted time\nusing targets provides a structure that handles these inter-relationships"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#how-targets-can-help",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#how-targets-can-help",
    "title": "Coffee and Coding",
    "section": "How {targets} can help",
    "text": "How {targets} can help\n\ngets you thinking about your analysis and its building blocks\ntargets forces you into a functions approach to workflow\nentire pipeline is reproducible\nvisualise on one page\nsaves time\n(maybe we need an advanced function writing session in another C&C?)"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#demonstration-in-a-live-project",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#demonstration-in-a-live-project",
    "title": "Coffee and Coding",
    "section": "Demonstration in a live project",
    "text": "Demonstration in a live project\nLet‚Äôs look at a real life example in a live project‚Ä¶"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#visualising",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#visualising",
    "title": "Coffee and Coding",
    "section": "Visualising",
    "text": "Visualising\nCurrent project in {targets} and visualised with tar_visnetwork()"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#code",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#code",
    "title": "Coffee and Coding",
    "section": "Code",
    "text": "Code\n\nit‚Äôs like a recipe of steps\nit‚Äôs easier to read\nyou have built functions which you can transfer and reuse\nit‚Äôs efficient, good practice\ndebugging is easier because if/when it fails you know exactly which target it has failed on\nit creates intermediate cached objects you can fetch at any time"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#how-can-i-start-using-it",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#how-can-i-start-using-it",
    "title": "Coffee and Coding",
    "section": "How can I start using it?",
    "text": "How can I start using it?\n\nYou could ‚Äúretro-fit‚Äù it to your project, but ‚Ä¶ ideally you should start your project off using {targets}\nThere are at least three of us in SU who have used it in our projects.\nWe are offering to hand hold you to get started with your next project.\nMatt, Tom, Jacqueline"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#useful-targets-links",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#useful-targets-links",
    "title": "Coffee and Coding",
    "section": "Useful {targets} links",
    "text": "Useful {targets} links\n\nTom‚Äôs previous coffee and coding presentation\nMatt‚Äôs previous presentations\nThe {targets} documentation is detailed and easy to follow.\nA demo repository demonstrated in last weeks NHSE C&C\nSoftware Carpentry are developing a course here Pre-alpha targets course\nLive project demonstrated in this presentation using {targets}"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#welcome-to-coffee-and-coding",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#welcome-to-coffee-and-coding",
    "title": "Coffee and coding",
    "section": "Welcome to coffee and coding",
    "text": "Welcome to coffee and coding\n\nProject demos, showcasing work from a particular project\nMethod demos, showcasing how to use a particular method/tool/package\nSurgery and problem solving sessions\nDefining code standards and SOP"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#what-are-we-trying-to-achieve",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#what-are-we-trying-to-achieve",
    "title": "Coffee and coding",
    "section": "What are we trying to achieve?",
    "text": "What are we trying to achieve?\n\nLegibility\nReproducibility\nAccuracy\nLaziness"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#what-are-some-of-the-fundamental-principles",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#what-are-some-of-the-fundamental-principles",
    "title": "Coffee and coding",
    "section": "What are some of the fundamental principles?",
    "text": "What are some of the fundamental principles?\n\nPredictability, reducing mental load, and reducing truck factor\nMaking it easy to collaborate with yourself and others on different computers, in the cloud, in six months‚Äô time‚Ä¶\nDRY"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#what-is-rap",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#what-is-rap",
    "title": "Coffee and coding",
    "section": "What is RAP",
    "text": "What is RAP\n\na process in which code is used to minimise manual, undocumented steps, and a clear, properly documented process is produced in code which can reliably give the same result from the same dataset\nRAP should be:\n\n\nthe core working practice that must be supported by all platforms and teams; make this a core focus of NHS analyst training\n\nGoldacre review"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#the-road-to-rap",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#the-road-to-rap",
    "title": "Coffee and coding",
    "section": "The road to RAP",
    "text": "The road to RAP\n\nWe‚Äôre roughly using NHS Digital‚Äôs RAP stages\nThere is an incredibly large amount to learn!\nConfession time! (everything I do not know‚Ä¶)\nYou don‚Äôt need to do it all at once\nYou don‚Äôt need to do it all at all ever\nEach thing you learn will incrementally help you\nRemember- that‚Äôs why we learnt this stuff. Because it helped us. And it can help you too"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--baseline",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--baseline",
    "title": "Coffee and coding",
    "section": "Levels of RAP- Baseline",
    "text": "Levels of RAP- Baseline\n\nData produced by code in an open-source language (e.g., Python, R, SQL).\nCode is version controlled (see Git basics and using Git collaboratively guides).\nRepository includes a README.md file (or equivalent) that clearly details steps a user must follow to reproduce the code\nCode has been peer reviewed.\nCode is published in the open and linked to & from accompanying publication (if relevant).\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--silver",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--silver",
    "title": "Coffee and coding",
    "section": "Levels of RAP- Silver",
    "text": "Levels of RAP- Silver\n\nCode is well-documented‚Ä¶\nCode is well-organised following standard directory format\nReusable functions and/or classes are used where appropriate\nPipeline includes a testing framework\nRepository includes dependency information (e.g.¬†requirements.txt, PipFile, environment.yml\nData is handled and output in a Tidy data format\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--gold",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--gold",
    "title": "Coffee and coding",
    "section": "Levels of RAP- Gold",
    "text": "Levels of RAP- Gold\n\nCode is fully packaged\nRepository automatically runs tests etc. via CI/CD or a different integration/deployment tool e.g.¬†GitHub Actions\nProcess runs based on event-based triggers (e.g., new data in database) or on a schedule\nChanges to the RAP are clearly signposted. E.g. a changelog in the package, releases etc. (See gov.uk info on Semantic Versioning)\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#a-learning-journey-to-get-us-there",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#a-learning-journey-to-get-us-there",
    "title": "Coffee and coding",
    "section": "A learning journey to get us there",
    "text": "A learning journey to get us there\n\nCode style, organising your files\nFunctions and iteration\nGit and GitHub\nPackaging your code\nTesting\nPackage management and versioning"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#how-we-can-help-each-other-get-there",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#how-we-can-help-each-other-get-there",
    "title": "Coffee and coding",
    "section": "How we can help each other get there",
    "text": "How we can help each other get there\n\nWork as a team!\nCoffee and coding!\nAsk for help!\nDo pair coding!\nGet your code reviewed!\nJoin the NHS-R/ NHSPycom communities"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#the-history-of-nhs-r",
    "href": "presentations/2024-12-02_dhsc/index.html#the-history-of-nhs-r",
    "title": "DHSC presentation",
    "section": "The history of NHS-R",
    "text": "The history of NHS-R\n\nFounded in 2018\nFirst conference was pretty meagre but very exciting\nNHS-R has gone from strength to strength, to‚Ä¶"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#goldacre",
    "href": "presentations/2024-12-02_dhsc/index.html#goldacre",
    "title": "DHSC presentation",
    "section": "Goldacre",
    "text": "Goldacre\n\nCreate and maintain a curated national open library of NHS analyst code\nEnsure all training is open by default\nSupport an NHS analyst community\nOversee funding and delivery of training, both open online and one-to-one"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#everything-open-all-the-time",
    "href": "presentations/2024-12-02_dhsc/index.html#everything-open-all-the-time",
    "title": "DHSC presentation",
    "section": "Everything open, all the time",
    "text": "Everything open, all the time\n\nEverything NHS-R does is open\n\n(except Slack üôÅ)\n\nEverything NHS-R produces has an open licence\nWe teach the skills of open\n\nGit, GitHub, how to write reusable code\n\n(our other value- we love beginners)"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#what-does-nhs-r-do-already",
    "href": "presentations/2024-12-02_dhsc/index.html#what-does-nhs-r-do-already",
    "title": "DHSC presentation",
    "section": "What does NHS-R do already?",
    "text": "What does NHS-R do already?\n\nTraining\nNHS-R academy\nNHS-R solutions\nConference\nWebinars\nSlack\nBlogs\nGitHub and collaboration"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#how-can-nhs-r-help-you",
    "href": "presentations/2024-12-02_dhsc/index.html#how-can-nhs-r-help-you",
    "title": "DHSC presentation",
    "section": "How can NHS-R help you?",
    "text": "How can NHS-R help you?\n\nTraining, and open training resources\nOnline, (often instant) help\nJoin the academy\nCommunity and belonging\nHear about (and copy üòâ) best practice\nPermission and visibility"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#a-couple-of-examples-of-nhs-r-at-its-best",
    "href": "presentations/2024-12-02_dhsc/index.html#a-couple-of-examples-of-nhs-r-at-its-best",
    "title": "DHSC presentation",
    "section": "A couple of examples of NHS-R at its best",
    "text": "A couple of examples of NHS-R at its best\n\nNHSRplotthedots- in production across the NHS\nNHSRwaitinglists\nIntro to R and intermediate R\nAdvent of code is a yearly event on the Slack- happening now üéÑ"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#ask-not-what-nhs-r-can-do-for-you",
    "href": "presentations/2024-12-02_dhsc/index.html#ask-not-what-nhs-r-can-do-for-you",
    "title": "DHSC presentation",
    "section": "Ask not what NHS-R can do for you‚Ä¶",
    "text": "Ask not what NHS-R can do for you‚Ä¶\n\nWe need you!\n\nTrain analysts (intro, reporting, Shiny‚Ä¶)\nWork on NHS-R solutions (like NHSRplotthedots)\nHost and run webinars\nContribute to shared policy and practice documents\nGive help to others\nBlog"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#conclusion",
    "href": "presentations/2024-12-02_dhsc/index.html#conclusion",
    "title": "DHSC presentation",
    "section": "Conclusion",
    "text": "Conclusion\n\nIf you want to go fast, go alone\nIf you want to go far, go together\n\nAfrican Proverb"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#the-strategy-unit",
    "href": "presentations/2024-12-02_dhsc/index.html#the-strategy-unit",
    "title": "DHSC presentation",
    "section": "The Strategy Unit",
    "text": "The Strategy Unit\n\n‚ÄúLeading research, analysis and change from within the NHS‚Äù\n70+ person unit, data science being the newest addition\nSpecialist qualitative and quantitative analysis\nThe data science team works closely with the other teams"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#meet-the-team",
    "href": "presentations/2024-12-02_dhsc/index.html#meet-the-team",
    "title": "DHSC presentation",
    "section": "Meet the team",
    "text": "Meet the team\n       \n\nWe have:\n\nPsychology, Maths/ computing, Librarian-ing, Statistics, Entomology, Shiny, Horses, and Maternity data"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#projects",
    "href": "presentations/2024-12-02_dhsc/index.html#projects",
    "title": "DHSC presentation",
    "section": "Projects",
    "text": "Projects\n\nNew Hospital Programme\nOne project, lots of components"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#the-model",
    "href": "presentations/2024-12-02_dhsc/index.html#the-model",
    "title": "DHSC presentation",
    "section": "The model",
    "text": "The model\n\n\n\n\n\nflowchart LR\n  classDef orange fill:#f9bf07,stroke:#2c2825,color:#2c2825;\n  classDef lightslate fill:#b2b7b9,stroke:#2c2825,color:#2c2825;\n\n  A[Data Extraction]\n  B[Inputs App]\n  C[Model]\n  D[Outputs App]\n\n\n  SB[(input app data)]\n  SC[(model data)]\n  SD[(results data)]\n\n  A ---&gt; SB\n  A ---&gt; SC\n  \n  SB ---&gt; B\n  SC ---&gt; C\n\n  B ---&gt; C\n\n  C ---&gt; SD\n  SD ---&gt; D\n\n  B -.-&gt; D\n\n  class A,B,C,D orange\n  class SB,SC,SD lightslate"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#the-model-itself",
    "href": "presentations/2024-12-02_dhsc/index.html#the-model-itself",
    "title": "DHSC presentation",
    "section": "The model itself",
    "text": "The model itself\n\nProbabilistic inputs and outputs\nMonte carlo simulation\nAccounts for:\n\nDemographic growth\nNon demographic growth\n‚ÄúMitigation‚Äù"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#the-work",
    "href": "presentations/2024-12-02_dhsc/index.html#the-work",
    "title": "DHSC presentation",
    "section": "The work",
    "text": "The work\n\nRollout with NHP schemes\nUpdating the model and interface in place\nReporting on and analysing the model results and inputs"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#our-interests",
    "href": "presentations/2024-12-02_dhsc/index.html#our-interests",
    "title": "DHSC presentation",
    "section": "Our interests",
    "text": "Our interests\n\nProductionisation of models for healthcare\nData science approaches to evidence review\nData science approaches to text\nLLMs"
  },
  {
    "objectID": "presentations/2024-09-05_earl-nhp/index.html#the-new-hospital-programme-nhp",
    "href": "presentations/2024-09-05_earl-nhp/index.html#the-new-hospital-programme-nhp",
    "title": "Using R and Python to model future hospital activity",
    "section": "The New Hospital Programme (NHP)",
    "text": "The New Hospital Programme (NHP)\n\n\n\nA manifesto commitment\nFuture activity must be modelled\nNeed consistency across schemes\n\n\n\n\n\n\nBuilding new hospitals - replacing crumbling infrastructure in some cases, completely new builds in others.\nIt‚Äôs important to size the hospitals according to the type and quantity of activity there will be in the future.\nThere are many proprietary black box models in use for estimating healthcare activity in the future - no consistency, difficult to compare results\nStrategy Unit was asked to develop a model to be used across all of the builds: a model owned and operated by the NHS, for the NHS.\n\n\n::::"
  },
  {
    "objectID": "presentations/2024-09-05_earl-nhp/index.html#model-process",
    "href": "presentations/2024-09-05_earl-nhp/index.html#model-process",
    "title": "Using R and Python to model future hospital activity",
    "section": "",
    "text": "A probabilistic Monte Carlo simulation that:\n\nTakes hospital activity from a baseline year, using NHS England‚Äôs Hospital Episode Statistics (HES) data\nApplies variables that:\n\nare outside of our control (e.g.¬†population changes, using ONS projections)\ncan reduce hospital activity (mitigators, e.g.¬†virtual wards or teleappointments)\n\nForecasts future demand based on these variables, outputting probabilistic predictive intervals"
  },
  {
    "objectID": "presentations/2024-09-05_earl-nhp/index.html#our-challenges",
    "href": "presentations/2024-09-05_earl-nhp/index.html#our-challenges",
    "title": "Using R and Python to model future hospital activity",
    "section": "Our challenges",
    "text": "Our challenges\n\n28 hospitals currently using the model\nModel is being developed whilst in production\nModel is very complex - technically, and for end users\n\n\n\nHospitals are actively using the model while it is still in development, which can be tricky\nDataset is massive for each hospital - hundreds and thousands of rows - all activity for a hospital trust in one year\nModel can accommodate hundreds of different variables, understanding and setting these can be challenging for end users\nWe have comprehensive, openly available documentation and also a team of Model Relationship Managers to help address this"
  },
  {
    "objectID": "presentations/2024-09-05_earl-nhp/index.html#tools-and-platforms",
    "href": "presentations/2024-09-05_earl-nhp/index.html#tools-and-platforms",
    "title": "Using R and Python to model future hospital activity",
    "section": "Tools and platforms",
    "text": "Tools and platforms\n\nData pipelines: {targets} , SQL \nModel: Python , Docker \nApps: {shiny} and {golem} , Posit Connect \nInfrastructure and storage: Azure \nDocumentation: Quarto \nVersion control and collaboration: Git , GitHub \n\n\n\nSo how did we solve the problem?\nHere‚Äôs a rundown of the tools and platforms that we use.\nThe data pipeline is orchestrated by {targets} for its recipe-like format and so we re-run only what needs re-running.\nThe model is built in Python and involves a lot of pandas DataFrame manipulations.\nWe use Azure for storage of model input data and JSON files of results.\nUsers input model paramters in one Shiny app and view results in another. This uses modules and {golem} for its package focus, as well as {bs4Dash}. We have development and productino environments.\nWe have a deployed Quarto website that contains the documentation for the whole project.\nIn general, we‚Äôre following the principles of Reproducible Analytical Pipelines (RAP) in everything we do.\nAll originally written by Tom.\nAs the team has grown we have shared responsibilities: YiWen in Python, Matt with Shiny, Tom as technical lead."
  },
  {
    "objectID": "presentations/2024-09-05_earl-nhp/index.html#structure",
    "href": "presentations/2024-09-05_earl-nhp/index.html#structure",
    "title": "Using R and Python to model future hospital activity",
    "section": "",
    "text": "This is a simplified overview of the structure and flow of information through the system.\nThe full structure is quite complex, reflecting the complexity of user needs and the scale of the task.\nData from our database is processed and stored in Azure Storage Containers via a targets pipeline. Additional data, like ONS population projections, are also stored.\nThe users interact with a Shiny app to set their input parameters. The app provides some contextual information derived from the data held in Azure. Users click a button to run the model.\nThe model is deployed as a Docker container in Azure Continer Instances, triggered by an API call.\nThe model results are stored as JSON in an Azure container, ready for collection and presentation in an outputs app.\nUsers can view charts and tables and download files for further analysis.\nSo there‚Äôs clear front- and backends and we have\nFurther complexity is added by the need to process and present information despite changes to the model over time.\nWe use development and production environments for our apps to help reduce errors."
  },
  {
    "objectID": "presentations/2024-09-05_earl-nhp/index.html#outputs-app",
    "href": "presentations/2024-09-05_earl-nhp/index.html#outputs-app",
    "title": "Using R and Python to model future hospital activity",
    "section": "",
    "text": "Here‚Äôs a preview of the outputs app.\nIn the navbar you can see that users can aggregate by hospital sites; view charts and tables; and download results files for further processing.\nThere are also context-specific drodown menus to focus in on certain data. For example, to see results by activity type: inpatients, outpatients or A&E.\nIn this particular tab we can see a beeswarm plot showing each simulation as an individual point. This kind of presentation is important to remind users that the model outputs a distribution; that there are range of possibilities.\nThe data provided here to users is used to drive decisions about the size of hospital that will be developed."
  },
  {
    "objectID": "presentations/2024-09-05_earl-nhp/index.html#next",
    "href": "presentations/2024-09-05_earl-nhp/index.html#next",
    "title": "Using R and Python to model future hospital activity",
    "section": "Next",
    "text": "Next\n\nForecast regionally and nationally\nMove data and pipelines into Databricks\nOpen-source model code\n\n\n\nWe‚Äôre currently working with hospitals and trusts, but we‚Äôre also expanding the geographical scale to produce results at the regional and national scale. This will require some thinking around processing, modelling and generating outputs.\nWe‚Äôre currently transferring data processing into Databricks, partly to bring all the steps into one platform but also as an opportunity to speed up the processing by using Spark.\nFinally, we already have some aspects in the open, like the project information site, but we‚Äôd also like to open-source the model code itself so that others can use and develop it."
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#patient-experience",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#patient-experience",
    "title": "What good data science looks like",
    "section": "Patient experience",
    "text": "Patient experience\n\nThe NHS collects a lot of patient experience data\nRate the service 1-5 (Very poor‚Ä¶ Excellent) but also give written feedback\n\n‚ÄúParking was difficult‚Äù\n‚ÄúDoctor was rude‚Äù\n‚ÄúYou saved my life‚Äù\n\nMany organisations lack the staffing to read all of the feedback in a systematic way\nProduce an algorithm to rate theme and ‚Äúcriticality‚Äù"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#help-people-to-do-their-jobs",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#help-people-to-do-their-jobs",
    "title": "What good data science looks like",
    "section": "Help people to do their jobs",
    "text": "Help people to do their jobs\n\nText based data is complex and built on human experience\nThe tool should enhance, not replace, human understanding\nEnhancing search and filtering\n\nIf they read 100 comments today, which should they read?\n\n‚ÄúA recommendation engine for feedback data‚Äù"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#reflect-what-users-want",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#reflect-what-users-want",
    "title": "What good data science looks like",
    "section": "Reflect what users want",
    "text": "Reflect what users want\n\nI have worked with this data since before it existed\nI came to realise that people were struggling to read all of their data\nFits alongside other work happening within NHSE\n\nA framework for understanding patient experience"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#useful",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#useful",
    "title": "What good data science looks like",
    "section": "Useful",
    "text": "Useful\n\nA fundamental principle is that everyone can use\nIf you can run the code, run it\nIf you can use the API, use it\nIf you just want the dashboard, use it\nCredit to the growth charts API"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#understandable",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#understandable",
    "title": "What good data science looks like",
    "section": "Understandable",
    "text": "Understandable\n\nTuned to the users needs\nNot simply tuning accuracy scores\nLook at the type of mistake the model is making\nLook at the category it‚Äôs predicting\n\nWe can lose a few of common unimportant categories\nWe need to get every rare and important category"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#iterative",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#iterative",
    "title": "What good data science looks like",
    "section": "Iterative",
    "text": "Iterative\n\nYear one\n\n10 categories\nModerate criticality performance\nNo deep learning\nWeak dashboard\nPositive evaluation"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#iterative-1",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#iterative-1",
    "title": "What good data science looks like",
    "section": "Iterative",
    "text": "Iterative\n\nYear two\n\n30-50 categories\nStrong criticality performance\nDeep learning\nImproved dashboard\nWIP\n\nOverall five minor versions of algorithm and seven of dashboard"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#documented",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#documented",
    "title": "What good data science looks like",
    "section": "Documented",
    "text": "Documented\n\nWe‚Äôve documented in the way you usually would\nWe were asked in year 1 to provide plain English documentation\nWe made a website with all the product details"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#develop-skills-of-the-staff-technical-and-otherwise",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#develop-skills-of-the-staff-technical-and-otherwise",
    "title": "What good data science looks like",
    "section": "Develop skills of the staff, technical and otherwise",
    "text": "Develop skills of the staff, technical and otherwise\n\nYear one created a Python programmer\nYear two created an R/ Shiny programmer\nThe team has learned:\n\nStatic website generation\nText cleaning/ searching/ mining\nCollaborative coding practices\nWorking with and communicating with users\nLinux, databases, APIs‚Ä¶"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#benefits-from-and-benefits-the-community",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#benefits-from-and-benefits-the-community",
    "title": "What good data science looks like",
    "section": "Benefits from, and benefits, the community",
    "text": "Benefits from, and benefits, the community\n\nNHSBSA R Shiny template"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#benefits-from-and-benefits-the-community-1",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#benefits-from-and-benefits-the-community-1",
    "title": "What good data science looks like",
    "section": "Benefits from, and benefits, the community",
    "text": "Benefits from, and benefits, the community\n\nWe benefit and benefit from\n\nNHS-R\nNHS-Pycom\nGovernment Digital Service\nColleagues and friends"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#open-and-reproducible",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#open-and-reproducible",
    "title": "What good data science looks like",
    "section": "Open and reproducible",
    "text": "Open and reproducible\n\nOff the shelf, proprietary data collection systems dominate\nThey often offer bundled analytic products of low quality\nThe DS time can‚Äôt and doesn‚Äôt want to offer a complete data system\nHow can we best contribute to improving patient experience for patients in the NHS?\n\nIf the patient experience data won‚Äôt come to the mountain‚Ä¶"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#open-source-ftw",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#open-source-ftw",
    "title": "What good data science looks like",
    "section": "Open source FTW!",
    "text": "Open source FTW!\n\nOften individuals in the NHS don‚Äôt want private companies to ‚Äúbenefit‚Äù from open code\nBut if they make their products better with open code the patients win\nBest practice as code"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#fun",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#fun",
    "title": "What good data science looks like",
    "section": "Fun!",
    "text": "Fun!\n\nCombing through spreadsheets looking for one comment is not fun\nDoing things the same way you did them last year is not fun\nTrying to implement a project that is too complicated is not fun\n\n¬†\n\nWorking with a diverse team with different skills is fun\nAccessing high quality documentation to understand a project better is fun*"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#team-and-code",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#team-and-code",
    "title": "What good data science looks like",
    "section": "Team and code",
    "text": "Team and code\n\nAndreas Soteriades (Y1)\nYiWen Hon, Oluwasegun Apejoye (Y2)\n\n¬†\n\npxtextmining\nexperiencesdashboard\nDocumentation\n\n\n\nchris.beeley1@nhs.net\nhttps://fosstodon.org/@chrisbeeley"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#the-problem",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#the-problem",
    "title": "Package tour of sconn",
    "section": "The problem",
    "text": "The problem\n\n\n\nSome of us are very precious and need our familiar keyboard shortcuts and UI\nWorking in a Databricks notebook is fine but for more advanced work in R it‚Äôs easier to work in your local IDE.\nInitial attempts to connect using {sparklyr} alone didn‚Äôt work (for me)\n\n\n\n\n\nyum"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#how-does-sconn-help",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#how-does-sconn-help",
    "title": "Package tour of sconn",
    "section": "How does {sconn} help?",
    "text": "How does {sconn} help?\n\nCreates a convenience function to connect (and disconnect) from our databricks instance\nProvides documentation for new users to get set up\nProvides a place to track issues"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#brief-usage",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#brief-usage",
    "title": "Package tour of sconn",
    "section": "Brief usage",
    "text": "Brief usage\nlibrary(sconn)\nsc()\n\nsc_disconnect()"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#development-history",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#development-history",
    "title": "Package tour of sconn",
    "section": "Development history",
    "text": "Development history\n\n\n\nSimilar package used to connect to SQL Server databases\nConnection functions are lazily bound to a secret environment (and not run?)\nThe user-facing function gets the function and thus activates it\n\n\n\n\n\nenvironments, Hadley‚Äôs version"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#under-the-hood-1",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#under-the-hood-1",
    "title": "Package tour of sconn",
    "section": "Under the hood 1",
    "text": "Under the hood 1\n\n\nsc_conn &lt;- function() {\n  check_vars()\n  sparklyr::spark_connect(\n    master = Sys.getenv(\"DATABRICKS_HOST\"),\n    cluster_id = Sys.getenv(\"DATABRICKS_CLUSTER_ID\"),\n    token = Sys.getenv(\"DATABRICKS_TOKEN\"),\n    envname = Sys.getenv(\"DATABRICKS_VENV\"),\n    app_name = \"sconn_sparklyr\",\n    method = \"databricks_connect\"\n  )\n}\n\n.onLoad &lt;- function(...) {\n  .conns &lt;&lt;- rlang::new_environment()\n  rlang::env_bind_lazy(.conns, sc = sc_conn())\n}\n\n\n\n\nempty env"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#under-the-hood-2",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#under-the-hood-2",
    "title": "Package tour of sconn",
    "section": "Under the hood 2",
    "text": "Under the hood 2\nsc &lt;- function(hide_output = TRUE) {\n  if (!rlang::env_has(.conns, \"sc\")) {\n    rlang::env_bind_lazy(.conns, sc = sc_conn())\n  }\n  sc &lt;- rlang::env_get(.conns, \"sc\", default = NULL)\n  if (hide_output) invisible(sc) else sc\n}"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#lazy-binding",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#lazy-binding",
    "title": "Package tour of sconn",
    "section": "Lazy binding?",
    "text": "Lazy binding?\n\n\n\nOn load (library()/load_all()) the package should lazily bind a connection function to the .conns environment\nWhen the user calls sc(), this function is activated by rlang::env_get()\n\n\n\n\n\nMr.¬†Lazy\n\n\n\nmrmen.fandom.com"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#remaining-questions",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#remaining-questions",
    "title": "Package tour of sconn",
    "section": "Remaining questions",
    "text": "Remaining questions\n\nDoes .onLoad work the way I expect it to?\nDoes env_get always have to activate the connection?\n(sparklyr‚Äôs spark_connection_is_open() function triggers it)\nConnection time-outs and reconnection? How best to handle?"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#further-resources",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#further-resources",
    "title": "Package tour of sconn",
    "section": "Further resources",
    "text": "Further resources\n\nbrickster\nsconn repo\ndatabricks docs\nunlocking databricks from R‚Ä¶"
  },
  {
    "objectID": "presentations/2025-02-27_word-not-quarto/index.html#backstory",
    "href": "presentations/2025-02-27_word-not-quarto/index.html#backstory",
    "title": "When Quarto wasn‚Äôt the right answer",
    "section": "Backstory üìñ",
    "text": "Backstory üìñ\n\nNHP outputs reports: need automation/reproducibility\nQuarto was the answer‚Ä¶ until it wasn‚Äôt!\nSolution: meet users where they are"
  },
  {
    "objectID": "presentations/2025-02-27_word-not-quarto/index.html#implementation",
    "href": "presentations/2025-02-27_word-not-quarto/index.html#implementation",
    "title": "When Quarto wasn‚Äôt the right answer",
    "section": "Implementation ‚úèÔ∏è",
    "text": "Implementation ‚úèÔ∏è\n\nAdd metadata and store site selections on Azure\nRead SharePoint template, insert content with {officer}\nWrite timestamped folder with doc, results, log\n\n\n\nHow do we identify chosen runs? On Aure, with run_stage metadata on results files and a list of chosen sites.\nHow to confirm with model relationship managers? Expose tagged scenarios and sites via a scheduled Quarto-report.\nBasic interface: supply a scheme code, at simplest level.\nUnderlying functions save figures and values independently.\nImages inserted by moving the imaginary ‚Äòcursor‚Äô to a known string of text in the report.\nValue-insertion is more complicated: insert ‚Äòfield‚Äô codes into the report, insert the values to custom document properties, refresh to insert these into the fields.\nThe output is a timestamped directory with the report, standalone images and values, and a log."
  },
  {
    "objectID": "presentations/2025-02-27_word-not-quarto/index.html#doing-it-right",
    "href": "presentations/2025-02-27_word-not-quarto/index.html#doing-it-right",
    "title": "When Quarto wasn‚Äôt the right answer",
    "section": "‚ÄòDoing it right‚Äô üíØ",
    "text": "‚ÄòDoing it right‚Äô üíØ\n\nGithub: ‚Äòlearn by doing‚Äô, challenge and improve things\nDocumentation/version history have saved us!\nThe repo is a springboard for parallel/future work"
  },
  {
    "objectID": "presentations/2025-02-27_word-not-quarto/index.html#reflections",
    "href": "presentations/2025-02-27_word-not-quarto/index.html#reflections",
    "title": "When Quarto wasn‚Äôt the right answer",
    "section": "Reflections ü™û",
    "text": "Reflections ü™û\n\nChallenge your thinking, be adaptable to user needs\nEmbrace agility and acknowledge fragility\n‚ÄòThe power of friendship‚Äô keeps things working\n\n\n\nThe default mode of ‚Äòmake a Quarto doc‚Äô wasn‚Äôt the right fit here, but we recognised that and pivoted quickly.\nWe know the solution isn‚Äôt perfect, but it is good enough.\nThe process has some frailties, but we recognise them. For example, the report path being changed on SharePoint means we can‚Äôt retrieve it!\nTalking between us on the code side, but also with the user, is really important to handle changing needs and to keep the process and outputs fir for purpose."
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#which-is-easier-to-read",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#which-is-easier-to-read",
    "title": "Coffee and Coding",
    "section": "Which is easier to read?",
    "text": "Which is easier to read?\n\nae_attendances |&gt;\n  filter(org_code %in% c(\"RNA\", \"RL4\")) |&gt;\n  mutate(performance = 1 + breaches / attendances) |&gt;\n  filter(type == 1) |&gt;\n  mutate(met_target = performance &gt;= 0.95)\n\nor\n\nae_attendances |&gt;\n  filter(\n    org_code %in% c(\"RNA\", \"RL4\"),\n    type == 1\n  ) |&gt;\n  mutate(\n    performance = 1 + breaches / attendances,\n    met_target = performance &gt;= 0.95\n  )\n\n\n  spending a few seconds to neatly format your code can greatly improve the legibility to future readers, making the intent of the code far clearer, and will make finding bugs easier to spot.\n\n\n  (have you spotted the mistake in the snippets above?)"
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#tidyverse-style-guide",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#tidyverse-style-guide",
    "title": "Coffee and Coding",
    "section": "Tidyverse Style Guide",
    "text": "Tidyverse Style Guide\n\nGood coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread\n\n\nAll style guides are fundamentally opinionated. Some decisions genuinely do make code easier to use (especially matching indenting to programming structure), but many decisions are arbitrary. The most important thing about a style guide is that it provides consistency, making code easier to write because you need to make fewer decisions.\n\ntidyverse style guide"
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#lintr-styler-are-your-new-best-friends",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#lintr-styler-are-your-new-best-friends",
    "title": "Coffee and Coding",
    "section": "{lintr} + {styler} are your new best friends",
    "text": "{lintr} + {styler} are your new best friends\n\n\n{lintr}\n\n{lintr} is a static code analysis tool that inspects your code (without running it)\nit checks for certain classes of errors (e.g.¬†mismatched { and (‚Äôs)\nit warns about potential issues (e.g.¬†using variables that aren‚Äôt defined)\nit warns about places where you are not adhering to the code style\n\n\n{styler}\n\n{styler} is an RStudio add in that automatically reformats your code, tidying it up to match the style guide\n99.9% of the time it will give you equivalent code, but there is the potential that it may change the behaviour of your code\nit will overwrite the files that you ask it to run on however, so it is vital to be using version control\na good workflow here is to save your file, ‚Äústage‚Äù the changes to your file, then run {styler}. You can then revert back to the staged changed if needed."
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#what-does-lintr-look-like",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#what-does-lintr-look-like",
    "title": "Coffee and Coding",
    "section": "What does {lintr} look like?",
    "text": "What does {lintr} look like?\n\n\n\nsource: Good practice for writing R code and R packages\n\nrunning lintr can be done in the console, e.g.\n\nlintr::lintr_dir(\".\")\n\nor via the Addins menu"
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#using-styler",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#using-styler",
    "title": "Coffee and Coding",
    "section": "Using {styler}",
    "text": "Using {styler}\n\nsource: Good practice for writing R code and R packages"
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#further-thoughts-on-improving-code-legibility",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#further-thoughts-on-improving-code-legibility",
    "title": "Coffee and Coding",
    "section": "Further thoughts on improving code legibility",
    "text": "Further thoughts on improving code legibility\n\ndo not let files grow too big\nbreak up logic into separate files, then you can use source(\"filename.R) to run the code in that file\nidealy, break up your logic into separate functions, each function having it‚Äôs own file, and then call those functions within your analysis\ndo not repeat yourself - if you are copying and pasting your code then you should be thinking about how to write a single function to handle this repeated logic"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#why",
    "href": "presentations/2024-05-16_store-data-safely/index.html#why",
    "title": "Store Data Safely",
    "section": "Why?",
    "text": "Why?\nBecause:\n\ndata may be sensitive\nGitHub was designed for source control of code\nGitHub has repository file-size limits\nit makes data independent from code\nit prevents repetition"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#other-approaches",
    "href": "presentations/2024-05-16_store-data-safely/index.html#other-approaches",
    "title": "Store Data Safely",
    "section": "Other approaches",
    "text": "Other approaches\nTo prevent data commits:\n\nuse a .gitignore file (*.csv, etc)\nuse Git hooks\navoid ‚Äòadd all‚Äô (git add .) when staging\nensure thorough reviews of (small) pull-requests"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#what-if-i-committed-data",
    "href": "presentations/2024-05-16_store-data-safely/index.html#what-if-i-committed-data",
    "title": "Store Data Safely",
    "section": "What if I committed data?",
    "text": "What if I committed data?\n‚ÄòIt depends‚Äô, but if it‚Äôs sensitive:\n\n‚Äòundo‚Äô the commit with git reset\nuse a tool like BFG to expunge the file from Git history\ndelete the repo and restart üî•\n\nA data security breach may have to be reported."
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#data-hosting-solutions",
    "href": "presentations/2024-05-16_store-data-safely/index.html#data-hosting-solutions",
    "title": "Store Data Safely",
    "section": "Data-hosting solutions",
    "text": "Data-hosting solutions\nWe‚Äôll talk about two main options for The Strategy Unit:\n\nPosit Connect and the {pins} package\nAzure Data Storage\n\nWhich to use? It depends."
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#a-platform-by-posit",
    "href": "presentations/2024-05-16_store-data-safely/index.html#a-platform-by-posit",
    "title": "Store Data Safely",
    "section": "A platform by Posit",
    "text": "A platform by Posit\n\n\nhttps://connect.strategyunitwm.nhs.uk/"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#a-package-by-posit",
    "href": "presentations/2024-05-16_store-data-safely/index.html#a-package-by-posit",
    "title": "Store Data Safely",
    "section": "A package by Posit",
    "text": "A package by Posit\n\n\nhttps://pins.rstudio.com/"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#basic-approach",
    "href": "presentations/2024-05-16_store-data-safely/index.html#basic-approach",
    "title": "Store Data Safely",
    "section": "Basic approach",
    "text": "Basic approach\ninstall.packages(\"pins\")\nlibrary(pins)\n\nboard_connect()\npin_write(board, data, \"pin_name\")\npin_read(board, \"user_name/pin_name\")"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#live-demo",
    "href": "presentations/2024-05-16_store-data-safely/index.html#live-demo",
    "title": "Store Data Safely",
    "section": "Live demo",
    "text": "Live demo\n\nLink RStudio to Posit Connect (authenticate)\nConnect to the board\nWrite a new pin\nCheck pin status and details\nPin versions\nUse pinned data\nUnpin your pin"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#should-i-use-it",
    "href": "presentations/2024-05-16_store-data-safely/index.html#should-i-use-it",
    "title": "Store Data Safely",
    "section": "Should I use it?",
    "text": "Should I use it?\n\n\n‚ö†Ô∏è {pins} is not great because:\n\nyou should not upload sensitive data!\nthere‚Äôs a file-size upload limit\npin organisation is a bit awkward (no subfolders)\n\n\n{pins} is helpful because:\n\nauthentication is straightforward\ndata can be versioned\nyou can control permissions\nthere are R and Python versions of the package"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#what-is-azure-data-storage",
    "href": "presentations/2024-05-16_store-data-safely/index.html#what-is-azure-data-storage",
    "title": "Store Data Safely",
    "section": "What is Azure Data Storage?",
    "text": "What is Azure Data Storage?\nMicrosoft cloud storage for unstructured data or ‚Äòblobs‚Äô (Binary Large Objects): data objects in binary form that do not necessarily conform to any file format.\nHow is it different?\n\nNo hierarchy ‚Äì although you can make pseudo-‚Äòfolders‚Äô with the blobnames.\nAuthenticates with your Microsoft account."
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#authenticating-to-azure-data-storage",
    "href": "presentations/2024-05-16_store-data-safely/index.html#authenticating-to-azure-data-storage",
    "title": "Store Data Safely",
    "section": "Authenticating to Azure Data Storage",
    "text": "Authenticating to Azure Data Storage\n\nYou are all part of the ‚Äústrategy-unit-analysts‚Äù group; this gives you read/write access to specific Azure storage containers.\nYou can store sensitive information like the container ID in a local .Renviron or .env file that should be ignored by git.\nUsing {AzureAuth}, {AzureStor} and your credentials, you can connect to the Azure storage container, upload files and download them, or read the files directly from storage!"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#step-1-load-your-environment-variables",
    "href": "presentations/2024-05-16_store-data-safely/index.html#step-1-load-your-environment-variables",
    "title": "Store Data Safely",
    "section": "Step 1: load your environment variables",
    "text": "Step 1: load your environment variables\nStore sensitive info in an .Renviron file that‚Äôs kept out of your Git history! The info can then be loaded in your script.\n.Renviron:\nAZ_STORAGE_EP=https://STORAGEACCOUNT.blob.core.windows.net/\nScript:\nep_uri &lt;- Sys.getenv(\"AZ_STORAGE_EP\")\nTip: reload .Renviron with readRenviron(\".Renviron\")"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#step-1-load-your-environment-variables-1",
    "href": "presentations/2024-05-16_store-data-safely/index.html#step-1-load-your-environment-variables-1",
    "title": "Store Data Safely",
    "section": "Step 1: load your environment variables",
    "text": "Step 1: load your environment variables\nIn the demo script we are providing, you will need these environment variables:\nep_uri &lt;- Sys.getenv(\"AZ_STORAGE_EP\")\napp_id &lt;- Sys.getenv(\"AZ_APP_ID\")\ncontainer_name &lt;- Sys.getenv(\"AZ_STORAGE_CONTAINER\")\ntenant &lt;- Sys.getenv(\"AZ_TENANT_ID\")"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#step-2-authenticate-with-azure",
    "href": "presentations/2024-05-16_store-data-safely/index.html#step-2-authenticate-with-azure",
    "title": "Store Data Safely",
    "section": "Step 2: Authenticate with Azure",
    "text": "Step 2: Authenticate with Azure\n\n\ntoken &lt;- AzureAuth::get_azure_token(\n  \"https://storage.azure.com\",\n  tenant = tenant,\n  app = app_id,\n  auth_type = \"device_code\",\n)\nThe first time you do this, you will have link to authenticate in your browser and a code in your terminal to enter. Use the browser that works best with your @mlcsu.nhs.uk account!"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#step-3-connect-to-container",
    "href": "presentations/2024-05-16_store-data-safely/index.html#step-3-connect-to-container",
    "title": "Store Data Safely",
    "section": "Step 3: Connect to container",
    "text": "Step 3: Connect to container\nendpoint &lt;- AzureStor::blob_endpoint(ep_uri, token = token)\ncontainer &lt;- AzureStor::storage_container(endpoint, container_name)\n\n# List files in container\nblob_list &lt;- AzureStor::list_blobs(container)\nIf you get 403 error, delete your token and re-authenticate, try a different browser/incognito, etc.\nTo clear Azure tokens: AzureAuth::clean_token_directory()"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#interact-with-the-container",
    "href": "presentations/2024-05-16_store-data-safely/index.html#interact-with-the-container",
    "title": "Store Data Safely",
    "section": "Interact with the container",
    "text": "Interact with the container\nIt‚Äôs possible to interact with the container via your browser!\nYou can upload and download files using the Graphical User Interface (GUI), login with your @mlcsu.nhs.uk account: https://portal.azure.com/#home\nAlthough it‚Äôs also cooler to interact via code‚Ä¶ üòé"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#interact-with-the-container-1",
    "href": "presentations/2024-05-16_store-data-safely/index.html#interact-with-the-container-1",
    "title": "Store Data Safely",
    "section": "Interact with the container",
    "text": "Interact with the container\n# Upload contents of a local directory to container\nAzureStor::storage_multiupload(\n  container,\n  \"LOCAL_FOLDERNAME/*\",\n  \"FOLDERNAME_ON_AZURE\"\n)\n\n# Upload specific file to container\nAzureStor::storage_upload(\n  container,\n  \"data/ronald.jpeg\",\n  \"newdir/ronald.jpeg\"\n)"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#load-csv-files-directly-from-azure-container",
    "href": "presentations/2024-05-16_store-data-safely/index.html#load-csv-files-directly-from-azure-container",
    "title": "Store Data Safely",
    "section": "Load csv files directly from Azure container",
    "text": "Load csv files directly from Azure container\ndf_from_azure &lt;- AzureStor::storage_read_csv(\n  container,\n  \"newdir/cats.csv\",\n  show_col_types = FALSE\n)\n\n# Load file directly from Azure container (by storing it in memory)\n\nparquet_in_memory &lt;- AzureStor::storage_download(\n  container, src = \"newdir/cats.parquet\", dest = NULL\n)\n\nparq_df &lt;- arrow::read_parquet(parquet_in_memory)"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#interact-with-the-container-2",
    "href": "presentations/2024-05-16_store-data-safely/index.html#interact-with-the-container-2",
    "title": "Store Data Safely",
    "section": "Interact with the container",
    "text": "Interact with the container\n# Delete from Azure container (!!!)\nAzureStor::delete_storage_file(container, BLOB_NAME)"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#what-does-this-achieve",
    "href": "presentations/2024-05-16_store-data-safely/index.html#what-does-this-achieve",
    "title": "Store Data Safely",
    "section": "What does this achieve?",
    "text": "What does this achieve?\n\nData is not in the repository, it is instead stored in a secure location\nCode can be open ‚Äì sensitive information like Azure container name stored as environment variables\nLarge filesizes possible, other people can also access the same container.\nNaming conventions can help to keep blobs organised (these create pseudo-folders)"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-is-rap",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-is-rap",
    "title": "RAP",
    "section": "What is RAP",
    "text": "What is RAP\n\na process in which code is used to minimise manual, undocumented steps, and a clear, properly documented process is produced in code which can reliably give the same result from the same dataset\nRAP should be:\n\n\nthe core working practice that must be supported by all platforms and teams; make this a core focus of NHS analyst training\n\nGoldacre review"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-are-we-trying-to-achieve",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-are-we-trying-to-achieve",
    "title": "RAP",
    "section": "What are we trying to achieve?",
    "text": "What are we trying to achieve?\n\nLegibility\nReproducibility\nAccuracy\nLaziness"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-are-some-of-the-fundamental-principles",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-are-some-of-the-fundamental-principles",
    "title": "RAP",
    "section": "What are some of the fundamental principles?",
    "text": "What are some of the fundamental principles?\n\nPredictability, reducing mental load, and reducing truck factor\nMaking it easy to collaborate with yourself and others on different computers, in the cloud, in six months‚Äô time‚Ä¶\nDRY"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#the-road-to-rap",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#the-road-to-rap",
    "title": "RAP",
    "section": "The road to RAP",
    "text": "The road to RAP\n\nWe‚Äôre roughly using NHS Digital‚Äôs RAP stages\nThere is an incredibly large amount to learn!\nConfession time! (everything I do not know‚Ä¶)\nYou don‚Äôt need to do it all at once\nYou don‚Äôt need to do it all at all ever\nEach thing you learn will incrementally help you\nRemember- that‚Äôs why we learnt this stuff. Because it helped us. And it can help you too"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--baseline",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--baseline",
    "title": "RAP",
    "section": "Levels of RAP- Baseline",
    "text": "Levels of RAP- Baseline\n\nData produced by code in an open-source language (e.g., Python, R, SQL).\nCode is version controlled (see Git basics and using Git collaboratively guides).\nRepository includes a README.md file (or equivalent) that clearly details steps a user must follow to reproduce the code\nCode has been peer reviewed.\nCode is published in the open and linked to & from accompanying publication (if relevant).\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--silver",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--silver",
    "title": "RAP",
    "section": "Levels of RAP- Silver",
    "text": "Levels of RAP- Silver\n\nCode is well-documented‚Ä¶\nCode is well-organised following standard directory format\nReusable functions and/or classes are used where appropriate\nPipeline includes a testing framework\nRepository includes dependency information (e.g.¬†requirements.txt, PipFile, environment.yml\nData is handled and output in a Tidy data format\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--gold",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--gold",
    "title": "RAP",
    "section": "Levels of RAP- Gold",
    "text": "Levels of RAP- Gold\n\nCode is fully packaged\nRepository automatically runs tests etc. via CI/CD or a different integration/deployment tool e.g.¬†GitHub Actions\nProcess runs based on event-based triggers (e.g., new data in database) or on a schedule\nChanges to the RAP are clearly signposted. E.g. a changelog in the package, releases etc. (See gov.uk info on Semantic Versioning)\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#a-learning-journey-to-get-you-there",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#a-learning-journey-to-get-you-there",
    "title": "RAP",
    "section": "A learning journey to get you there",
    "text": "A learning journey to get you there\n\nCode style, organising your files\nFunctions and iteration\nGit and GitHub\nPackaging your code\nTesting\nPackage management and versioning"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#how-we-can-help-each-other-get-there",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#how-we-can-help-each-other-get-there",
    "title": "RAP",
    "section": "How we can help each other get there",
    "text": "How we can help each other get there\n\nWork as a team!\nCoffee and coding!\nAsk for help!\nDo pair coding!\nGet your code reviewed!\nJoin the NHS-R/ NHSPycom communities"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#haca",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#haca",
    "title": "RAP",
    "section": "HACA",
    "text": "HACA\n\nThe first national analytics conference for health and care\nInsight to action!\nJuly 11th and 12th, University of Birmingham\nAccepting abstracts for short and long talks and posters\nAbstract deadline 27th March\nHelp is available (with abstract, poster, preparing presentation‚Ä¶)!"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#health-data-in-the-headlines",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#health-data-in-the-headlines",
    "title": "System Dynamics in health and care",
    "section": "Health Data in the Headlines",
    "text": "Health Data in the Headlines\n\n\n\n\nUsed to seeing headlines that give a snapshot figure but doesn‚Äôt say much about the system.\nNow starting to see headlines that recognise flow through the system rather than snapshot in time of just one part.\nCan get better understanding of the issues in a system if we can map it as stocks and flows, but our datasets not designed to give up this information very readily. This talk is how I have tried to meet that challenge."
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#through-the-system-dynamics-lens",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#through-the-system-dynamics-lens",
    "title": "System Dynamics in health and care",
    "section": "Through the System Dynamics lens",
    "text": "Through the System Dynamics lens\n\nStock-flow model\nDynamic behaviour, feedback loops\n\nIn a few seconds, what is SD?\nAn approach to understanding the behaviour of complex systems over time. A method of mapping a system as stocks, whose levels can only change due to flows in and flows out. Stocks could be people on a waiting list, on a ward, money, ‚Ä¶\nFlows are the rate at which things change in a given time period e.g.¬†admissions per day, referrals per month.\nBehaviour of the system is determined by how the components interact with each other, not what each component does. Mapping the structure of a system like this leads us to identify feedback loops, and consequences of an action - both intended and unintended.\nIn this capacity-constrained model we only need 3 parameters to run the model (exogenous). All the behaviour within the grey box is determined by the interactions of those components (indogenous).\nHow do we get a value/values for referrals per day?\n(currently use specialist software to build and run our models, aim is to get to a point where we can run in open source.)"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#determining-flows",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#determining-flows",
    "title": "System Dynamics in health and care",
    "section": "Determining flows",
    "text": "Determining flows\n\n\n\n\n‚Äòadmissions per day‚Äô is needed to populate the model.\n‚Äòdischarged‚Äô could be used to verify the model against known data\n\nHow many admissions per day (or week, month‚Ä¶)\n\n\n\n\n\n\n\n   \n\n\nGoing to use very simple model shown to explain how to extract flow data for admissions. Will start with visual explainer before going into the code.\n1. generate list of key dates (in this case daily, could be weekly, monthly)\n2. take our patient-level ID with admission and discharge dates\n3. count of admissions on that day/week"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#determining-occupancy",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#determining-occupancy",
    "title": "System Dynamics in health and care",
    "section": "Determining occupancy",
    "text": "Determining occupancy\n\n\n\n\n‚Äòon ward‚Äô is used to verify the model against known data\n\nLogic statement testing if the key date is wholly between admission and discharge dates\nflag for a match \n\n\n\n\n\n\n     \n\n\nMight also want to generate occupancy, to compare the model output with actual data to verify/validate.\n1. generate list of key dates\n2. take our patient-level ID with admission and discharge dates\n3. going to take each date in our list of keydates, and see if there is an admission before that date and discharge after 4. this creates a wide data frame, the same length as patient data.\n5. once run through all the dates in the list, sum each column\nPatient A admitted on 2nd, so only starts being classed as resident on 3rd."
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---flows",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---flows",
    "title": "System Dynamics in health and care",
    "section": "in R - flows",
    "text": "in R - flows\nEasy to do with count, or group_by and summarise\n\n\n  admit_d &lt;- spell_dates |&gt; \n  group_by(date_admit) |&gt;\n  count(date_admit)\n\nhead(admit_d)\n\n\n# A tibble: 6 √ó 2\n# Groups:   date_admit [6]\n  date_admit     n\n  &lt;date&gt;     &lt;int&gt;\n1 2022-01-01    22\n2 2022-01-02    24\n3 2022-01-03    18\n4 2022-01-04    19\n5 2022-01-05    23\n6 2022-01-06    27"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---occupancy",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---occupancy",
    "title": "System Dynamics in health and care",
    "section": "in R - occupancy",
    "text": "in R - occupancy\nGenerate list of key dates\n\n\n\ndate_start &lt;- dmy(01012022) \ndate_end &lt;- dmy(31012022)\nrun_len &lt;- length(seq(from = date_start, to = date_end, by = \"day\"))\n\nkeydates &lt;- data.frame(\n  date = c(seq(date_start, by = \"day\", length.out=run_len)))  \n\n\n\n\n        date\n1 2022-01-01\n2 2022-01-02\n3 2022-01-03\n4 2022-01-04\n5 2022-01-05\n6 2022-01-06\n\n\n\n\nStart by generating the list of keydates. In this example we‚Äôre running the model in days, and checking each day in 2022.\nNeed the run length for the next step, to know how many times to iterate over"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---occupancy-1",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---occupancy-1",
    "title": "System Dynamics in health and care",
    "section": "in R - occupancy",
    "text": "in R - occupancy\nIterate over each date - need to have been admitted before, and discharged after\n\noccupancy_flag &lt;- function(df) {\n\n # pre-allocate tibble size to speed up iteration in loop\n  activity_all &lt;- tibble(nrow = nrow(df))  |&gt;  \n    select()\n \n   for (i in 1:run_len) {\n     \n      activity_period &lt;-  case_when(\n     \n      # creates 1 flag if resident for complete day\n      df$date_admit &lt; keydates$keydate[i] & \n        df$date_discharge &gt; keydates$keydate[i] ~ 1,\n      TRUE ~ 0)\n   \n      # column bind this day's flags to previous\n      activity_all &lt;- bind_cols(activity_all, activity_period)\n \n   }\n  \n    # rename column to match the day being counted\n  activity_all &lt;- activity_all |&gt; \n    setNames(paste0(\"d_\", keydates$date))\n    \n  # bind flags columns to patient data\n  daily_adm &lt;- bind_cols(df, activity_all) |&gt; \n    pivot_longer(\n      cols = starts_with(\"d_\"),\n      names_to = \"date\",\n      values_to = \"count\"\n    ) |&gt; \n    \n    group_by(date) |&gt; \n    summarise(resident = sum(count)) |&gt; \n    ungroup() |&gt; \n  mutate(date = str_remove(date, \"d_\"))\n   \n } \n\n\nIs there a better way than using a for loop?\n\nPre-allocate tibbles\nactivity_all will end up as very wide tibble, with a column for each date in list of keydates.\nFor each date in the list of key dates, compares with admission date & discharge date; need to be admitted before the key date and discharged after the key date. If match, flag = 1.\nCreates a column for each day, then binds this to activity all.\nRename each column with the date it was checking (add a character to start of column name so column doesn‚Äôt start with numeric)\nPivot long, then group by date and sum the flags (other variables could be added here, such as TFC or provider code)"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods---flows",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods---flows",
    "title": "System Dynamics in health and care",
    "section": "Longer Time Periods - flows",
    "text": "Longer Time Periods - flows\nUse lubridate::floor_date to generate the date at start of week/month\n\nadmit_wk &lt;- spell_dates |&gt; \n  mutate(week_start = floor_date(\n    date_admit, unit = \"week\", week_start = 1   # start week on Monday\n  )) |&gt; \n  count(week_start)     # could add other parameters such as provider code, TFC etc\n\nhead(admit_wk)\n\n\n\n# A tibble: 6 √ó 2\n  week_start     n\n  &lt;date&gt;     &lt;int&gt;\n1 2021-12-27    46\n2 2022-01-03   162\n3 2022-01-10   188\n4 2022-01-17   177\n5 2022-01-24   193\n6 2022-01-31   188\n\n\n\nMight run SD model in weeks or months - e.g.¬†months for care homes Use lubridate to create new variable with start date of week/month/year etc"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods---occupancy",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods---occupancy",
    "title": "System Dynamics in health and care",
    "section": "Longer Time Periods - occupancy",
    "text": "Longer Time Periods - occupancy\nKey dates to include the dates at the start and end of each time period\n\n\n\ndate_start &lt;- dmy(03012022) # first Monday of the year\ndate_end &lt;- dmy(01012023)\nrun_len &lt;- length(seq(from = date_start, to = date_end, by = \"week\"))\n\nkeydates &lt;- data.frame(wk_start = c(seq(date_start, \n                                        by = \"week\", \n                                        length.out=run_len))) |&gt;  \n  mutate(\n    wk_end = wk_start + 6)    # last date in time period\n\n\n\n\n    wk_start     wk_end\n1 2022-01-03 2022-01-09\n2 2022-01-10 2022-01-16\n3 2022-01-17 2022-01-23\n4 2022-01-24 2022-01-30\n5 2022-01-31 2022-02-06\n6 2022-02-07 2022-02-13\n\n\n\n\nModel might make more sense to run in weeks or months (e.g.¬†care home), so list of keydates need a start date and end date for each time period."
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods",
    "title": "System Dynamics in health and care",
    "section": "Longer Time Periods",
    "text": "Longer Time Periods\nMore logic required if working in weeks or months - can only be in one place at any given time\n\n# flag for occupancy\nactivity_period &lt;-  case_when(\n  \n        # creates 1 flag if resident for complete week\n      df$date_admit &lt; keydates$wk_start[i] & df$date_discharge &gt; keydates$wk_end[i] ~ 1,\n        TRUE ~ 0)\n\n\nAnd a little bit more logic\nOccupancy requires the patient to have been admitted before the start of the week/month, and discharged after the end of the week/month"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#applying-the-data",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#applying-the-data",
    "title": "System Dynamics in health and care",
    "section": "Applying the data",
    "text": "Applying the data\n\n\nHow to apply this wrangling of data to the system dynamic model?\nAdmissions data used as an input to the flow - could be reduced to a single figure (average), or there may be variation by season/day of week etc.\nOccupancy (and discharges) used to verify the model output against known data."
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#next-steps",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#next-steps",
    "title": "System Dynamics in health and care",
    "section": "Next Steps",
    "text": "Next Steps\n\nGeneralise function to a state where it can be used by others - onto Github\nTurn this into a package\nOpen-source SD models and interfaces - R Shiny or Python"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#questions-comments-suggestions",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#questions-comments-suggestions",
    "title": "System Dynamics in health and care",
    "section": "Questions, comments, suggestions?",
    "text": "Questions, comments, suggestions?\n\n\n\nPlease get in touch!\n\nSally.Thompson37@nhs.net"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#what-is-testing",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#what-is-testing",
    "title": "Unit testing in R",
    "section": "What is testing?",
    "text": "What is testing?\n\nSoftware testing is the act of examining the artifacts and the behavior of the software under test by validation and verification. Software testing can also provide an objective, independent view of the software to allow the business to appreciate and understand the risks of software implementation\nwikipedia"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-can-we-test-our-code",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-can-we-test-our-code",
    "title": "Unit testing in R",
    "section": "How can we test our code?",
    "text": "How can we test our code?\n\n\nStatically\n\n\n(without executing the code)\nhappens constantly, as we are writing code\nvia code reviews\ncompilers/interpreters/linters statically analyse the code for syntax errors\n\n\n\n\n\nDynamically"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-can-we-test-our-code-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-can-we-test-our-code-1",
    "title": "Unit testing in R",
    "section": "How can we test our code?",
    "text": "How can we test our code?\n\n\nStatically\n\n(without executing the code)\nhappens constantly, as we are writing code\nvia code reviews\ncompilers/interpreters/linters statically analyse the code for syntax errors\n\n\n\n\nDynamically\n\n\n(by executing the code)\nsplit into functional and non-functional testing\ntesting can be manual, or automated\n\n\n\n\n\nnon-functional testing covers things like performance, security, and usability testing"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests",
    "title": "Unit testing in R",
    "section": "Different types of functional tests",
    "text": "Different types of functional tests\nUnit Testing checks each component (or unit) for accuracy independently of one another.\n\nIntegration Testing integrates units to ensure that the code works together.\n\n\nEnd-to-End Testing (e2e) makes sure that the entire system functions correctly.\n\n\nUser Acceptance Testing (UAT) ensures that the product meets the real user‚Äôs requirements."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests-1",
    "title": "Unit testing in R",
    "section": "Different types of functional tests",
    "text": "Different types of functional tests\nUnit Testing checks each component (or unit) for accuracy independently of one another.\nIntegration Testing integrates units to ensure that the code works together.\nEnd-to-End Testing (e2e) makes sure that the entire system functions correctly.\n\nUser Acceptance Testing (UAT) ensures that the product meets the real user‚Äôs requirements.\n\n\nUnit, Integration, and E2E testing are all things we can automate in code, whereas UAT testing is going to be manual"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests-2",
    "title": "Unit testing in R",
    "section": "Different types of functional tests",
    "text": "Different types of functional tests\nUnit Testing checks each component (or unit) for accuracy independently of one another.\n\nIntegration Testing integrates units to ensure that the code works together.\nEnd-to-End Testing (e2e) makes sure that the entire system functions correctly.\nUser Acceptance Testing (UAT) ensures that the product meets the real user‚Äôs requirements.\n\n\nOnly focussing on unit testing in this talk, but the techniques/packages could be extended to integration testing. Often other tools (potentially specific tools) are needed for E2E testing."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#example",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#example",
    "title": "Unit testing in R",
    "section": "Example",
    "text": "Example\nWe have a {shiny} app which grabs some data from a database, manipulates the data, and generates a plot.\n\n\nwe would write unit tests to check the data manipulation and plot functions work correctly (with pre-created sample/simple datasets)\nwe would write integration tests to check that the data manipulation function works with the plot function (with similar data to what we used for the unit tests)\nwe would write e2e tests to ensure that from start to finish the app grabs the data and produces a plot as required\n\n\n\nsimple (unit tests) to complex (e2e tests)"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-pyramid",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-pyramid",
    "title": "Unit testing in R",
    "section": "Testing Pyramid",
    "text": "Testing Pyramid\n\n\nImage source: The Testing Pyramid: Simplified for One and All headspin.io"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function",
    "title": "Unit testing in R",
    "section": "Let‚Äôs create a simple function‚Ä¶",
    "text": "Let‚Äôs create a simple function‚Ä¶\n\nmy_function &lt;- function(x, y) {\n  \n  stopifnot(\n    \"x must be numeric\" = is.numeric(x),\n    \"y must be numeric\" = is.numeric(y),\n    \"x must be same length as y\" = length(x) == length(y),\n    \"cannot divide by zero!\" = y != 0\n  )\n\n  x / y\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function-1",
    "title": "Unit testing in R",
    "section": "Let‚Äôs create a simple function‚Ä¶",
    "text": "Let‚Äôs create a simple function‚Ä¶\n\nmy_function &lt;- function(x, y) {\n  \n  stopifnot(\n    \"x must be numeric\" = is.numeric(x),\n    \"y must be numeric\" = is.numeric(y),\n    \"x must be same length as y\" = length(x) == length(y),\n    \"cannot divide by zero!\" = y != 0\n  )\n\n  x / y\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function-2",
    "title": "Unit testing in R",
    "section": "Let‚Äôs create a simple function‚Ä¶",
    "text": "Let‚Äôs create a simple function‚Ä¶\n\nmy_function &lt;- function(x, y) {\n  \n  stopifnot(\n    \"x must be numeric\" = is.numeric(x),\n    \"y must be numeric\" = is.numeric(y),\n    \"x must be same length as y\" = length(x) == length(y),\n    \"cannot divide by zero!\" = y != 0\n  )\n\n  x / y\n}\n\n\nThe Ten Rules of Defensive Programming in R"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test",
    "title": "Unit testing in R",
    "section": "‚Ä¶ and create our first test",
    "text": "‚Ä¶ and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-1",
    "title": "Unit testing in R",
    "section": "‚Ä¶ and create our first test",
    "text": "‚Ä¶ and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-2",
    "title": "Unit testing in R",
    "section": "‚Ä¶ and create our first test",
    "text": "‚Ä¶ and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-3",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-3",
    "title": "Unit testing in R",
    "section": "‚Ä¶ and create our first test",
    "text": "‚Ä¶ and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-4",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-4",
    "title": "Unit testing in R",
    "section": "‚Ä¶ and create our first test",
    "text": "‚Ä¶ and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-5",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-5",
    "title": "Unit testing in R",
    "section": "‚Ä¶ and create our first test",
    "text": "‚Ä¶ and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})\n\nTest passed üò∏"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#other-expect_-functions",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#other-expect_-functions",
    "title": "Unit testing in R",
    "section": "other expect_*() functions‚Ä¶",
    "text": "other expect_*() functions‚Ä¶\n\ntest_that(\"my_function correctly divides values\", {\n  expect_lt(\n    my_function(4, 2),\n    10\n  )\n  expect_gt(\n    my_function(1, 4),\n    0.2\n  )\n  expect_length(\n    my_function(c(4, 1), c(2, 4)),\n    2\n  )\n})\n\nTest passed üéä\n\n\n\n{testthat} documentation"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert",
    "title": "Unit testing in R",
    "section": "Arrange, Act, Assert",
    "text": "Arrange, Act, Assert\n\n\n\n\n\ntest_that(\"my_function works\", {\n  # arrange\n  #  \n  #\n  #\n\n  # act\n  #\n\n  # assert\n  #\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-1",
    "title": "Unit testing in R",
    "section": "Arrange, Act, Assert",
    "text": "Arrange, Act, Assert\n\n\nwe arrange the environment, before running the function\n\n\nto create sample values\ncreate fake/temporary files\nset random seed\nset R options/environment variables\n\n\n\n\ntest_that(\"my_function works\", {\n  # arrange\n  x &lt;- 5\n  y &lt;- 7\n  expected &lt;- 0.714285\n\n  # act\n  #\n\n  # assert\n  #\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-2",
    "title": "Unit testing in R",
    "section": "Arrange, Act, Assert",
    "text": "Arrange, Act, Assert\n\n\nwe arrange the environment, before running the function\nwe act by calling the function\n\n\ntest_that(\"my_function works\", {\n  # arrange\n  x &lt;- 5\n  y &lt;- 7\n  expected &lt;- 0.714285\n\n  # act\n  actual &lt;- my_function(x, y)\n\n  # assert\n  #\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-3",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-3",
    "title": "Unit testing in R",
    "section": "Arrange, Act, Assert",
    "text": "Arrange, Act, Assert\n\n\nwe arrange the environment, before running the function\nwe act by calling the function\nwe assert that the actual results match our expected results\n\n\ntest_that(\"my_function works\", {\n  # arrange\n  x &lt;- 5\n  y &lt;- 7\n  expected &lt;- 0.714285\n\n  # act\n  actual &lt;- my_function(x, y)\n\n  # assert\n  expect_equal(actual, expected)\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#our-test-failed",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#our-test-failed",
    "title": "Unit testing in R",
    "section": "Our test failed!?! üò¢",
    "text": "Our test failed!?! üò¢\n\ntest_that(\"my_function works\", {\n  # arrange\n  x &lt;- 5\n  y &lt;- 7\n  expected &lt;- 0.714285\n\n  # act\n  actual &lt;- my_function(x, y)\n\n  # assert\n  expect_equal(actual, expected)\n})\n\n‚îÄ‚îÄ Failure: my_function works ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n`actual` not equal to `expected`.\n1/1 mismatches\n[1] 0.714 - 0.714 == 7.14e-07\n\n\nError:\n! Test failed"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#tolerance-to-the-rescue",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#tolerance-to-the-rescue",
    "title": "Unit testing in R",
    "section": "Tolerance to the rescue üôÇ",
    "text": "Tolerance to the rescue üôÇ\n\ntest_that(\"my_function works\", {\n  # arrange\n  x &lt;- 5\n  y &lt;- 7\n  expected &lt;- 0.714285\n\n  # act\n  actual &lt;- my_function(x, y)\n\n  # assert\n  expect_equal(actual, expected, tolerance = 1e-6)\n})\n\nTest passed üò∏\n\n\n\n(this is a slightly artificial example, usually the default tolerance is good enough)"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-edge-cases",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-edge-cases",
    "title": "Unit testing in R",
    "section": "Testing edge cases",
    "text": "Testing edge cases\n\n\nRemember the validation steps we built into our function to handle edge cases?\n\nLet‚Äôs write tests for these edge cases:\nwe expect errors\n\n\ntest_that(\"my_function works\", {\n  expect_error(my_function(5, 0))\n  expect_error(my_function(\"a\", 3))\n  expect_error(my_function(3, \"a\"))\n  expect_error(my_function(1:2, 4))\n})\n\nTest passed üéä"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#another-simple-example",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#another-simple-example",
    "title": "Unit testing in R",
    "section": "Another (simple) example",
    "text": "Another (simple) example\n\n\n\nmy_new_function &lt;- function(x, y) {\n  if (x &gt; y) {\n    \"x\"\n  } else {\n    \"y\"\n  }\n}\n\n\nConsider this function - there is branched logic, so we need to carefully design tests to validate the logic works as intended."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#another-simple-example-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#another-simple-example-1",
    "title": "Unit testing in R",
    "section": "Another (simple) example",
    "text": "Another (simple) example\n\nmy_new_function &lt;- function(x, y) {\n  if (x &gt; y) {\n    \"x\"\n  } else {\n    \"y\"\n  }\n}\n\n\n\ntest_that(\"it returns 'x' if x is bigger than y\", {\n  expect_equal(my_new_function(4, 3), \"x\")\n})\n\nTest passed üåà\n\ntest_that(\"it returns 'y' if y is bigger than x\", {\n  expect_equal(my_new_function(3, 4), \"y\")\n  expect_equal(my_new_function(3, 3), \"y\")\n})\n\nTest passed üòÄ"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-to-design-good-tests",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-to-design-good-tests",
    "title": "Unit testing in R",
    "section": "How to design good tests",
    "text": "How to design good tests\na non-exhaustive list\n\nconsider all the functions arguments,\nwhat are the expected values for these arguments?\nwhat are unexpected values, and are they handled?\nare there edge cases that need to be handled?\nhave you covered all of the different paths in your code?\nhave you managed to create tests that check the range of results you expect?"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#but-why-create-tests",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#but-why-create-tests",
    "title": "Unit testing in R",
    "section": "But, why create tests?",
    "text": "But, why create tests?\nanother non-exhaustive list\n\ngood tests will help you uncover existing issues in your code\nwill defend you from future changes that break existing functionality\nwill alert you to changes in dependencies that may have changed the functionality of your code\ncan act as documentation for other developers"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-complex-functions",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-complex-functions",
    "title": "Unit testing in R",
    "section": "Testing complex functions",
    "text": "Testing complex functions\n\n\n\nmy_big_function &lt;- function(type) {\n  con &lt;- dbConnect(RSQLite::SQLite(), \"data.db\")\n  df &lt;- tbl(con, \"data_table\") |&gt;\n    collect() |&gt;\n    mutate(across(date, lubridate::ymd))\n\n  conditions &lt;- read_csv(\n    \"conditions.csv\", col_types = \"cc\"\n  ) |&gt;\n    filter(condition_type == type)\n\n  df |&gt;\n    semi_join(conditions, by = \"condition\") |&gt;\n    count(date) |&gt;\n    ggplot(aes(date, n)) +\n    geom_line() +\n    geom_point()\n}\n\n\nWhere do you even begin to start writing tests for something so complex?\n\n\nNote: to get the code on the left to fit on one page, I skipped including a few library calls\n\nlibrary(tidyverse)\nlibrary(DBI)"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions",
    "title": "Unit testing in R",
    "section": "Split the logic into smaller functions",
    "text": "Split the logic into smaller functions\nFunction to get the data from the database\n\nget_data_from_sql &lt;- function() {\n  con &lt;- dbConnect(RSQLite::SQLite(), \"data.db\")\n  tbl(con, \"data_table\") |&gt;\n    collect() |&gt;\n    mutate(across(date, lubridate::ymd))\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-1",
    "title": "Unit testing in R",
    "section": "Split the logic into smaller functions",
    "text": "Split the logic into smaller functions\nFunction to get the relevant conditions\n\nget_conditions &lt;- function(type) {\n  read_csv(\n    \"conditions.csv\", col_types = \"cc\"\n  ) |&gt;\n    filter(condition_type == type)\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-2",
    "title": "Unit testing in R",
    "section": "Split the logic into smaller functions",
    "text": "Split the logic into smaller functions\nFunction to combine the data and create a count by date\n\nsummarise_data &lt;- function(df, conditions) {\n  df |&gt;\n    semi_join(conditions, by = \"condition\") |&gt;\n    count(date)\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-3",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-3",
    "title": "Unit testing in R",
    "section": "Split the logic into smaller functions",
    "text": "Split the logic into smaller functions\nFunction to generate a plot from the summarised data\n\ncreate_plot &lt;- function(df) {\n  df |&gt;\n    ggplot(aes(date, n)) +\n    geom_line() +\n    geom_point()\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-4",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-4",
    "title": "Unit testing in R",
    "section": "Split the logic into smaller functions",
    "text": "Split the logic into smaller functions\nThe original function refactored to use the new functions\n\nmy_big_function &lt;- function(type) {\n  conditions &lt;- get_conditions(type)\n\n  get_data_from_sql() |&gt;\n    summarise_data(conditions) |&gt;\n    create_plot()\n}\n\n\nThis is going to be significantly easier to test, because we now can verify that the individual components work correctly, rather than having to consider all of the possibilities at once."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data",
    "title": "Unit testing in R",
    "section": "Let‚Äôs test summarise_data",
    "text": "Let‚Äôs test summarise_data\nsummarise_data &lt;- function(df, conditions) {\n  df |&gt;\n    semi_join(conditions, by = \"condition\") |&gt;\n    count(date)\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-1",
    "title": "Unit testing in R",
    "section": "Let‚Äôs test summarise_data",
    "text": "Let‚Äôs test summarise_data\ntest_that(\"it summarises the data\", {\n  # arrange\n  \n\n\n\n\n\n\n  \n\n  \n  # act\n  \n  # assert\n  \n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-2",
    "title": "Unit testing in R",
    "section": "Let‚Äôs test summarise_data",
    "text": "Let‚Äôs test summarise_data\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  \n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  \n\n\n\n\n  # act\n  \n  # assert\n  \n})\n\nGenerate some random data to build a reasonably sized data frame.\nYou could also create a table manually, but part of the trick of writing good tests for this function is to make it so the dates don‚Äôt all have the same count.\nThe reason for this is it‚Äôs harder to know for sure that the count worked if every row returns the same value.\nWe don‚Äôt need the values to be exactly like they are in the real data, just close enough. Instead of dates, we can use numbers, and instead of actual conditions, we can use letters."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-3",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-3",
    "title": "Unit testing in R",
    "section": "Let‚Äôs test summarise_data",
    "text": "Let‚Äôs test summarise_data\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  set.seed(123)\n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  \n\n\n\n\n  # act\n  \n  # assert\n  \n})\n\nTests need to be reproducible, and generating our table at random will give us unpredictable results.\nSo, we need to set the random seed; now every time this test runs we will generate the same data."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-4",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-4",
    "title": "Unit testing in R",
    "section": "Let‚Äôs test summarise_data",
    "text": "Let‚Äôs test summarise_data\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  set.seed(123)\n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  conditions &lt;- tibble(condition = c(\"a\", \"b\"))    \n  \n\n\n\n  # act\n  \n  # assert\n  \n})\n\nCreate the conditions table. We don‚Äôt need all of the columns that are present in the real csv, just the ones that will make our code work.\nWe also need to test that the filtering join (semi_join) is working, so we want to use a subset of the conditions that were used in df."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-5",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-5",
    "title": "Unit testing in R",
    "section": "Let‚Äôs test summarise_data",
    "text": "Let‚Äôs test summarise_data\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  set.seed(123)\n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  conditions &lt;- tibble(condition = c(\"a\", \"b\"))    \n  \n  \n\n  \n  # act\n  actual &lt;- summarise_data(df, conditions)\n  # assert\n  \n})\n\nBecause we are generating df randomly, to figure out what our ‚Äúexpected‚Äù results are, I simply ran the code inside of the test to generate the ‚Äúactual‚Äù results.\nGenerally, this isn‚Äôt a good idea. You are creating the results of your test from the code; ideally, you want to be thinking about what the results of your function should be.\nImagine your function doesn‚Äôt work as intended, there is some subtle bug that you are not yet aware of. By writing tests ‚Äúbackwards‚Äù you may write test cases that confirm the results, but not expose the bug. This is why it‚Äôs good to think about edge cases."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-6",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-6",
    "title": "Unit testing in R",
    "section": "Let‚Äôs test summarise_data",
    "text": "Let‚Äôs test summarise_data\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  set.seed(123)\n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  conditions &lt;- tibble(condition = c(\"a\", \"b\"))    \n  expected &lt;- tibble(\n    date = 1:10,\n    n = c(19, 18, 12, 14, 17, 18, 24, 18, 31, 21)\n  )  \n  # act\n  actual &lt;- summarise_data(df, conditions)\n  # assert\n  \n})\n\nThat said, in cases where we can be confident (say by static analysis of our code) that it is correct, building tests in this way will give us the confidence going forwards that future changes do not break existing functionality.\nIn this case, I have created the expected data frame using the results from running the function."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-7",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-7",
    "title": "Unit testing in R",
    "section": "Let‚Äôs test summarise_data",
    "text": "Let‚Äôs test summarise_data\n\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  set.seed(123)\n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  conditions &lt;- tibble(condition = c(\"a\", \"b\"))\n  expected &lt;- tibble(\n    date = 1:10,\n    n = c(19, 18, 12, 14, 17, 18, 24, 18, 31, 21)\n  )\n  # act\n  actual &lt;- summarise_data(df, conditions)\n  # assert\n  expect_equal(actual, expected)\n})\n\nTest passed üò∏\n\n\n\nThe test works!"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#next-steps",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#next-steps",
    "title": "Unit testing in R",
    "section": "Next steps",
    "text": "Next steps\n\nYou can add tests to any R project (to test functions),\nBut {testthat} works best with Packages\nThe R Packages book has 3 chapters on testing\nThere are two useful helper functions in {usethis}\n\nuse_testthat() will set up the folders for test scripts\nuse_test() will create a test file for the currently open script"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#next-steps-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#next-steps-1",
    "title": "Unit testing in R",
    "section": "Next steps",
    "text": "Next steps\n\nIf your test needs to temporarily create a file, or change some R-options, the {withr} package has a lot of useful functions that will automatically clean things up when the test finishes\nIf you are writing tests that involve calling out to a database, or you want to test my_big_function (from before) without calling the intermediate functions, then you should look at the {mockery} package"
  },
  {
    "objectID": "style/team.html",
    "href": "style/team.html",
    "title": "Team work",
    "section": "",
    "text": "In advanced of the weekly stand-up, all team members share a short summary, in Slack, answering the following questions:\n\n‚≠ê What went well last week\nü¶â What lessons did you learn\nüéØ Goal(s) for this week\n\nThere is also a Slack bot every Wednesday & Friday prompting a check-in. You don‚Äôt have to respond to this, but it‚Äôs a nice way to share any recent wins or blockers.\n\n\n\nEvery Monday from 10:00 - 10:50 we have a team meeting.\nWe start with a stand-up from two team members. These are randomly selected by a bot on Slack in advance of the meeting.\nIf it‚Äôs your week to talk, you have 5 minutes to chat about anything that matters to you. You could expand on your Slack stand-up, perhaps sharing your wins or learnings in more detail. You might also choose to share a project you are working on in, showing code or a shiny application. Chat about your weekend or an update on how you‚Äôre doing is also totally fine!\nWe then discuss any items that team members have added to the rolling agenda.\n\n\n\nYour line manager should arrange (1 : 1) meetings with you once a fortnight.\n\n\n\nWe use MS Teams to communicate about project related work. We have a private slack channel on NHS-R slack which we use for general team chat and more casually. (Slack has much better emojis ü§∏üéÆü¶•)\n\n\n\nOne of the ways we celebrate each other and the successes we have is by presenting each other with a ‚Äútrophy‚Äù üèÜ in the slack channel. This is for good code, problem solving, helping someone - anything that deserves a special mention.\n\n\n\nCore hours vary considerably between team members and there is a lot of flexibility. Team members may send messages out of hours, but that is purely because it works for them, and will not expect anything immediate in response. Flexibility is not only allowed but recommended if it benefits you. Please agree core hours with your line manager and where possible communicate any changes in your core hours to team members via out of office on your calendar and/or Slack (e.g.¬†‚ÄúI‚Äôm out all morning because my car is broken - working later on‚Äù).\n\n\n\nThe team is primarily remote. The Strategy Unit has two all company gathering a year, in Birmingham. Currently the Data Science team don‚Äôt have any in-person meetings, although we might start doing them occasionally when it‚Äôs beneficial. Some of the London-based team coordinate when they work in the office.\n\n\n\nMost of the team‚Äôs time is working on a large project which we manage in an agile-like manner. We use GitHub to capture issues, and a project board to manage the sprint.\nTasks outside this project can be managed how you like. If you want to use GitHub, there is a Data Science project planner.\n\n\n\nWe organise a fortnightly coffee and code for analysts in the Strategy Unit. This is planned in a Coffee and Coding ‚òïüßë‚Äçüíª GitHub project. Materials from previous sessions are shared here as blogs or presentations.",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#slack-check-ins",
    "href": "style/team.html#slack-check-ins",
    "title": "Team work",
    "section": "",
    "text": "In advanced of the weekly stand-up, all team members share a short summary, in Slack, answering the following questions:\n\n‚≠ê What went well last week\nü¶â What lessons did you learn\nüéØ Goal(s) for this week\n\nThere is also a Slack bot every Wednesday & Friday prompting a check-in. You don‚Äôt have to respond to this, but it‚Äôs a nice way to share any recent wins or blockers.",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#weekly-stand-up",
    "href": "style/team.html#weekly-stand-up",
    "title": "Team work",
    "section": "",
    "text": "Every Monday from 10:00 - 10:50 we have a team meeting.\nWe start with a stand-up from two team members. These are randomly selected by a bot on Slack in advance of the meeting.\nIf it‚Äôs your week to talk, you have 5 minutes to chat about anything that matters to you. You could expand on your Slack stand-up, perhaps sharing your wins or learnings in more detail. You might also choose to share a project you are working on in, showing code or a shiny application. Chat about your weekend or an update on how you‚Äôre doing is also totally fine!\nWe then discuss any items that team members have added to the rolling agenda.",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#line-management",
    "href": "style/team.html#line-management",
    "title": "Team work",
    "section": "",
    "text": "Your line manager should arrange (1 : 1) meetings with you once a fortnight.",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#communication",
    "href": "style/team.html#communication",
    "title": "Team work",
    "section": "",
    "text": "We use MS Teams to communicate about project related work. We have a private slack channel on NHS-R slack which we use for general team chat and more casually. (Slack has much better emojis ü§∏üéÆü¶•)",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#data-science-trophy",
    "href": "style/team.html#data-science-trophy",
    "title": "Team work",
    "section": "",
    "text": "One of the ways we celebrate each other and the successes we have is by presenting each other with a ‚Äútrophy‚Äù üèÜ in the slack channel. This is for good code, problem solving, helping someone - anything that deserves a special mention.",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#working-times",
    "href": "style/team.html#working-times",
    "title": "Team work",
    "section": "",
    "text": "Core hours vary considerably between team members and there is a lot of flexibility. Team members may send messages out of hours, but that is purely because it works for them, and will not expect anything immediate in response. Flexibility is not only allowed but recommended if it benefits you. Please agree core hours with your line manager and where possible communicate any changes in your core hours to team members via out of office on your calendar and/or Slack (e.g.¬†‚ÄúI‚Äôm out all morning because my car is broken - working later on‚Äù).",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#working-location",
    "href": "style/team.html#working-location",
    "title": "Team work",
    "section": "",
    "text": "The team is primarily remote. The Strategy Unit has two all company gathering a year, in Birmingham. Currently the Data Science team don‚Äôt have any in-person meetings, although we might start doing them occasionally when it‚Äôs beneficial. Some of the London-based team coordinate when they work in the office.",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#task-management",
    "href": "style/team.html#task-management",
    "title": "Team work",
    "section": "",
    "text": "Most of the team‚Äôs time is working on a large project which we manage in an agile-like manner. We use GitHub to capture issues, and a project board to manage the sprint.\nTasks outside this project can be managed how you like. If you want to use GitHub, there is a Data Science project planner.",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#coffee-and-code",
    "href": "style/team.html#coffee-and-code",
    "title": "Team work",
    "section": "",
    "text": "We organise a fortnightly coffee and code for analysts in the Strategy Unit. This is planned in a Coffee and Coding ‚òïüßë‚Äçüíª GitHub project. Materials from previous sessions are shared here as blogs or presentations.",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/git_and_github.html",
    "href": "style/git_and_github.html",
    "title": "Using Git and GitHub",
    "section": "",
    "text": "Important\n\n\n\nNever push to the main branch! Code must always be reviewed first.\n\n\n\n\n\n\nIf you haven‚Äôt already, file an issue that describes what you are doing - whether it be fixing a bug, adding a feature, or something else. Issues help to keep track of both the work to be done, and the work that has been done.\nYou should ensure that the issue is as detailed as possible as it may need to be picked up by someone else. Even if only you intend to work on the issue, over time it can be easy to forget things if there isn‚Äôt enough information. They can also help you to remember what changes you have made to your code, and why you had to make these changes, which can be useful if you need to provide change logs when you release your code. You could even attach screenshots or other images that help explain the problem and anything you‚Äôve tried in the past that hasn‚Äôt worked out. If you can, try to provide a reproducible example (‚Äòreprex‚Äô), perhaps by using the {reprex} R package.\n\n\nIssues are a good record to explain what a PR is for. It‚Äôs also a useful area for input from other members of the team.\n\n\n\nTo contribute to someone else‚Äôs code, you will first need to clone a copy of their repository onto your computer. You can do this by first obtaining the URL of the repository of interest, by clicking the green ‚ÄòCode‚Äô button on the top right of the repository home page.\n\nUsing the command line, Git Bash, the RStudio terminal or another shell script program, navigate to the location you‚Äôd like the files stored in, then run:\ngit clone https://github.com/&lt;user or org name&gt;/&lt;repository name&gt;.git\nYou can then use file explorer (if on Windows) to navigate to the R project folder within the files you‚Äôve cloned, and open it to begin working with the code.\n\n\n\nOnce the issue is created and you have decided that you are going to work on it, first assign yourself to the issue in GitHub. This is an indication to others and helps to prevent multiple people from independently working on the same issue.\nOnce you have assigned yourself, you must make a new branch to work on that feature. GitHub offers a button to do this automatically on the issue page, on the right-hand side.\n\n\n\nScreenshot links to specific GitHub help page\n\n\n\n\nIf you used the create branch button on GitHub it will automatically close the issue when merged.\nLocally you can then work on the branch, pushing your code regularly to GitHub so it can be run and inspected when you are not around.\n\n\nAll commits should be atomic, in short,\n\nEach commit does one, and only one simple thing, that can be summed up in a simple sentence.\n\nAll of your commits should be in branches, the only changes that are made to the main branch would be via pull requests (PR) that have been reviewed by at minimum one colleague.\nWhen you think that your changes are ready to be merged, it‚Äôs time to create a PR and request a code review.\n\n\n\n\n\nWhen you create a PR, you must do two things:\n\nImmediately make someone an assignee - this is the person who will merge the PR. Typically, this should be the person creating the PR (you)\nSelect a person (or people) to review the PR (if the repository is open and has a CODEOWNERS file, then the users named in that file will be automatically assigned as reviewers)\n\nIf your code is not yet ready to be merged then you should use a draft PR.\n\n\nNote that draft PRs are only available on public GitHub repos.\nWait until the reviewer(s) has completed the review and marked it as ready to merge. At this point, the person who is assigned to the PR can complete the merge.\n\n\nMost merges will be the default Create a merge commit but sometimes you may wish to Squash and merge. As the person requesting the PR, you can select whichever option is wanted from the drop-down in GitHub as part of the PR.\n\n\nBy using this approach of the assignee completing the merge, it ensures that code quality is maintained and prevents code from being merged when it is not yet ready. For example, you may have started a PR thinking your work is complete, and a reviewer checks the code and agrees to merge, however, you may realise that there are still things to work on, or there are issues that need to be addressed.\nThe person who is assigned to the PR should be the only person making commits to the branch and this will prevent merge conflicts. If you wish for someone else to collaborate on the branch, then you should assign the PR to that person. At that point, they can pull your branch down and work on it, but you must stop using that branch locally.\n\n\n\n\n\n\nImportant\n\n\n\nOnly one person should ever work on changes to a branch at any time, and it is important to communicate with colleagues so they know to pull the latest changes in.\n\n\n\n\nIf the PR is later assigned back to you then you must immediately pull changes.\nThere may be times when you cannot be the assignee on a PR and in those situations you should nominate someone else to be the assignee and in charge of the PR, the same rules as in the paragraph above would then apply.\nIf, as a reviewer, you find that no one is assigned to the branch, you should get in contact with the person who created the branch and decide who is going to be the assigned owner of the PR.\n\n\nIn circumstances where the person who created the PR is an outside collaborator and doesn‚Äôt have permission to merge, then the reviewer should also be the assignee. In these circumstances, the collaborator will be working from their local fork and will be the only person who can push to the branch. The reviewer, once happy to approve the changes, can merge the PR.\n\n\n\n\n\nWe use semantic versioning.\n\n\n\nEach of the team‚Äôs repositories should have two assigned roles: owner and deputy.\nThe owner is likely to be the person who created the repository and/or has good knowledge of its content. The responsibility of the owner is to:\n\nhave overall responsibility for quality\ntriage issues, label them (especially ‚Äòbug‚Äô and ‚Äòmust‚Äô) and put into milestones\ntag and release the code and, if relevant, deploy it\nbring important issues to sprint by adding to backlog\ndefine how PRs are reviewed (e.g.¬†issue/PR templates, CI Actions) and that they are reviewed\nensure adequate testing is in place\nadd and maintain the CODEOWNERS file\n\nThe deputy should perform these duties in the absence of the owner and provide updates to the owner on their return.\n\n\nThe Data Science team at The Strategy Unit maintain a template repository that contains the minimum set of files and basic structure needed to ensure organisational consistency in repository management. You are welcome to view, clone, copy or reuse the template as you see fit.\n\n\n\n\nSee also our GitHub as a Team Sport presentation.",
    "crumbs": [
      "Using Git and GitHub"
    ]
  },
  {
    "objectID": "style/git_and_github.html#workflow-with-git-and-github",
    "href": "style/git_and_github.html#workflow-with-git-and-github",
    "title": "Using Git and GitHub",
    "section": "",
    "text": "If you haven‚Äôt already, file an issue that describes what you are doing - whether it be fixing a bug, adding a feature, or something else. Issues help to keep track of both the work to be done, and the work that has been done.\nYou should ensure that the issue is as detailed as possible as it may need to be picked up by someone else. Even if only you intend to work on the issue, over time it can be easy to forget things if there isn‚Äôt enough information. They can also help you to remember what changes you have made to your code, and why you had to make these changes, which can be useful if you need to provide change logs when you release your code. You could even attach screenshots or other images that help explain the problem and anything you‚Äôve tried in the past that hasn‚Äôt worked out. If you can, try to provide a reproducible example (‚Äòreprex‚Äô), perhaps by using the {reprex} R package.\n\n\nIssues are a good record to explain what a PR is for. It‚Äôs also a useful area for input from other members of the team.\n\n\n\nTo contribute to someone else‚Äôs code, you will first need to clone a copy of their repository onto your computer. You can do this by first obtaining the URL of the repository of interest, by clicking the green ‚ÄòCode‚Äô button on the top right of the repository home page.\n\nUsing the command line, Git Bash, the RStudio terminal or another shell script program, navigate to the location you‚Äôd like the files stored in, then run:\ngit clone https://github.com/&lt;user or org name&gt;/&lt;repository name&gt;.git\nYou can then use file explorer (if on Windows) to navigate to the R project folder within the files you‚Äôve cloned, and open it to begin working with the code.\n\n\n\nOnce the issue is created and you have decided that you are going to work on it, first assign yourself to the issue in GitHub. This is an indication to others and helps to prevent multiple people from independently working on the same issue.\nOnce you have assigned yourself, you must make a new branch to work on that feature. GitHub offers a button to do this automatically on the issue page, on the right-hand side.\n\n\n\nScreenshot links to specific GitHub help page\n\n\n\n\nIf you used the create branch button on GitHub it will automatically close the issue when merged.\nLocally you can then work on the branch, pushing your code regularly to GitHub so it can be run and inspected when you are not around.\n\n\nAll commits should be atomic, in short,\n\nEach commit does one, and only one simple thing, that can be summed up in a simple sentence.\n\nAll of your commits should be in branches, the only changes that are made to the main branch would be via pull requests (PR) that have been reviewed by at minimum one colleague.\nWhen you think that your changes are ready to be merged, it‚Äôs time to create a PR and request a code review.",
    "crumbs": [
      "Using Git and GitHub"
    ]
  },
  {
    "objectID": "style/git_and_github.html#creating-and-reviewing-prs",
    "href": "style/git_and_github.html#creating-and-reviewing-prs",
    "title": "Using Git and GitHub",
    "section": "",
    "text": "When you create a PR, you must do two things:\n\nImmediately make someone an assignee - this is the person who will merge the PR. Typically, this should be the person creating the PR (you)\nSelect a person (or people) to review the PR (if the repository is open and has a CODEOWNERS file, then the users named in that file will be automatically assigned as reviewers)\n\nIf your code is not yet ready to be merged then you should use a draft PR.\n\n\nNote that draft PRs are only available on public GitHub repos.\nWait until the reviewer(s) has completed the review and marked it as ready to merge. At this point, the person who is assigned to the PR can complete the merge.\n\n\nMost merges will be the default Create a merge commit but sometimes you may wish to Squash and merge. As the person requesting the PR, you can select whichever option is wanted from the drop-down in GitHub as part of the PR.\n\n\nBy using this approach of the assignee completing the merge, it ensures that code quality is maintained and prevents code from being merged when it is not yet ready. For example, you may have started a PR thinking your work is complete, and a reviewer checks the code and agrees to merge, however, you may realise that there are still things to work on, or there are issues that need to be addressed.\nThe person who is assigned to the PR should be the only person making commits to the branch and this will prevent merge conflicts. If you wish for someone else to collaborate on the branch, then you should assign the PR to that person. At that point, they can pull your branch down and work on it, but you must stop using that branch locally.\n\n\n\n\n\n\nImportant\n\n\n\nOnly one person should ever work on changes to a branch at any time, and it is important to communicate with colleagues so they know to pull the latest changes in.\n\n\n\n\nIf the PR is later assigned back to you then you must immediately pull changes.\nThere may be times when you cannot be the assignee on a PR and in those situations you should nominate someone else to be the assignee and in charge of the PR, the same rules as in the paragraph above would then apply.\nIf, as a reviewer, you find that no one is assigned to the branch, you should get in contact with the person who created the branch and decide who is going to be the assigned owner of the PR.\n\n\nIn circumstances where the person who created the PR is an outside collaborator and doesn‚Äôt have permission to merge, then the reviewer should also be the assignee. In these circumstances, the collaborator will be working from their local fork and will be the only person who can push to the branch. The reviewer, once happy to approve the changes, can merge the PR.",
    "crumbs": [
      "Using Git and GitHub"
    ]
  },
  {
    "objectID": "style/git_and_github.html#preparing-a-package-release-of-your-code",
    "href": "style/git_and_github.html#preparing-a-package-release-of-your-code",
    "title": "Using Git and GitHub",
    "section": "",
    "text": "We use semantic versioning.",
    "crumbs": [
      "Using Git and GitHub"
    ]
  },
  {
    "objectID": "style/git_and_github.html#repository-organisation",
    "href": "style/git_and_github.html#repository-organisation",
    "title": "Using Git and GitHub",
    "section": "",
    "text": "Each of the team‚Äôs repositories should have two assigned roles: owner and deputy.\nThe owner is likely to be the person who created the repository and/or has good knowledge of its content. The responsibility of the owner is to:\n\nhave overall responsibility for quality\ntriage issues, label them (especially ‚Äòbug‚Äô and ‚Äòmust‚Äô) and put into milestones\ntag and release the code and, if relevant, deploy it\nbring important issues to sprint by adding to backlog\ndefine how PRs are reviewed (e.g.¬†issue/PR templates, CI Actions) and that they are reviewed\nensure adequate testing is in place\nadd and maintain the CODEOWNERS file\n\nThe deputy should perform these duties in the absence of the owner and provide updates to the owner on their return.\n\n\nThe Data Science team at The Strategy Unit maintain a template repository that contains the minimum set of files and basic structure needed to ensure organisational consistency in repository management. You are welcome to view, clone, copy or reuse the template as you see fit.",
    "crumbs": [
      "Using Git and GitHub"
    ]
  },
  {
    "objectID": "style/git_and_github.html#further-reading",
    "href": "style/git_and_github.html#further-reading",
    "title": "Using Git and GitHub",
    "section": "",
    "text": "See also our GitHub as a Team Sport presentation.",
    "crumbs": [
      "Using Git and GitHub"
    ]
  },
  {
    "objectID": "style/project_structure.html",
    "href": "style/project_structure.html",
    "title": "Project Structure",
    "section": "",
    "text": "Analytical projects should be self-contained and portable. This means that all the materials required for an analysis should be organised into a single folder that can be shared in its entirety and be re-run by other people, ideally via GitHub.\nWe recommend RStudio Projects as a system for creating standardised project structures that meet these goals. The {usethis} package contains a number of helper functions to help get you started, including usethis::create_project().\n\n\nOne of the most common issues you‚Äôll face when using a project someone else has created, or you created previously, is maintaining the required packages to run the project. Knowing what packages are needed to run a particular project isn‚Äôt always obvious, and over time packages can change, rendering code that once worked unusable.\nThe {renv} R package helps solve this problem by:\n\nKeeping track of the packages that are required for a particular project.\nLogging the installed version of all of the packages.\nMaintaining a per-project library of packages, so projects don‚Äôt interfere with one another.\n\n\n\n\nIt‚Äôs helpful to split discrete analytical tasks into separate script files, which can make it easier to handle the codebase in context and provide an obvious order of operations. For example, 01_read.R, 02_wrangle.R, 03_model.R, etc.\nYou could still forget to re-run one of the numbered files, however, or it may take a long time to re-run all the steps again if you only make one small change to the code. This is where a workflow manager is useful.\nWe recommend the {targets} R package as a workflow manager. You write a series of steps and {targets} automatically recognises all the relationships between functions and objects as a graph. This means {targets} knows the order that things should be run and knows which bits of code need to be re-run if there are upstream changes. It‚Äôs a well-documented and supported package.\n\n\n\nIt‚Äôs beneficial to convert code into discrete functions where possible. This makes it easier to:\n\nreduce the chance of errors, because you‚Äôll avoid repetitive and mistake-prone copy-pasting of code\nunderstand your scripts, because code can be condensed into a simpler calls that are easier to read\nreuse your code, because functions allow you to consistently call the same code more than once and can be copied into other projects\ndebug, because the source of an error can be more easily traced and your code can be tested more easily\n\nConsider the DRY (Don‚Äôt Repeat Yourself) principle when deciding whether or not to convert some code into a function. It may be better to write a function if you‚Äôve used the same piece of code more than once in an analysis, especially if it contains many lines.\nFunction names should be short but descriptive and should contain a verb that describes what the function does. For example, get_geospatial_data() may be better than the generic get_data(), which is certainly better than the uninformative data().\nIn a project, it‚Äôs conventional to put your functions in a folder called R in the project‚Äôs root directory. You can group functions into separate R scripts with meaningful names to make it easier to organise them (read-data.R, model.R, etc). You can then source() these function scripts into your analytical scripts as required.\n\n\n\n\nIt may be beneficial to gather your functions into a discrete package so that you and others can install and reuse them for other projects.\nThe {usethis} package has a number of shortcuts to help you set up a package. You can begin with usethis::create_package() to generate the basic structure and then usethis::use_r and usethis::use_test() to add scripts and {testthat} tests into the correct folder structure.\nWe recommend you include a number of extra files in your package to make its purpose clear and to encourage collaboration. This includes:\n\na README file to describe the purpose of your package and provide some simple examples, which you can set up with usethis::use_readme_md() or usethis::use_readme_rmd() if it contains R code that you want to execute\na NEWS file with usethis::use_news_md(), which is used to communicate the latest changes to your package\na CODE_OF_CONDUCT file with usethis::use_code_of_conduct to explain to collaborators how they should engage with your project\nvignettes with usethis::use_vignette(), which are short documents that let you mix code with prose to describe how to use the functions in your package\n\nWe recommend semantic versioning as you develop your package. In this system, the version number is composed of three digits (like ‚Äò1.2.3‚Äô) that are each incremented as you make major breaking changes, minor changes and patches or bug fixes. The usethis::use_version() function can help you to do this and to automatically update the DESCRIPTION and NEWS file.\nUse {pkgdown} to autogenerate a website from your package‚Äôs documentation. This lets people see your documentation rendered nicely on the internet, without the need to install the package. You can serve this site on the web and update it automatically using GitHub Pages and GitHub Actions.",
    "crumbs": [
      "Project Structure"
    ]
  },
  {
    "objectID": "style/project_structure.html#rstudio-projects",
    "href": "style/project_structure.html#rstudio-projects",
    "title": "Project Structure",
    "section": "",
    "text": "Analytical projects should be self-contained and portable. This means that all the materials required for an analysis should be organised into a single folder that can be shared in its entirety and be re-run by other people, ideally via GitHub.\nWe recommend RStudio Projects as a system for creating standardised project structures that meet these goals. The {usethis} package contains a number of helper functions to help get you started, including usethis::create_project().\n\n\nOne of the most common issues you‚Äôll face when using a project someone else has created, or you created previously, is maintaining the required packages to run the project. Knowing what packages are needed to run a particular project isn‚Äôt always obvious, and over time packages can change, rendering code that once worked unusable.\nThe {renv} R package helps solve this problem by:\n\nKeeping track of the packages that are required for a particular project.\nLogging the installed version of all of the packages.\nMaintaining a per-project library of packages, so projects don‚Äôt interfere with one another.\n\n\n\n\nIt‚Äôs helpful to split discrete analytical tasks into separate script files, which can make it easier to handle the codebase in context and provide an obvious order of operations. For example, 01_read.R, 02_wrangle.R, 03_model.R, etc.\nYou could still forget to re-run one of the numbered files, however, or it may take a long time to re-run all the steps again if you only make one small change to the code. This is where a workflow manager is useful.\nWe recommend the {targets} R package as a workflow manager. You write a series of steps and {targets} automatically recognises all the relationships between functions and objects as a graph. This means {targets} knows the order that things should be run and knows which bits of code need to be re-run if there are upstream changes. It‚Äôs a well-documented and supported package.\n\n\n\nIt‚Äôs beneficial to convert code into discrete functions where possible. This makes it easier to:\n\nreduce the chance of errors, because you‚Äôll avoid repetitive and mistake-prone copy-pasting of code\nunderstand your scripts, because code can be condensed into a simpler calls that are easier to read\nreuse your code, because functions allow you to consistently call the same code more than once and can be copied into other projects\ndebug, because the source of an error can be more easily traced and your code can be tested more easily\n\nConsider the DRY (Don‚Äôt Repeat Yourself) principle when deciding whether or not to convert some code into a function. It may be better to write a function if you‚Äôve used the same piece of code more than once in an analysis, especially if it contains many lines.\nFunction names should be short but descriptive and should contain a verb that describes what the function does. For example, get_geospatial_data() may be better than the generic get_data(), which is certainly better than the uninformative data().\nIn a project, it‚Äôs conventional to put your functions in a folder called R in the project‚Äôs root directory. You can group functions into separate R scripts with meaningful names to make it easier to organise them (read-data.R, model.R, etc). You can then source() these function scripts into your analytical scripts as required.",
    "crumbs": [
      "Project Structure"
    ]
  },
  {
    "objectID": "style/project_structure.html#packages",
    "href": "style/project_structure.html#packages",
    "title": "Project Structure",
    "section": "",
    "text": "It may be beneficial to gather your functions into a discrete package so that you and others can install and reuse them for other projects.\nThe {usethis} package has a number of shortcuts to help you set up a package. You can begin with usethis::create_package() to generate the basic structure and then usethis::use_r and usethis::use_test() to add scripts and {testthat} tests into the correct folder structure.\nWe recommend you include a number of extra files in your package to make its purpose clear and to encourage collaboration. This includes:\n\na README file to describe the purpose of your package and provide some simple examples, which you can set up with usethis::use_readme_md() or usethis::use_readme_rmd() if it contains R code that you want to execute\na NEWS file with usethis::use_news_md(), which is used to communicate the latest changes to your package\na CODE_OF_CONDUCT file with usethis::use_code_of_conduct to explain to collaborators how they should engage with your project\nvignettes with usethis::use_vignette(), which are short documents that let you mix code with prose to describe how to use the functions in your package\n\nWe recommend semantic versioning as you develop your package. In this system, the version number is composed of three digits (like ‚Äò1.2.3‚Äô) that are each incremented as you make major breaking changes, minor changes and patches or bug fixes. The usethis::use_version() function can help you to do this and to automatically update the DESCRIPTION and NEWS file.\nUse {pkgdown} to autogenerate a website from your package‚Äôs documentation. This lets people see your documentation rendered nicely on the internet, without the need to install the package. You can serve this site on the web and update it automatically using GitHub Pages and GitHub Actions.",
    "crumbs": [
      "Project Structure"
    ]
  },
  {
    "objectID": "style/style_guide.html",
    "href": "style/style_guide.html",
    "title": "Style Guide",
    "section": "",
    "text": "In general, follow the conventions of the tidyverse style guide.\nPrefer packages to be explicitly namespaced with a double colon in production code, like dplyr::mutate(), though this is not essential in exploratory data analysis.\nFavour the base R pipe (|&gt;) over the {magrittr} pipe (%&gt;%).\nAvoid library(tidyverse) in production code because it attaches a lot of packages that might not be used, though you may use it in exploratory data analysis.\nUse {styler} and {lintr} (or Python equivalents such as black) to tidy your code.\nInsert linebreaks in your code at or before column 80.\nWhen using {dplyr}, favour one mutate over many. For example, between the two examples below, example B is preferred:\n\nEXAMPLE A:\nlibrary(dplyr)\n\nstarwars |&gt;\n  mutate(height_cm = height) |&gt;\n  mutate(name_copy = name)\nEXAMPLE B:\nstarwars |&gt;\n  mutate(\n    height_cm = height,\n    name_copy = name\n  )\n\n\n\nTo avoid excessive whitespace cluttering up Git diffs, set your chosen IDE to use LF line endings.\nFor RStudio, select Tools &gt; Global Options &gt; Code &gt; Saving. Under Serialization, set the Line ending conversion to Posix (LF).\n\n\n\nScreenshot showing how to set line endings correctly in RStudio\n\n\nFor VSCode, follow the instructions here.\nYou should also change your git settings to checkout in LF by running this in your terminal: git config --global core.autocrlf false\nPer repository, you can run git add --renormalize . to fix line endings.\n\n\n\n\nFavour Quarto (.qmd files) over R Markdown (.Rmd) for document production.\nUse Git for all projects and GitHub as the remote home for of all of the project code.\nUse the Reproducible Analytical Pipelines (RAP) approach wherever possible.\nLine breaks in Markdown (.md) files should be at 120 characters or at sentence breaks.\nWhen writing about code, use curly braces to identify a {package} name and use backticks around `functions()` as these render nicely and highlight the words clearly.\nIf you‚Äôre not sure about something try the NHS-R Way, the UK Government accessibility guidelines, or the Turing Way. If you‚Äôre still not sure, just ask the team.",
    "crumbs": [
      "Style Guide"
    ]
  },
  {
    "objectID": "style/style_guide.html#code-style",
    "href": "style/style_guide.html#code-style",
    "title": "Style Guide",
    "section": "",
    "text": "In general, follow the conventions of the tidyverse style guide.\nPrefer packages to be explicitly namespaced with a double colon in production code, like dplyr::mutate(), though this is not essential in exploratory data analysis.\nFavour the base R pipe (|&gt;) over the {magrittr} pipe (%&gt;%).\nAvoid library(tidyverse) in production code because it attaches a lot of packages that might not be used, though you may use it in exploratory data analysis.\nUse {styler} and {lintr} (or Python equivalents such as black) to tidy your code.\nInsert linebreaks in your code at or before column 80.\nWhen using {dplyr}, favour one mutate over many. For example, between the two examples below, example B is preferred:\n\nEXAMPLE A:\nlibrary(dplyr)\n\nstarwars |&gt;\n  mutate(height_cm = height) |&gt;\n  mutate(name_copy = name)\nEXAMPLE B:\nstarwars |&gt;\n  mutate(\n    height_cm = height,\n    name_copy = name\n  )",
    "crumbs": [
      "Style Guide"
    ]
  },
  {
    "objectID": "style/style_guide.html#line-endings",
    "href": "style/style_guide.html#line-endings",
    "title": "Style Guide",
    "section": "",
    "text": "To avoid excessive whitespace cluttering up Git diffs, set your chosen IDE to use LF line endings.\nFor RStudio, select Tools &gt; Global Options &gt; Code &gt; Saving. Under Serialization, set the Line ending conversion to Posix (LF).\n\n\n\nScreenshot showing how to set line endings correctly in RStudio\n\n\nFor VSCode, follow the instructions here.\nYou should also change your git settings to checkout in LF by running this in your terminal: git config --global core.autocrlf false\nPer repository, you can run git add --renormalize . to fix line endings.",
    "crumbs": [
      "Style Guide"
    ]
  },
  {
    "objectID": "style/style_guide.html#additional-assorted-notes-on-style",
    "href": "style/style_guide.html#additional-assorted-notes-on-style",
    "title": "Style Guide",
    "section": "",
    "text": "Favour Quarto (.qmd files) over R Markdown (.Rmd) for document production.\nUse Git for all projects and GitHub as the remote home for of all of the project code.\nUse the Reproducible Analytical Pipelines (RAP) approach wherever possible.\nLine breaks in Markdown (.md) files should be at 120 characters or at sentence breaks.\nWhen writing about code, use curly braces to identify a {package} name and use backticks around `functions()` as these render nicely and highlight the words clearly.\nIf you‚Äôre not sure about something try the NHS-R Way, the UK Government accessibility guidelines, or the Turing Way. If you‚Äôre still not sure, just ask the team.",
    "crumbs": [
      "Style Guide"
    ]
  },
  {
    "objectID": "style/data_storage.html",
    "href": "style/data_storage.html",
    "title": "Data Storage",
    "section": "",
    "text": "All projects should be commited to version control, with a repository created in the Strategy Unit‚Äôs GitHub organisation.\nIdeally, any data that is used within the project should be part of a targets pipeline.\nThere are a number of considerations about whether to add the data to version control or not. At a high level:\n\nis the data OK to release publicly?\nis the data in a text-based (non-binary) format, such as .csv, .json (rather than say .xlsx)?\nis the data relatively small in size?\n\n\n\nIf data is grabbed from a website, or via an API, create code to download the file/data. Consider whether this is likely to be a stable way of getting the data (does the data change over time? do you suspect that the location of the resource may disappear? is it quick to retrieve the data?). If so, then it doesn‚Äôt make much sense to commit the data to version control as it can always be quickly regenerated.\n\n\n\nLarge files tend not to work particularly well with version control. Specifically, files larger than 100MB will be blocked by GitHub, and files larger than 50MB will generate a warning. But you may even want to class any file over a few MB as large.\nAlternatives for storing large files:\n\nif the file is something that is generated (and reproducible) from other sources, then do not bother tracking the file\nif the file is something that you want tracking with version control, look at git LFS\nif the file needs to be shared publicly, but LFS is not suitable, the file could be stored in Azure blob storage\nif the file needs to be shared privately, also consider Azure blob storage (using something like SAS tokens)\nif the file needs to only be shared within the Strategy Unit then store in SharePoint (i.e.¬†within a teams channel)\n\nUse of network drives should be deprecated and avoided at all costs due to issues of lack of versioning of files and the performance bottleneck that is created by using a network share. If a network share is truly the only way of storing the data for sharing with colleagues, then look at using ways of syncing the file to local storage to avoid performance bottlenecks, such as robocopy.",
    "crumbs": [
      "Data Storage"
    ]
  },
  {
    "objectID": "style/data_storage.html#data-from-websites",
    "href": "style/data_storage.html#data-from-websites",
    "title": "Data Storage",
    "section": "",
    "text": "If data is grabbed from a website, or via an API, create code to download the file/data. Consider whether this is likely to be a stable way of getting the data (does the data change over time? do you suspect that the location of the resource may disappear? is it quick to retrieve the data?). If so, then it doesn‚Äôt make much sense to commit the data to version control as it can always be quickly regenerated.",
    "crumbs": [
      "Data Storage"
    ]
  },
  {
    "objectID": "style/data_storage.html#filesize",
    "href": "style/data_storage.html#filesize",
    "title": "Data Storage",
    "section": "",
    "text": "Large files tend not to work particularly well with version control. Specifically, files larger than 100MB will be blocked by GitHub, and files larger than 50MB will generate a warning. But you may even want to class any file over a few MB as large.\nAlternatives for storing large files:\n\nif the file is something that is generated (and reproducible) from other sources, then do not bother tracking the file\nif the file is something that you want tracking with version control, look at git LFS\nif the file needs to be shared publicly, but LFS is not suitable, the file could be stored in Azure blob storage\nif the file needs to be shared privately, also consider Azure blob storage (using something like SAS tokens)\nif the file needs to only be shared within the Strategy Unit then store in SharePoint (i.e.¬†within a teams channel)\n\nUse of network drives should be deprecated and avoided at all costs due to issues of lack of versioning of files and the performance bottleneck that is created by using a network share. If a network share is truly the only way of storing the data for sharing with colleagues, then look at using ways of syncing the file to local storage to avoid performance bottlenecks, such as robocopy.",
    "crumbs": [
      "Data Storage"
    ]
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#intro",
    "href": "presentations/2024-11-21_agile_project_management/index.html#intro",
    "title": "Forged in fire",
    "section": "Intro",
    "text": "Intro\n\nI got a new job two years ago\nLearning ++\nI want to tell the story\nI‚Äôve drawn from three wells:\n\nAgile/ Scrum\nMLOps\nThe skills and experience of those around me\n\nThis talk will not properly define scrum or MLOps"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#the-new-hospital-programme",
    "href": "presentations/2024-11-21_agile_project_management/index.html#the-new-hospital-programme",
    "title": "Forged in fire",
    "section": "The New Hospital Programme",
    "text": "The New Hospital Programme\n\nJanuary 2023: Development phase\nApril-ish: Thinking about deployment\nOctober-ish: Model is on its way to full production and much remains to do"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#operational-mode",
    "href": "presentations/2024-11-21_agile_project_management/index.html#operational-mode",
    "title": "Forged in fire",
    "section": "Operational mode",
    "text": "Operational mode\n\nThe team was growing in November\nThere were two priorities\n\nIncrease bus factor for Tom (which I‚Äôll come back to)\nDeploy the first production ready version"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#the-first-deployed-version",
    "href": "presentations/2024-11-21_agile_project_management/index.html#the-first-deployed-version",
    "title": "Forged in fire",
    "section": "The first deployed version",
    "text": "The first deployed version\n\nDecember-ish: Many needs that were not anticipated\nThis first release kicked off lots of other work\nThe second release kicked off lots of other work\nIt was very hard to do any long term planning\nBy March we were confused and tired (and it was all my fault)"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#scrum",
    "href": "presentations/2024-11-21_agile_project_management/index.html#scrum",
    "title": "Forged in fire",
    "section": "Scrum",
    "text": "Scrum\n\nWe are not doing ‚Äúproper‚Äù scrum\nProduct owner, scrum master, everyone else\nFive week sprints with a one week recovery run between each one\nSprint planning, sprint catchup, sprint retro"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#sprint-retro",
    "href": "presentations/2024-11-21_agile_project_management/index.html#sprint-retro",
    "title": "Forged in fire",
    "section": "Sprint retro",
    "text": "Sprint retro\n\nWhat went well, what could have gone better, and what to improve next time\nLooking at process, not blaming individuals\nRequires maturity and trust to bring up issues, and to respond to them in a constructive way\nShould agree at the end on one process improvement which goes in the next sprint\nWe‚Äôve had some really, really good retros and I think it‚Äôs a really important process for a team"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#what-did-scrum-give-the-team",
    "href": "presentations/2024-11-21_agile_project_management/index.html#what-did-scrum-give-the-team",
    "title": "Forged in fire",
    "section": "What did scrum give the team?",
    "text": "What did scrum give the team?\n\nSimultaneous releases of linked repos\nThe team works autonomously in the sprint\nBetter conversations about ‚Äúno‚Äù\nThe planning and retro process improves the team‚Äôs processes, not just the code"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#product-owner",
    "href": "presentations/2024-11-21_agile_project_management/index.html#product-owner",
    "title": "Forged in fire",
    "section": "Product owner",
    "text": "Product owner\n\nMy lessson- get out the way\nA better connection between high level and low level planning\nClear release dates and responsibilities\nClear what I should be doing"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#agility",
    "href": "presentations/2024-11-21_agile_project_management/index.html#agility",
    "title": "Forged in fire",
    "section": "Agility",
    "text": "Agility\n\nThis project was agile whether we liked it or not!\nMy 2022 agile definition:\n\nCustomers can‚Äôt make up their minds\nIt‚Äôs hard to design software all at once\nContinuous delivery keeps customers happy"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#some-highlights-from-the-agile-manifesto",
    "href": "presentations/2024-11-21_agile_project_management/index.html#some-highlights-from-the-agile-manifesto",
    "title": "Forged in fire",
    "section": "Some highlights from The agile manifesto",
    "text": "Some highlights from The agile manifesto\n\n‚ÄúWelcome changing requirements, even late in development‚Äù\n‚ÄúAt regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly‚Äù\n‚ÄúContinuous attention to technical excellence and good design enhances agility‚Äù\n‚ÄúSimplicity- the art of maximizing the amount of work not done- is essential‚Äù"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#how-scrum-helped",
    "href": "presentations/2024-11-21_agile_project_management/index.html#how-scrum-helped",
    "title": "Forged in fire",
    "section": "How scrum helped",
    "text": "How scrum helped\n\nThose shaping the project wanted to be able to make quick changes- and see the long term plan\nAgility is a mindset, a mode of practice\nIf anything we were actually too agile\nBeing agile is all about being able to review and make decisions frequently\nBut it isn‚Äôt about changing what you‚Äôre doing all the time\nGood code and good teams are ready to change direction- whether they change or not"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#mlops",
    "href": "presentations/2024-11-21_agile_project_management/index.html#mlops",
    "title": "Forged in fire",
    "section": "MLOps",
    "text": "MLOps\n\nLike DevOps, but for ML üôÇ\nDeploying, updating, and monitoring models in production\nThis isn‚Äôt a very ML-y project but much of it still applies (not all though)\nWe have a set of customers using a model and we want to push updates to that model monthly\nWe also want to be able to rerun old models (for audit purposes)"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#levels",
    "href": "presentations/2024-11-21_agile_project_management/index.html#levels",
    "title": "Forged in fire",
    "section": "Levels",
    "text": "Levels\n\nLevels of MLOps- 0, 1, 2\nContinuous integration and continuous delivery\nThere are various finicky differences in definition\nI‚Äôll just talk about the bottom, top, and where we are"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#level-0-mlops",
    "href": "presentations/2024-11-21_agile_project_management/index.html#level-0-mlops",
    "title": "Forged in fire",
    "section": "Level 0 MLOps",
    "text": "Level 0 MLOps\n\nYou process your data in a Jupyter notebook\nYou make a model on a Jupyter notebook\nYou give your code to the engineers who attempt to recreate your data and environment"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#level-2-mlops",
    "href": "presentations/2024-11-21_agile_project_management/index.html#level-2-mlops",
    "title": "Forged in fire",
    "section": "Level 2 MLOps",
    "text": "Level 2 MLOps\n\nData is processed in a pipeline\nCode is continously integrated and deployed to production\nDevelopment, staging, and production environments are configured identically"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#where-are-we",
    "href": "presentations/2024-11-21_agile_project_management/index.html#where-are-we",
    "title": "Forged in fire",
    "section": "Where are we?",
    "text": "Where are we?\n\nWe‚Äôre sort of level 1-ish\nData pipelines are definitely RAP! (and recently quicker)\nWe have a dev environment\nMerge to main deploys to dev, GitHub release deploys to prod"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#qa-qa-qa",
    "href": "presentations/2024-11-21_agile_project_management/index.html#qa-qa-qa",
    "title": "Forged in fire",
    "section": "QA, QA, QA",
    "text": "QA, QA, QA\n\nQA is super, super, super important\nQA of data in particular- it‚Äôs also time consuming (trying to RAP!)\nQA is done at regular staging points\nQA is done continuously"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#other-stuff-i-learned-along-the-way",
    "href": "presentations/2024-11-21_agile_project_management/index.html#other-stuff-i-learned-along-the-way",
    "title": "Forged in fire",
    "section": "Other stuff I learned along the way",
    "text": "Other stuff I learned along the way\n\nIt doesn‚Äôt matter who‚Äôs right, it matters how people feel\nTransparency isn‚Äôt just good for robust code and analysis- it builds relationships\nBus factor, bus factor, bus factor\nDocumentation, documentation, documentation"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#the-future",
    "href": "presentations/2024-11-21_agile_project_management/index.html#the-future",
    "title": "Forged in fire",
    "section": "The future",
    "text": "The future\n\nOpen source\nMo‚Äô team members, mo‚Äô conscious work on ‚Äúteaming‚Äù and collaboration"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#what-is-data-science",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#what-is-data-science",
    "title": "Everything you ever wanted to know about data science",
    "section": "What is data science?",
    "text": "What is data science?\n\n‚ÄúA data scientist knows more about computer science than the average statistician, and more about statistics than the average computer scientist‚Äù"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#drew-conways-famous-venn-diagram",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#drew-conways-famous-venn-diagram",
    "title": "Everything you ever wanted to know about data science",
    "section": "Drew Conway‚Äôs famous Venn diagram",
    "text": "Drew Conway‚Äôs famous Venn diagram\n\nSource"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#around-the-web",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#around-the-web",
    "title": "Everything you ever wanted to know about data science",
    "section": "Around the web‚Ä¶",
    "text": "Around the web‚Ä¶\n\n\n\nThe difference between a statitician and a data scientist? About $30,000\n‚Ä¶ an actual definition of data science. Taking a database and making it do something else. (warning: this quote is me! :wink:)\nStatistics done on a Mac"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#what-are-the-skills-of-data-science",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#what-are-the-skills-of-data-science",
    "title": "Everything you ever wanted to know about data science",
    "section": "What are the skills of data science?",
    "text": "What are the skills of data science?\n\nAnalysis\n\nML\nStats\nData viz\n\nSoftware engineering\n\nProgramming\nSQL/ data\nDevOps\nRAP"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#what-are-the-skills-of-data-science-1",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#what-are-the-skills-of-data-science-1",
    "title": "Everything you ever wanted to know about data science",
    "section": "What are the skills of data science?",
    "text": "What are the skills of data science?\n\nDomain knowledge\n\nCommunication\nProblem formulation\nDashboards and reports"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#ml",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#ml",
    "title": "Everything you ever wanted to know about data science",
    "section": "ML",
    "text": "ML\n\nSource"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#inevitable-xkcd",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#inevitable-xkcd",
    "title": "Everything you ever wanted to know about data science",
    "section": "Inevitable XKCD",
    "text": "Inevitable XKCD\n\n\n\nSource\n\n\nGoogle flu trends"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#stats-and-data-viz",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#stats-and-data-viz",
    "title": "Everything you ever wanted to know about data science",
    "section": "Stats and data viz",
    "text": "Stats and data viz\n\nML leans a bit more towards atheoretical prediction\nStats leans a bit more towards inference (but they both do both)\nData scientists may use different visualisations\n\nInteractive web based tools\nDashboard based visualisers e.g.¬†{stminsights}"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#software-engineering",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#software-engineering",
    "title": "Everything you ever wanted to know about data science",
    "section": "Software engineering",
    "text": "Software engineering\n\nProgramming\n\nNo/ low code data science?\n\nSQL/ data\n\nTend to use reproducible automated processes\n\nDevOps\n\nPlan, code, build, test, release, deploy, operate, monitor\n\nRAP\n\nI will come back to this"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#domain-knowledge",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#domain-knowledge",
    "title": "Everything you ever wanted to know about data science",
    "section": "Domain knowledge",
    "text": "Domain knowledge\n\nDo stuff that matters\n\nThe best minds of my generation are thinking about how to make people click ads. That sucks. Jeffrey Hammerbacher\n\nConvince other people that it matters\nThis is the hardest part of data science\nCommunicate, communicate, communicate!\nMany of you are expert at this"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#reproducibility",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#reproducibility",
    "title": "Everything you ever wanted to know about data science",
    "section": "Reproducibility",
    "text": "Reproducibility\n\nReproducibility in science\nThe $6B spreadsheet error\nGeorge Osbourne‚Äôs austerity was based on a spreadsheet error\nFor us, reproducibility also means we can do the same analysis 50 times in one minute\n\nWhich is why I started down the road of data science"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#what-is-rap",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#what-is-rap",
    "title": "Everything you ever wanted to know about data science",
    "section": "What is RAP",
    "text": "What is RAP\n\na process in which code is used to minimise manual, undocumented steps, and a clear, properly documented process is produced in code which can reliably give the same result from the same dataset\nRAP should be:\n\n\nthe core working practice that must be supported by all platforms and teams; make this a core focus of NHS analyst training\n\n\nGoldacre review"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--baseline",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--baseline",
    "title": "Everything you ever wanted to know about data science",
    "section": "Levels of RAP- Baseline",
    "text": "Levels of RAP- Baseline\n\nData produced by code in an open-source language (e.g., Python, R, SQL).\nCode is version controlled (see Git basics and using Git collaboratively guides).\nRepository includes a README.md file (or equivalent) that clearly details steps a user must follow to reproduce the code\nCode has been peer reviewed.\nCode is published in the open and linked to & from accompanying publication (if relevant).\n\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--silver",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--silver",
    "title": "Everything you ever wanted to know about data science",
    "section": "Levels of RAP- Silver",
    "text": "Levels of RAP- Silver\n\nCode is well-documented‚Ä¶\nCode is well-organised following standard directory format\nReusable functions and/or classes are used where appropriate\nPipeline includes a testing framework\nRepository includes dependency information (e.g.¬†requirements.txt, PipFile, environment.yml\nData is handled and output in a Tidy data format\n\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--gold",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--gold",
    "title": "Everything you ever wanted to know about data science",
    "section": "Levels of RAP- Gold",
    "text": "Levels of RAP- Gold\n\nCode is fully packaged\nRepository automatically runs tests etc. via CI/CD or a different integration/deployment tool e.g.¬†GitHub Actions\nProcess runs based on event-based triggers (e.g., new data in database) or on a schedule\nChanges to the RAP are clearly signposted. E.g. a changelog in the package, releases etc. (See gov.uk info on Semantic Versioning)\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#the-data-science-unicorn",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#the-data-science-unicorn",
    "title": "Everything you ever wanted to know about data science",
    "section": "The data science ‚ÄúUnicorn‚Äù",
    "text": "The data science ‚ÄúUnicorn‚Äù\n\nThe maybe-mythical data science ‚ÄúUnicorn‚Äù has mastered:\n\nDomain knowledge\nStats and ML\nSoftware engineering"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-a-team-sport",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-a-team-sport",
    "title": "Everything you ever wanted to know about data science",
    "section": "Data science is a team sport",
    "text": "Data science is a team sport\n\nIn my extended DS team I have:\nStats and DevOps (and rabble rousing) [this one is me :wink:]\nSQL, data, and training\nDevOps and programming\nText mining, Python, and APIs\nBilingual R/ Python, Shiny dashboards"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-an-mmo",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-an-mmo",
    "title": "Everything you ever wanted to know about data science",
    "section": "Data science is an MMO",
    "text": "Data science is an MMO\n\nData scientists need help with:\n\nStakeholder communication and engagement\nQualitative analysis\nTranslating models and prediction into the real world\nEvidence review and problem definition"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-an-mmo-1",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-an-mmo-1",
    "title": "Everything you ever wanted to know about data science",
    "section": "Data science is an MMO",
    "text": "Data science is an MMO\n\nData scientists are an excellent help when you:\n\nNeed a lot of pretty graphs\nNeed the same analysis done 50+ times with different data\nHave too much text and not enough time to analyse it\nWant to carefully document your analysis and make it reproducible\nHave a hideously messy, large dataset that you can‚Äôt hack together yourself"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#the-team",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#the-team",
    "title": "Everything you ever wanted to know about data science",
    "section": "The team",
    "text": "The team\n\nWe will be organising code review and pair coding sessions\nWe will be running coffee and coding sessions\nWe can be relied on to get very excited about thorny data problems, especially if they involve:\n\nDrawing pretty graphs\nNHS-R and other communities and events\nSpending long hours in a bunker writing open source code\nProcessing text\nDocumenting and version controlling analyses"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#note",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#note",
    "title": "Everything you ever wanted to know about data science",
    "section": "Note",
    "text": "Note\nAll copyrighted material is reused under Fair Dealing"
  },
  {
    "objectID": "presentations/index.html",
    "href": "presentations/index.html",
    "title": "Presentations",
    "section": "",
    "text": "Title\nAuthor\nDate\n\n\n\n\nPackage tour of sconn: An R package to connect to databricks\nFran Barton\n2025-03-13\n\n\nDeployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS\nChris Beeley\n2025-02-27\n\n\nWhen Quarto wasn't the right answer: Mail merge is back, baby!\nJac Grout, Matt Dray\n2025-02-27\n\n\nA gentle introduction to databricks: What the heck is databricks?\nChris Beeley\n2025-01-16\n\n\nWhat the heck do I do all day?: Transitioning to management from a technical analytical role\nChris Beeley\n2024-12-04\n\n\nDHSC presentation: NHS-R and data science at the Strategy Unit\nChris Beeley\n2024-12-02\n\n\nText mining: What it can and can't do, and what it could do\nChris Beeley\n2024-11-27\n\n\nText mining: Some practical examples\nYiWen Hon\n2024-11-26\n\n\nForged in fire: Project management lessons from the frontline\nChris Beeley\n2024-11-22\n\n\nGitHub as a team sport: NHS RPySOC 2024\nMatt Dray\n2024-11-22\n\n\nWhat is AI?\nData science team, Strategy Unit\n2024-10-10\n\n\nLarge Language Models (LLMs): Is this AI?\nData Science team, Strategy Unit\n2024-10-10\n\n\nComputer Vision: Is this AI?\nTom Jemmett\n2024-10-10\n\n\nIdentifying patients at risk: Is this AI?\nYiWen Hon\n2024-10-10\n\n\nRAP: A cautionary tale\nNA\n2024-10-09\n\n\nUsing R and Python to model future hospital activity: EARL Conference 2024\nYiWen Hon, Matt Dray, Tom Jemmett\n2024-09-05\n\n\nAgile and scrum working\nChris Beeley\n2024-08-22\n\n\nOpen source licensing: Or: how I learned to stop worrying and love openness\nChris Beeley\n2024-05-30\n\n\nGitHub as a team sport: DfT QA Month\nMatt Dray\n2024-05-23\n\n\nStore Data Safely: Coffee & Coding\nYiWen Hon, Matt Dray\n2024-05-16\n\n\nCoffee and Coding: Making my analytical workflow more reproducible with {targets}\nJacqueline Grout\n2024-01-25\n\n\nConference Check-in App: NHS-R/NHS.pycom 2023\nTom Jemmett\n2023-10-17\n\n\nSystem Dynamics in health and care: fitting square data into round models\nSally Thompson\n2023-10-09\n\n\nRepeating Yourself with Functions: Coffee and Coding\nSally Thompson\n2023-09-07\n\n\nCoffee and Coding: Working with Geospatial Data in R\nTom Jemmett\n2023-08-24\n\n\nUnit testing in R: NHS-R Community Webinar\nTom Jemmett\n2023-08-23\n\n\nEverything you ever wanted to know about data science: but were too afraid to ask\nChris Beeley\n2023-08-02\n\n\nTravels with R and Python: the power of data science in healthcare\nChris Beeley\n2023-08-02\n\n\nAn Introduction to the New Hospital Programme Demand Model: HACA 2023\nTom Jemmett\n2023-07-11\n\n\nWhat good data science looks like\nChris Beeley\n2023-05-23\n\n\nText mining of patient experience data\nChris Beeley\n2023-05-15\n\n\nCoffee and Coding: {targets}\nTom Jemmett\n2023-03-23\n\n\nCollaborative working\nChris Beeley\n2023-03-23\n\n\nCoffee and Coding: Good Coding Practices\nTom Jemmett\n2023-03-09\n\n\nRAP: what is it and how can my team start using it effectively?\nChris Beeley\n2023-03-09\n\n\nCoffee and coding: Intro session\nChris Beeley\n2023-02-23"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#ai-could-help-identify-high-risk-heart-patients",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#ai-could-help-identify-high-risk-heart-patients",
    "title": "Identifying patients at risk",
    "section": "AI could help identify high-risk heart patients1",
    "text": "AI could help identify high-risk heart patients1\nThe University of Leeds has helped train an AI system called Optimise, that looked at health records of more than two million people.\n‚Ä¶\nOf those two million records that were scanned, more than 400,000 people were identified as being high risk for the likes of heart failure, stroke and diabetes.\nhttps://www.bbc.co.uk/news/articles/cj620yl96kzo"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#how-it-works",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#how-it-works",
    "title": "Identifying patients at risk",
    "section": "How it works",
    "text": "How it works\n\nThe input: Health records\nHealth records can be structured or unstructured\n\nStructured: can be stored in a table\nUnstructured: can‚Äôt be stored in a table, different shapes/sizes (e.g.¬†text, audio, images)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#example-of-structured-data-in-health-records",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#example-of-structured-data-in-health-records",
    "title": "Identifying patients at risk",
    "section": "Example of structured data in health records",
    "text": "Example of structured data in health records\n\n\n\nID\nBMI\nAge\nIMD Decile\nSmoker\nBlood Pressure\n\n\n\n\n1\n17\n49\n3\n1\n110/70\n\n\n2\n25\n67\n1\n1\n129/70\n\n\n3\n20\n39\n8\n0\n140/90\n\n\n4\n28\n81\n6\n0\n130/85\n\n\n5\n29\n41\n4\n0\n120/80\n\n\n\nData is consistent within each column in the table."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#example-of-unstructured-data-in-health-records",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#example-of-unstructured-data-in-health-records",
    "title": "Identifying patients at risk",
    "section": "Example of unstructured data in health records",
    "text": "Example of unstructured data in health records\n\n\n\n\n\n\n\nID\nNotes\n\n\n\n\n1\nShortness of breath\n\n\n2\nPatient attended clinic following one week of fever, vomiting, and abdominal pain.\n\n\n\nThe length of each sentence is different - data not consistent."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#a-simple-approach-to-classifying-data-knn",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#a-simple-approach-to-classifying-data-knn",
    "title": "Identifying patients at risk",
    "section": "A simple approach to classifying data: KNN",
    "text": "A simple approach to classifying data: KNN\n\n\nClustering algorithms like K Nearest Neighbours (KNN) are on the more basic end of the scale, requiring very little computational power.\n\n1\n\nAntti Ajanki AnAj, CC BY-SA 3.0 http://creativecommons.org/licenses/by-sa/3.0/, via Wikimedia Commons"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#a-simple-approach-to-classifying-data-decision-tree",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#a-simple-approach-to-classifying-data-decision-tree",
    "title": "Identifying patients at risk",
    "section": "A simple approach to classifying data: Decision Tree",
    "text": "A simple approach to classifying data: Decision Tree\n1\nhttps://www.researchgate.net/publication/26635430_Using_Machine_Learning_Algorithms_in_Cardiovascular_Disease_Risk_Evaluation"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#there-are-many-different-models-out-there",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#there-are-many-different-models-out-there",
    "title": "Identifying patients at risk",
    "section": "There are many different models out there! ü•¥",
    "text": "There are many different models out there! ü•¥\n1\nhttps://scikit-learn.org/1.3/tutorial/machine_learning_map/"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#what-makes-a-model-simple-or-complex",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#what-makes-a-model-simple-or-complex",
    "title": "Identifying patients at risk",
    "section": "What makes a model simple or complex?",
    "text": "What makes a model simple or complex?\n\nThere are dozens of different algorithms out there\nEach algorithm has different strengths and weaknesses\nWhat makes a model simple or complex is the amount of computational power required and how much the model needs to ‚Äúlearn‚Äù - how many parameters there are"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#is-the-input-or-the-computation-complex",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#is-the-input-or-the-computation-complex",
    "title": "Identifying patients at risk",
    "section": "Is the input or the computation complex?",
    "text": "Is the input or the computation complex?\n‚ÄúWe used UK primary care EHR data from 2,081,139 individuals aged ‚â• 30 years‚Ä¶\nWe trained a random forest classifier using age, sex, ethnicity and comorbidities (OPTIMISE).‚Äù1\nNadarajah, Ramesh, et al.¬†‚ÄúMachine learning to identify community-dwelling individuals at higher risk of incident cardio-renal-metabolic diseases and death.‚Äù Future Healthcare Journal 11 (2024): 100109. https://www.sciencedirect.com/science/article/pii/S2514664524002212"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#pros-and-cons-of-simple-a.i.-approaches",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#pros-and-cons-of-simple-a.i.-approaches",
    "title": "Identifying patients at risk",
    "section": "Pros and cons of simple ‚ÄúA.I.‚Äù approaches",
    "text": "Pros and cons of simple ‚ÄúA.I.‚Äù approaches\n\n\nPros:\n\nSimple models are more easily explained\nCan sometimes find new patterns in the data\n\n\nCons:\n\nThe quality of the data determines the quality of the model\nNot able to handle very complex tasks"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#issues-to-look-out-for",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#issues-to-look-out-for",
    "title": "Identifying patients at risk",
    "section": "üö© Issues to look out for üö©",
    "text": "üö© Issues to look out for üö©\n\nHow complex is the input, or the computational approach?\nHow is the model‚Äôs performance measured?\nDoes the model get updated?\nWhere did the data come from?\nHave issues of bias or ethics been considered?\n\n\n\nContinuously learning, or learning from mistakes vs.¬†snapshot in time"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#introduction",
    "href": "presentations/2023-03-23_collaborative-working/index.html#introduction",
    "title": "Collaborative working",
    "section": "Introduction",
    "text": "Introduction\n\nThis is definitely an art and not a science\nI do not claim to have all, or even most of, the answers\nHow you use these tools is way more important than the tools themselves\nThis is a culture and not a technique"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#costs",
    "href": "presentations/2023-03-23_collaborative-working/index.html#costs",
    "title": "Collaborative working",
    "section": "Costs",
    "text": "Costs\n\nDelay and time\nStress and disagreement\nCommittee thinking\nLearning and effort"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#benefits",
    "href": "presentations/2023-03-23_collaborative-working/index.html#benefits",
    "title": "Collaborative working",
    "section": "Benefits",
    "text": "Benefits\n\n‚ÄúFrom each according to their ability‚Äù\nLearning\nReproducibility and reduced truck factor\nFun!"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#github-as-an-organising-principle-behind-work",
    "href": "presentations/2023-03-23_collaborative-working/index.html#github-as-an-organising-principle-behind-work",
    "title": "Collaborative working",
    "section": "GitHub as an organising principle behind work",
    "text": "GitHub as an organising principle behind work\n\nA project is just a set of milestones\nA milestone is just a set of issues\nAn issue is just a set of commits\nA commit is just text added and removed"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#the-repo-owner",
    "href": "presentations/2023-03-23_collaborative-working/index.html#the-repo-owner",
    "title": "Collaborative working",
    "section": "The repo owner",
    "text": "The repo owner\n\nReview milestones\nReview issues\n\nDiscuss the issue on the issue- NOT on email!\n\nReview pull requests and get your pull requests reviewed!"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#asynchronous-communication",
    "href": "presentations/2023-03-23_collaborative-working/index.html#asynchronous-communication",
    "title": "Collaborative working",
    "section": "Asynchronous communication",
    "text": "Asynchronous communication\n\nInvolve others before you pull request\nInvolve others when you pull request\nRead issues!\nComment on issues!\nFile issues- suggestions/ bug reports/ questions\n\nNOT in emails"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#asynchronous-work",
    "href": "presentations/2023-03-23_collaborative-working/index.html#asynchronous-work",
    "title": "Collaborative working",
    "section": "Asynchronous work",
    "text": "Asynchronous work\n\nEvery piece of work has an issues associated with it\nEvery piece of work associated with an issue lives on its own branch\nEvery branch is incorporated to the main repo by a pull request\nEvery pull request is reviewed"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#iteration-and-documentation",
    "href": "presentations/2023-03-23_collaborative-working/index.html#iteration-and-documentation",
    "title": "Collaborative working",
    "section": "Iteration and documentation",
    "text": "Iteration and documentation\n\nAnalyse early, analyse often (using RAPs!)\nWrite down what you did\nWrite down what you did but then changed your mind about\nFavour Quarto/ RMarkdown\n\nClean sessions\nDocumentation and graphics"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#data-and-.gitignore",
    "href": "presentations/2023-03-23_collaborative-working/index.html#data-and-.gitignore",
    "title": "Collaborative working",
    "section": "Data and .gitignore",
    "text": "Data and .gitignore\n\nYour repo needs to be reproducible but also needs to be safe\nThe main branch should be reproducible by anyone at any time\n\nDocument package dependencies (using renv)\nDocument data loads if the data isn‚Äôt in the repo"
  },
  {
    "objectID": "presentations/2024-10-09-rap-cautionary-tales/index.html#many-moons-ago",
    "href": "presentations/2024-10-09-rap-cautionary-tales/index.html#many-moons-ago",
    "title": "RAP",
    "section": "Many moons ago‚Ä¶",
    "text": "Many moons ago‚Ä¶\n\n\nI received an FOI request\nIt was clearly sent to multiple trusts\nIt was the second year this request was sent through\nThey provided the figures we sent the year before\nI hadn‚Äôt saved my work from the year before\nI wrote the query to answer the request\nChecked to see I got broadly similar results to the previous year"
  },
  {
    "objectID": "presentations/2024-10-09-rap-cautionary-tales/index.html#my-figures-were-way-off",
    "href": "presentations/2024-10-09-rap-cautionary-tales/index.html#my-figures-were-way-off",
    "title": "RAP",
    "section": "My figures were way off",
    "text": "My figures were way off"
  },
  {
    "objectID": "presentations/2024-10-09-rap-cautionary-tales/index.html#what-could-i-have-done-differently",
    "href": "presentations/2024-10-09-rap-cautionary-tales/index.html#what-could-i-have-done-differently",
    "title": "RAP",
    "section": "What could I have done differently?",
    "text": "What could I have done differently?\n\n\nSaved my Sql query?\nSaved the Excel file along with the query?\nSaved a full set of instructions alongside?\n\n\n\nIt‚Äôs easy not to do these when it‚Äôs (seemingly) a super simple request.\n\n\nIf you haven‚Äôt got a good structure in your team for where to save these kinds of requests, then does it even help to save them?"
  },
  {
    "objectID": "presentations/2024-10-09-rap-cautionary-tales/index.html#what-would-i-do-differently-now",
    "href": "presentations/2024-10-09-rap-cautionary-tales/index.html#what-would-i-do-differently-now",
    "title": "RAP",
    "section": "What would I do differently now?",
    "text": "What would I do differently now?\n\nIdeally, use something like Quarto (.Qmd) [all one script]\n\nquery the data from the database\ndo any data transformations needed\nsave the results to the required format\n\nsave it in version control with a sensible name (Year/FOI #?)\ndocument what it is doing\nMake this part of our teams strategy for dealing with FOI‚Äôs"
  },
  {
    "objectID": "presentations/2024-10-09-rap-cautionary-tales/index.html#another-cautionary-example",
    "href": "presentations/2024-10-09-rap-cautionary-tales/index.html#another-cautionary-example",
    "title": "RAP",
    "section": "Another cautionary example",
    "text": "Another cautionary example\n\n\nA colleague was the point of contact for one specialty\nThey always produced their reports\nOne month they were off\n‚òéÔ∏è ‚ÄúWe need the report for our meeting, now‚Äù\nWhat report?"
  },
  {
    "objectID": "presentations/2024-10-09-rap-cautionary-tales/index.html#the-report",
    "href": "presentations/2024-10-09-rap-cautionary-tales/index.html#the-report",
    "title": "RAP",
    "section": "The report‚Ä¶",
    "text": "The report‚Ä¶\n\n\nAfter a bit of searching I found a couple of different Excel files\nWhich one was the most recent?\nGuessed based on time stamps\nInside the file were three sheets: Sql, Data, Table\nCould I just run the Sql, dump it into Data, and refresh the Table?\nThere was no notes or documentation"
  },
  {
    "objectID": "presentations/2024-10-09-rap-cautionary-tales/index.html#did-i-provide-the-correct-report-for-the-user",
    "href": "presentations/2024-10-09-rap-cautionary-tales/index.html#did-i-provide-the-correct-report-for-the-user",
    "title": "RAP",
    "section": "Did I provide the correct report for the user?",
    "text": "Did I provide the correct report for the user?\n\nü§∑‚Äç‚ôÇÔ∏è"
  },
  {
    "objectID": "presentations/2024-10-09-rap-cautionary-tales/index.html#rap-is-cool-and-all-but",
    "href": "presentations/2024-10-09-rap-cautionary-tales/index.html#rap-is-cool-and-all-but",
    "title": "RAP",
    "section": "ü•â/ü•à/ü•á RAP is cool and all‚Ä¶ but",
    "text": "ü•â/ü•à/ü•á RAP is cool and all‚Ä¶ but\n\n\nThere are so many easy wins that get you most of the way\nConsistently save files\nStore any query and steps to reproduce alongside results\nUse PowerQuery or equivalent to automate data-&gt;Excel\nUse .Rmd/.Qmd documents to do everything in one file\nUtilise git/version control\nblah blah blah, docker, CI/CD‚Ä¶"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#intro",
    "href": "presentations/2024-11-27_text-mining/index.html#intro",
    "title": "Text mining",
    "section": "Intro",
    "text": "Intro\n\nRange of possible applications\nThere are a range of caveats depending on the use case\nSome of them are more relevant than others\nSome of them are ‚Äúfree‚Äù, and some aren‚Äôt"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#what-is-text-mining",
    "href": "presentations/2024-11-27_text-mining/index.html#what-is-text-mining",
    "title": "Text mining",
    "section": "What is text mining?",
    "text": "What is text mining?\n\nA variety of supervised and unsupervised methods\nThe uses we‚Äôll discuss today have no ‚Äúunderstanding‚Äù of text (no ‚Äúintelligence‚Äù)\nThey all have the potential, therefore, to be very inaccurate (e.g.¬†negation)\nBut they can be a low cost way of gathering insight, properly applied"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#i-just-want-a-bit-of-help-sorting",
    "href": "presentations/2024-11-27_text-mining/index.html#i-just-want-a-bit-of-help-sorting",
    "title": "Text mining",
    "section": "‚ÄúI just want a bit of help sorting‚Äù",
    "text": "‚ÄúI just want a bit of help sorting‚Äù\n\nThis is one of the safer things to do\nThere are a range of methods\nSome will give you some say of the ‚Äútheme‚Äù of the piles, and some don‚Äôt\nGreat for: sifting, looking at relative size of piles, novel suggestions for themes for text\nBad for: accuracy, control over what‚Äôs in the piles"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#i-just-want-to-find-useful-examples",
    "href": "presentations/2024-11-27_text-mining/index.html#i-just-want-to-find-useful-examples",
    "title": "Text mining",
    "section": "‚ÄúI just want to find useful examples‚Äù",
    "text": "‚ÄúI just want to find useful examples‚Äù\n\nAlso quite a safe application, and one we implemented for patient experience\nThe algorithm is just helping you to find things with a particular theme or sentiment\nYou bring the understanding\nMany ways of achieving this, from easy to difficult\n\nUnsupervised and supervised\nSearching for strings vs semantic search"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#generating-summary-statistics",
    "href": "presentations/2024-11-27_text-mining/index.html#generating-summary-statistics",
    "title": "Text mining",
    "section": "Generating summary statistics",
    "text": "Generating summary statistics\n\nThis needs to be done with care\nYou can potentially lose a lot of nuance and meaning\nEven the best model is probably only around 80% accurate\nUseful for monitoring and making rough estimates about the size of things\nNot suitable for anything that needs accuracy (e.g.¬†safeguarding)"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#free",
    "href": "presentations/2024-11-27_text-mining/index.html#free",
    "title": "Text mining",
    "section": "‚ÄúFree‚Äù",
    "text": "‚ÄúFree‚Äù\n\nUnsupervised learning is ‚Äúfree‚Äù- no labelling necessary\nArguably you may as well use it for everything, speculatively\nHowever there are lots of models and parameters to set\nSo ‚Äúfree‚Äù is not really ‚Äúfree‚Äù"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#whats-going-on",
    "href": "presentations/2024-11-27_text-mining/index.html#whats-going-on",
    "title": "Text mining",
    "section": "What‚Äôs going on?",
    "text": "What‚Äôs going on?\n\nText models are only as sensible as their inputs\nWe call the algorithm that turns text to numbers a ‚Äúvectoriser‚Äù"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#bag-of-words-vs-tf-idf",
    "href": "presentations/2024-11-27_text-mining/index.html#bag-of-words-vs-tf-idf",
    "title": "Text mining",
    "section": "Bag of words vs TF-IDF",
    "text": "Bag of words vs TF-IDF\n\nBag of words just counts the number of times each word appears\nCrude but effective\nTF-IDF works similarly but makes rare words more important, which can help with topic modelling and classification"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#word-embeddings",
    "href": "presentations/2024-11-27_text-mining/index.html#word-embeddings",
    "title": "Text mining",
    "section": "Word embeddings",
    "text": "Word embeddings\n\nFrom Sutor et al., reproduced under fair use"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#word-embeddings-cont.",
    "href": "presentations/2024-11-27_text-mining/index.html#word-embeddings-cont.",
    "title": "Text mining",
    "section": "Word embeddings cont.",
    "text": "Word embeddings cont.\n\nThere are some smallish ones (like Glove), and some huge ones (like BERT)\nVectors are the only way to encode meaning and context"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#the-future",
    "href": "presentations/2024-11-27_text-mining/index.html#the-future",
    "title": "Text mining",
    "section": "The future",
    "text": "The future\n\nA number of different things suggest themselves\nUse of topic models to explore and search\nTraining of a supervised model for a particular project"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#the-dream",
    "href": "presentations/2024-11-27_text-mining/index.html#the-dream",
    "title": "Text mining",
    "section": "The dream",
    "text": "The dream\n\nZero shot model with human in the loop learning"
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#why",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#why",
    "title": "Repeating Yourself with Functions",
    "section": "Why?",
    "text": "Why?\n\nForecasting project, need to do the same thing with data for 6 centres.\nCopy-paste runs risk of not doing the same thing each time (and boring/time-consuming/frustrating).\nRepetition ‚Äì&gt; function."
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#what",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#what",
    "title": "Repeating Yourself with Functions",
    "section": "What?",
    "text": "What?\n\n\nDemo with plots, equally applicable to ‚Äòdoing stuff‚Äô with data.\n\n\n# preview data\nhead(new_rtt)\n\n  provider_code count rtt_yrmon rtt_mon\n1           RJE    83  Nov 2015      11\n2           RJE    75  Dec 2015      12\n3           RJE    82  Jan 2016       1\n4           RJE    74  Feb 2016       2\n5           RJE    62  Mar 2016       3\n6           RJE    76  Apr 2016       4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember, this is about writing functions, not creating stunning visualisations!\n\n\n\nRepeat this for each of the 6 centres"
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#how",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#how",
    "title": "Repeating Yourself with Functions",
    "section": "How?",
    "text": "How?\nDo it ‚Äònormally‚Äô for one centre. What are the parameters to change?\n\n\np1 &lt;- new_rtt |&gt; \n  filter(provider_code == \"RJE\") |&gt; \n  ggplot(aes(x = rtt_yrmon, y = count)) +\n  geom_line() +\n  su_theme() +\n    theme(legend.position = \"none\") +\n  labs(title = \"RJE\",\n       subtitle = \"time trend of new referrals\")\n\np2 &lt;- new_rtt |&gt; \n  filter(provider_code == \"RJE\") |&gt; \n  ggplot(aes(x = month(rtt_yrmon), y = count)) +\n  geom_col() +\n  su_theme() +\n    theme(legend.position = \"none\") +\n  labs(\n       subtitle = \"monthly pattern of new referrals\")\n\nplots &lt;- ggarrange(p1, p2, nrow = 2)\n\nplots\n\n\n\n\n\n\n\n\n\n\nThis becomes the argument for the function.\nChoose a name for the argument (!= variable_name)\nIn this example we will use prov in place of \"RJE\"\n\n\n\nPlease remember, this is about writing functions, not creating stunning visualisations!"
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#anatomy-of-a-function",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#anatomy-of-a-function",
    "title": "Repeating Yourself with Functions",
    "section": "Anatomy of a Function",
    "text": "Anatomy of a Function\n\nfn_name &lt;- function(arguments){\n  \n  # do stuff\n  \n}\n\nRun the function with fn_name(parameter as argument)"
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#turning-our-code-into-a-function",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#turning-our-code-into-a-function",
    "title": "Repeating Yourself with Functions",
    "section": "Turning our code into a function",
    "text": "Turning our code into a function\n\n\n\np1 &lt;- new_rtt |&gt; \n  filter(provider_code == \"RJE\") |&gt; \n  ggplot(aes(x = rtt_yrmon, y = count)) +\n  geom_line() +\n  su_theme() +\n    theme(legend.position = \"none\") +\n  labs(title = \"RJE\",\n       subtitle = \"time trend of new referrals\")\n\np2 &lt;- new_rtt |&gt; \n  filter(provider_code == \"RJE\") |&gt; \n  ggplot(aes(x = month(rtt_yrmon), y = count)) +\n  geom_col() +\n  su_theme() +\n    theme(legend.position = \"none\") +\n  labs(\n       subtitle = \"monthly pattern of new referrals\")\n\nplots &lt;- ggarrange(p1, p2, nrow = 2)\n\nplots\n\n\n\nfn_plots &lt;- function(prov){\n  \n    p1 &lt;- new_rtt |&gt; \n      filter(provider_code == prov) |&gt; \n      ggplot(aes(x = rtt_yrmon, y = count)) +\n      geom_line() +\n      su_theme() +\n        theme(legend.position = \"none\") +\n      labs(title = prov,\n           subtitle = \"time trend of new referrals\")\n    \n    p2 &lt;- new_rtt |&gt; \n      filter(provider_code == prov) |&gt; \n      ggplot(aes(x = month(rtt_yrmon), y = count)) +\n      geom_col() +\n      su_theme() +\n        theme(legend.position = \"none\") +\n      labs(\n           subtitle = \"monthly pattern of new referrals\")\n    \n    plots &lt;- ggarrange(p1, p2, nrow = 2)\n    \n    plots\n    \n}"
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#running-our-function",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#running-our-function",
    "title": "Repeating Yourself with Functions",
    "section": "Running our function",
    "text": "Running our function\n\n\n\nfn_plots &lt;- function(prov){\n  \n    p1 &lt;- new_rtt |&gt; \n      filter(provider_code == prov) |&gt; \n      ggplot(aes(x = rtt_yrmon, y = count)) +\n      geom_line() +\n      su_theme() +\n        theme(legend.position = \"none\") +\n      labs(title = prov,\n           subtitle = \"time trend of new referrals\")\n    \n    p2 &lt;- new_rtt |&gt; \n      filter(provider_code == prov) |&gt; \n      ggplot(aes(x = month(rtt_yrmon), y = count)) +\n      geom_col() +\n      su_theme() +\n        theme(legend.position = \"none\") +\n      labs(\n           subtitle = \"monthly pattern of new referrals\")\n    \n    plots &lt;- ggarrange(p1, p2, nrow = 2)\n    \n    plots\n    \n}\n\n\n\nfn_plots(\"RKB\")"
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#what-if-we-want-more-than-one-argument",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#what-if-we-want-more-than-one-argument",
    "title": "Repeating Yourself with Functions",
    "section": "What if we want more than one argument?",
    "text": "What if we want more than one argument?\nEasy! Just add them to the arguments when you define the function.\nIf I wanted to run this function on multiple dataframes I would change the function to:\n\nfn_plots &lt;- function(df, prov){\n  \n    p1 &lt;- df |&gt; \n      filter(provider_code == prov) \n    # and the rest as before\n}\n\nand run it with fn_plots(new_rtt, \"RKB\").\nNote that the order of entering the parameters is important. If I tried to run fn_plots(\"RKB\", new_rtt) it would look for a dataframe called \"RKB\" and a provider called new_rtt."
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#working-through-a-list-of-parameters",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#working-through-a-list-of-parameters",
    "title": "Repeating Yourself with Functions",
    "section": "Working through a list of parameters",
    "text": "Working through a list of parameters\nAvoid manually running fn_plots() for each provider.\nUse purrr::map to iterate over a list\n\n\n# create a vector of all the providers\nprov_labels &lt;- c(\"RJE\", \"RKB\", \"RL4\", \"RRK\", \"RWE\", \"RX1\")\n\nmap(prov_labels, ~ fn_plots(.x))\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]"
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#troubleshooting---does-the-function-work",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#troubleshooting---does-the-function-work",
    "title": "Repeating Yourself with Functions",
    "section": "Troubleshooting - does the function work?",
    "text": "Troubleshooting - does the function work?\nCrawl before you can walk - make sure fn_plot() works for one parameter.\nInsert browser() into the function while testing - steps into the function (don‚Äôt forget to remove it when it works!)\n\n\nThis is a new function that will save each time-trend plot\n\nfn_save_plot &lt;- function(prov){\n  \n    p &lt;- new_rtt |&gt; \n      filter(provider_code == prov) |&gt; \n      ggplot(aes(x = month(rtt_yrmon), y = count)) +\n      geom_col() +\n      su_theme() +\n        theme(legend.position = \"none\") +\n      labs(\n           subtitle = paste0(prov, \" - monthly pattern of new referrals\"))\n    \n    ggsave(paste0(prov, \"_plot.png\"), \n           plot = p)\n  \n}\n\n\n \n\n\n\nCheck out Shannon Pileggi‚Äôs slides for more options"
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#troubleshooting---does-it-walk-the-walk",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#troubleshooting---does-it-walk-the-walk",
    "title": "Repeating Yourself with Functions",
    "section": "Troubleshooting - does it walk the walk?",
    "text": "Troubleshooting - does it walk the walk?\nWhen learning to walk, use safely() or possibly() in your walk function - it will indicate if any parameters have failed, rather than just fall down.\n\n\n\n# wrap fn_plots in safely\nsafe_pl &lt;- safely(.f = fn_save_plot)\n\nmap(prov_labels, ~ safe_pl(.x))\n\n\n# wrap fn_plots in possibly\nposs_pl &lt;- possibly(.f = fn_save_plot)\n\nmap(prov_labels, ~ poss_pl(.x))\n\n\nConsole output of wrapping function in possibly"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#section",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#section",
    "title": "Conference Check-in App",
    "section": "",
    "text": "digital.library.unt.edu/ark:/67531/metadc1039451/m1/1/\n\n\nClark, Junebug. [Registration Desk for the LPC Conference], photograph, 2016-03-17/2016-03-19; (https://digital.library.unt.edu/ark:/67531/metadc1039451/m1/1/: accessed October 16, 2023), University of North Texas Libraries, UNT Digital Library, https://digital.library.unt.edu; crediting UNT Libraries Special Collections."
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#qr-codes-are-great",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#qr-codes-are-great",
    "title": "Conference Check-in App",
    "section": "QR codes are great",
    "text": "QR codes are great"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#and-can-be-easily-generated-in-r",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#and-can-be-easily-generated-in-r",
    "title": "Conference Check-in App",
    "section": "and can be easily generated in R",
    "text": "and can be easily generated in R\ninstall.packages(\"qrcode\")\nlibrary(qrcode)\n\nqr_code(\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\")"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#why-not",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#why-not",
    "title": "Conference Check-in App",
    "section": "Why not?",
    "text": "Why not?\n\n{shiny} would be doing all the processing on the server side\nwe would need to read from a camera client side\nthen stream video to the server for {shiny} to detect and decode the QR codes"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work",
    "title": "Conference Check-in App",
    "section": "How does this work?",
    "text": "How does this work?\n\n\nFront-end\n\n\nuses the React JavaScript framework\n@yidel/react-qr-scanner\nApp scan‚Äôs a QR code, then sends this to our backend\nA window pops up to say who has checked in, or shows an error message"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work-1",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work-1",
    "title": "Conference Check-in App",
    "section": "How does this work?",
    "text": "How does this work?\nBack-end\nUses the {plumber} R package to build the API, with endpoints for\n\ngetting the list of all of the attendees for that day\nuploading a list of attendees in bulk\nadding an attendee individually\ngetting an attendee\nchecking the attendee in"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work-2",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work-2",
    "title": "Conference Check-in App",
    "section": "How does this work?",
    "text": "How does this work?\nMore Back-end Stuff\n\nuses a simple SQLite DB that will be thrown away at the end of the conference\nwe send personalised emails using {blastula} to the attendees with their QR codes\nthe QR codes are just random ids (UUIDs) that identify each attendee\nuses websockets to update all of the clients when a user checks in (to update the list of attendees)"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#learning-different-tools-can-show-you-the-light",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#learning-different-tools-can-show-you-the-light",
    "title": "Conference Check-in App",
    "section": "Learning different tools can show you the light",
    "text": "Learning different tools can show you the light\n\nunsplash.com/photos/tMGMINwFOtI"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#what-is-computer-vision-cv",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#what-is-computer-vision-cv",
    "title": "Computer Vision",
    "section": "What is Computer Vision (CV)",
    "text": "What is Computer Vision (CV)\n\n\nComputer vision is a field of computer science that focuses on enabling computers to identify and understand objects and people in images and videos.\n\n\n\n\nLike other types of AI, computer vision seeks to perform and automate tasks that replicate human capabilities.\n\n\n\n\nIn this case, computer vision seeks to replicate both the way humans see, and the way humans make sense of what they see.\n\n\n\nSource: What is computer vision? Microsoft"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#classification",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#classification",
    "title": "Computer Vision",
    "section": "Classification",
    "text": "Classification\n\nAssign a single label to each image\n\n\n\n\n\n\n\nDog\n\n\nWelsh Spaniel\n\n\nAnimal in water\n\n\n\n\n\n\n\n\nDog\n\n\nSussex Spaniel\n\n\nAnimal on land\n\n\n\nImages from imagenet"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#multi-classification",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#multi-classification",
    "title": "Computer Vision",
    "section": "Multi-Classification",
    "text": "Multi-Classification\n\nAssign one or more labels to each image\n\n\n\n\n\nDog, Welsh Spaniel, Animal in water\n\n\n\n\n\n\nDog, Sussex Spaniel, Animal on land\n\n\n\nImages from imagenet"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#object-detection",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#object-detection",
    "title": "Computer Vision",
    "section": "Object Detection",
    "text": "Object Detection\n\n\nImage sourced from Wikimedia"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#event-detection",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#event-detection",
    "title": "Computer Vision",
    "section": "Event Detection",
    "text": "Event Detection\n\n\nIs camera-only the future of self-driving cars?"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#how-does-cv-work",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#how-does-cv-work",
    "title": "Computer Vision",
    "section": "How does CV work?",
    "text": "How does CV work?\nfor classification tasks\n\n\nget a very large corpus or labelled images\nconvert the images to a form that the computer can work with (tensors)\nfeed into a convolutional neural network\nprofit?"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#large-corpus-of-labelled-images",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#large-corpus-of-labelled-images",
    "title": "Computer Vision",
    "section": "Large corpus of labelled images",
    "text": "Large corpus of labelled images\nImageNet\n\na large visual database\nover 14 million hand-annotated images\nmore than 20,000 categories\neach category has ‚Äúseveral hundred‚Äù images\nstarted in 2006"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#convert-the-images-to-tensors",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#convert-the-images-to-tensors",
    "title": "Computer Vision",
    "section": "Convert the images to tensors",
    "text": "Convert the images to tensors\n\n\nA fancy way of saying:\nturn the images into a 2d table\nof values between 0 and 1"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#convolutional-neural-networks",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#convolutional-neural-networks",
    "title": "Computer Vision",
    "section": "(Convolutional) Neural Networks",
    "text": "(Convolutional) Neural Networks\n\n\n3Blue1Brown, YouTube Channel"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#use-pre-trained-models",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#use-pre-trained-models",
    "title": "Computer Vision",
    "section": "Use pre-trained models",
    "text": "Use pre-trained models\nModels that have been pre-trained on some image datasets which can be downloaded and used\n\n\nResNet (Microsoft Research)\nInception (Google)\nTrending classifiers from Hugging Face\n\n\n\nTransfer learning is the concept of taking a pre-trained model as a basis, then fine-tuning it to classify based on your own images."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#how-can-cv-be-used-in-healthcare",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#how-can-cv-be-used-in-healthcare",
    "title": "Computer Vision",
    "section": "How can CV be used in Healthcare?",
    "text": "How can CV be used in Healthcare?\n\nclassification\nmulti-classification\nobject detection\nevent detection"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#how-can-cv-be-used-in-healthcare-1",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#how-can-cv-be-used-in-healthcare-1",
    "title": "Computer Vision",
    "section": "How can CV be used in Healthcare?",
    "text": "How can CV be used in Healthcare?\n\n\n\ndetecting disease or injury\nmonitoring patients vitals, e.g.¬†respiratory rate\ndetecting bounds of a tumour when planning radiotherapy\nautomating cell counting\ncalculating the grade of a cancer\nmonitor for long-term changes, e.g.¬†AAA\ndevices for patients with vision impairments\ndetecting when patients move (fall prevention)\nmonitoring the hygiene of a ward"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#issues-with-computer-vision",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#issues-with-computer-vision",
    "title": "Computer Vision",
    "section": "Issues with Computer Vision",
    "text": "Issues with Computer Vision\n\n‚Ä¶ one neural network learned to differentiate between dogs and wolves. It didn‚Äôt learn the differences between dogs and wolves, but instead learned that wolves were on snow in their picture and dogs were on grass.\n\n\nDogs, Wolves, Data Science, and Why Machines Must Learn Like Humans Do (2017)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#issues-with-computer-vision-1",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#issues-with-computer-vision-1",
    "title": "Computer Vision",
    "section": "Issues with Computer Vision",
    "text": "Issues with Computer Vision\n\n\nArtificial intelligence could revolutionize medical care. But don‚Äôt trust it to read your x-ray just yet (2019)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#issues-with-computer-vision-2",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#issues-with-computer-vision-2",
    "title": "Computer Vision",
    "section": "Issues with Computer Vision",
    "text": "Issues with Computer Vision\n\n\nalgorithm trained at Mount Sinai Hospital, New York City\nBusy ICU, many elderly patients\n34% of their x-rays came from patients with pneumonia\n93% accuracy\n\n\n\n\ntested at other sites, pneumonia ~1% of x-rays\naccuracy dropped to 73%-80%\n\n\n\nArtificial intelligence could revolutionize medical care. But don‚Äôt trust it to read your x-ray just yet (2019)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#issues-with-computer-vision-3",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#issues-with-computer-vision-3",
    "title": "Computer Vision",
    "section": "Issues with Computer Vision",
    "text": "Issues with Computer Vision\n\nAt Mount Sinai, many of the infected patients were too sick to get out of bed, and so doctors used a portable chest x-ray machine. Portable x-ray images look very different from those created when a patient is standing up. Because of what it learned from Mount Sinai‚Äôs x-rays, the algorithm began to associate a portable x-ray with illness. It also anticipated a high rate of pneumonia.\n\n\nArtificial intelligence could revolutionize medical care. But don‚Äôt trust it to read your x-ray just yet (2019)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#the-unique-problems-of-medical-computer-vision",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#the-unique-problems-of-medical-computer-vision",
    "title": "Computer Vision",
    "section": "The Unique Problems of Medical Computer Vision",
    "text": "The Unique Problems of Medical Computer Vision\n\n\n\n\n\n\nThis is the very unique problem of medical computer vision: we are attempting to solve a small signal on the background of small noise whereas standard computer vision‚Äôs problem is a large signal on the background of large noise.\n\n\n\n\nThe Unique Problems of Medical Computer Vision"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#the-unique-problems-of-medical-computer-vision-1",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#the-unique-problems-of-medical-computer-vision-1",
    "title": "Computer Vision",
    "section": "The Unique Problems of Medical Computer Vision",
    "text": "The Unique Problems of Medical Computer Vision\n\n\n\n\nIs this a dog?\n\n\n\n\n\nEnglish Springer Spaniel in a Field, Wikipedia\n\n\n\n\n\n\n\nSmall-cell carcinoma of the lung, Wikipedia\n\n\n\n\n\n\nAnnotations of cats & dogs is cheaper than reviewing medical scans/slides. The latter adds an additional burden on health systems."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#other-issues-with-cv",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#other-issues-with-cv",
    "title": "Computer Vision",
    "section": "Other issues with CV",
    "text": "Other issues with CV\n\n\nEarly detection vs over diagnosis\nAdversarial attacks can trick CV algorithms to incorrectly classify images\nComputational power (environmental impact)\nGovernance: have we got consent to use images?\nExplainability of Neural Networks"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#patient-experience",
    "href": "presentations/2023-05-15_text-mining/index.html#patient-experience",
    "title": "Text mining of patient experience data",
    "section": "Patient experience",
    "text": "Patient experience\n\nThe NHS collects a lot of patient experience data\nRate the service 1-5 (Very poor‚Ä¶ Excellent) but also give written feedback\n\n‚ÄúParking was difficult‚Äù\n‚ÄúDoctor was rude‚Äù\n‚ÄúYou saved my life‚Äù\n\nMany organisations lack the staffing to read all of the feedback in a systematic way"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#text-mining",
    "href": "presentations/2023-05-15_text-mining/index.html#text-mining",
    "title": "Text mining of patient experience data",
    "section": "Text mining",
    "text": "Text mining\n\nWe have built an algorithm to read it\n\nTheme\n‚ÄúCriticality‚Äù\n\nFits alongside other work happening within NHSE\n\nA framework for understanding patient experience"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#patient-experience-101",
    "href": "presentations/2023-05-15_text-mining/index.html#patient-experience-101",
    "title": "Text mining of patient experience data",
    "section": "Patient experience 101",
    "text": "Patient experience 101\n\nTick box scoring is not useful (or accurate)\nText based data is complex and built on human experience\nWe‚Äôre not making word clouds!\nWe‚Äôre not classifying movie reviews or Reddit posts\nThe tool should enhance, not replace, human understanding\n‚ÄúA recommendation engine for feedback data‚Äù"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#everything-open-all-the-time",
    "href": "presentations/2023-05-15_text-mining/index.html#everything-open-all-the-time",
    "title": "Text mining of patient experience data",
    "section": "Everything open, all the time",
    "text": "Everything open, all the time\n\nThis project was coded in the open and is MIT licensed\nEngage with the organisations as we find them\n\nDo they want code or a docker image?\nDo they want to fetch their own themes from an API?\nDo they want to use our dashboard?"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#phase-1",
    "href": "presentations/2023-05-15_text-mining/index.html#phase-1",
    "title": "Text mining of patient experience data",
    "section": "Phase 1",
    "text": "Phase 1\n\n10 categories and moderate performance on criticality analysis\nscikit-learn\nShiny\nReticulate\nR package of Python code"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#golem-all-the-things",
    "href": "presentations/2023-05-15_text-mining/index.html#golem-all-the-things",
    "title": "Text mining of patient experience data",
    "section": "Golem all the things!",
    "text": "Golem all the things!\n\nOpinionated way of building Shiny\nAllows flexibility in deployed versions using YAML\nAgnostic to deployment\nEmphasises dependency management and testing\nSeparate ‚Äúreactive‚Äù and ‚Äúbusiness‚Äù logic (see the accompanying book)"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#phase-2",
    "href": "presentations/2023-05-15_text-mining/index.html#phase-2",
    "title": "Text mining of patient experience data",
    "section": "Phase 2",
    "text": "Phase 2\n\n30-50 categories and excellent criticality performance\nscikit-learn/ BERT\nMore Shiny\nSeparate the code bases\nFastAPI\nInspired by the Royal College of Paediatrics and Child Health API\nDocumentation, documentation, documentation"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#making-it-useful",
    "href": "presentations/2023-05-15_text-mining/index.html#making-it-useful",
    "title": "Text mining of patient experience data",
    "section": "Making it useful",
    "text": "Making it useful\n\nAccurately rating low frequency categories\nPer category precision and recall\nSpeed versus accuracy\nRepresenting the thematic structure"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#the-future",
    "href": "presentations/2023-05-15_text-mining/index.html#the-future",
    "title": "Text mining of patient experience data",
    "section": "The future",
    "text": "The future\n\nOff the shelf, proprietary data collection systems dominate\nThey often offer bundled analytic products of low quality\nThe DS time can‚Äôt and doesn‚Äôt want to offer a complete data system\nHow can we best contribute to improving patient experience for patients in the NHS?\n\nIf the patient experience data won‚Äôt come to the mountain‚Ä¶"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#open-source-ftw",
    "href": "presentations/2023-05-15_text-mining/index.html#open-source-ftw",
    "title": "Text mining of patient experience data",
    "section": "Open source FTW!",
    "text": "Open source FTW!\n\nOften individuals in the NHS don‚Äôt want private companies to ‚Äúbenefit‚Äù from open code\nBut if they make their products better with open code the patients win\nBest practice as code"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#the-projects",
    "href": "presentations/2023-05-15_text-mining/index.html#the-projects",
    "title": "Text mining of patient experience data",
    "section": "The projects",
    "text": "The projects\n\nhttps://github.com/CDU-data-science-team/pxtextmining\nhttps://github.com/CDU-data-science-team/experiencesdashboard\nhttps://github.com/CDU-data-science-team/PatientExperience-QDC"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#the-team",
    "href": "presentations/2023-05-15_text-mining/index.html#the-team",
    "title": "Text mining of patient experience data",
    "section": "The team",
    "text": "The team\n\nYiWen Hon (Python & Machine learning)\nOluwasegun Apejoye (Shiny)\n\nContact:\n\nchris.beeley1@nhs.net\nhttps://fosstodon.org/@chrisbeeley"
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#tldr",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#tldr",
    "title": "GitHub as a team sport",
    "section": "tl;dr",
    "text": "tl;dr\n\n\n\nGitHub organises code\nGitHub can help organise people\nWe‚Äôre learning as we go\n\n\n\n\n\n\n‚ÄòToo long; didn‚Äôt read‚Äô.\nGitHub isn‚Äôt just a dumping ground for code and version histories.\nThere are features that can help with communication and collaboration.\nWe‚Äôve been learning what works for us as our team continues to grow."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#the-data-science-team",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#the-data-science-team",
    "title": "GitHub as a team sport",
    "section": "The Data Science Team",
    "text": "The Data Science Team\n       \n\nExpanding to 8, all remote\nComplex New Hospital Programme (NHP)\nHow should we work together?\n\n\n\nWe‚Äôre a growing team (soon to be 8).\nWe‚Äôve got different backgrounds and experiences.\nWe do modelling, data pipelines, apps, etc.\nWe work largely on a big, complicated project with lots of stakeholders and tasks.\nWe want to bring other teams in the SU along with us.\nAre there tools or approaches we can use to help us?"
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#the-dream",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#the-dream",
    "title": "GitHub as a team sport",
    "section": "The dream",
    "text": "The dream\n\n\n\nOrder from chaos\nGood communication\n‚ÄòBus factor‚Äô reduction\n\n\n\n\n\n\nWe have a big project with lots of repositories. We have lots of different tasks and goals.\nWe want to improve clarity and reduce the chance of misunderstanding and error.\nWe don‚Äôt want information locked up in one person‚Äôs brain."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#living-the-dream",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#living-the-dream",
    "title": "GitHub as a team sport",
    "section": "Living the dream",
    "text": "Living the dream\n\n\n\nThis works (for now)\nNew folks are joining\nThings can will change\n\n\n\n\n\n\nWe‚Äôve been slowly changing how we work and the tools we use.\nOur standards will make it easier for new starters, but they should also have an influence on how we do things.\nNothing is set in stone. We‚Äôre continually thinking about what works and what doesn‚Äôt."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#github-projects",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#github-projects",
    "title": "GitHub as a team sport",
    "section": "GitHub Projects",
    "text": "GitHub Projects\n\n\n\nWe‚Äôre ‚Äòagile‚Äô\nMany tasks/respositories\nWe want to show progress\n\n\n\n\n\n\nWe work in sprints.\nThere‚Äôs lots to keep track of: the model, a couple of apps, a documentation site, etc.\nWe want to show others how things are progressing.\nGitHub Projects helps us by arranging individual tasks from across lots of different repositories.\nWe can also add custom labelling to help us organise and track."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#section",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#section",
    "title": "GitHub as a team sport",
    "section": "",
    "text": "We can show the tasks in kanban style, or as a list or as a calendar.\nWe can filter down to show only certain labels, statuses or assigned people.\nThis is helps us find, organise and focus during sprint planning and weekly sprint catch-ups."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#division-of-labour",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#division-of-labour",
    "title": "GitHub as a team sport",
    "section": "Division of labour",
    "text": "Division of labour\n\n\n\nThe ‚Äòscrum master‚Äô\nOwners and deputies (CODEOWNERS)\nIssue and pull-request assignees\n\n\n\n\n\n\nAt the level of the sprint, we have a scrum master that oversees the movement of tasks from the backlog and takes us through the GitHub Project in weekly sprint catch-ups.\nWithin each repository we have an owner and deputy on each repository, with the goal of keeping the it shipshape (e.g.¬†good docs, no stale branches, PRs are reviewed).\nAnd we have people assigned to issues and PRs, which signals the tasks that people are working on.\nHaving an identifiable person in charge makes it easier to identify ownership and for others to talk to the right person."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#task-sorting",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#task-sorting",
    "title": "GitHub as a team sport",
    "section": "Task sorting",
    "text": "Task sorting\n\n\n\nMoSCoW method\nRelease-aligned milestones\nEfficient triage\n\n\n\n\n\n\nOrganising repositories at a higher level doesn‚Äôt preclude organisation at the repository level, which is foundational.\nWe typically include the labels Must, Should, Could, Won‚Äôt (MoSCoW) to filter tasks and to help assess importance.\nThe issues associated with the current sprint are added to a milestone with the upcoming version number. This makes it easier to focus, but also release the code with auto-generated notes.\nThese approaches signal intent and help the team to more efficiently decide what to do next."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#pull-requests-prs",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#pull-requests-prs",
    "title": "GitHub as a team sport",
    "section": "Pull requests (PRs)",
    "text": "Pull requests (PRs)\n\n\n\nTalk!\nUse suggestions\nThe assignee merges the PR\n\n\n\n\n\n\nRespect each others‚Äô time. ‚ÄòCloses #10‚Äô isn‚Äôt always enough.\nGitHub comments don‚Äôt replace talking. Discuss if unclear.\nSuggestions are efficient and respect the submitter.\nThe submitter owns the PR. They‚Äôre responsible for closing it."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#github-is-a-team-member",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#github-is-a-team-member",
    "title": "GitHub as a team sport",
    "section": "GitHub is a team member",
    "text": "GitHub is a team member\n\n\nAutomate with Actions\nIssue templates\nRepo templates\n\n\n\nMost of the team is human. GitHub itself has features that can automate away some boring things and help prevent accidents or forgetfulness.\nGitHub Actions for continuous integration. R-CMD check at least for R projects. Start with r-lib examples as a basis.\nWe‚Äôre looking towards things like templates at the issue and repo levels; again to remove drudgery."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#are-we-curling",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#are-we-curling",
    "title": "GitHub as a team sport",
    "section": "Are we curling? ü•å",
    "text": "Are we curling? ü•å\n\n\nWe:\n\nare a small team\nassume specialist roles\nwork in sync\n\n\n\n\n\n\nYou have been wondering: if this is a ‚Äòteam sport‚Äô, what sport is it?\nThis is a terrible metaphor. But think about it."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#the-bottom-line-actually",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#the-bottom-line-actually",
    "title": "GitHub as a team sport",
    "section": "The bottom line, actually",
    "text": "The bottom line, actually\n\n\n\n\n\nCommunicate\nHelp each other\nBe kind\n\n\n\n\nThe features of GitHub should help you do the things you should already be doing.\nI am the guy falling over, the stones are tasks, my team mates are picking me up and dusting me off.\nWhat has your team been doing? What works for you?"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-is-data-science",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-is-data-science",
    "title": "Travels with R and Python",
    "section": "What is data science?",
    "text": "What is data science?\n\n‚ÄúA data scientist knows more about computer science than the average statistician, and more about statistics than the average computer scientist‚Äù\n\n(Josh Wills, a former head of data engineering at Slack)"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#drew-conways-famous-venn-diagram",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#drew-conways-famous-venn-diagram",
    "title": "Travels with R and Python",
    "section": "Drew Conway‚Äôs famous Venn diagram",
    "text": "Drew Conway‚Äôs famous Venn diagram\n\nSource"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-are-the-skills-of-data-science",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-are-the-skills-of-data-science",
    "title": "Travels with R and Python",
    "section": "What are the skills of data science?",
    "text": "What are the skills of data science?\n\nAnalysis\n\nML\nStats\nData viz\n\nSoftware engineering\n\nProgramming\nSQL/ data\nDevOps\nRAP"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-are-the-skills-of-data-science-1",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-are-the-skills-of-data-science-1",
    "title": "Travels with R and Python",
    "section": "What are the skills of data science?",
    "text": "What are the skills of data science?\n\nDomain knowledge\n\nCommunication\nProblem formulation\nDashboards and reports"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#stats-and-data-viz",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#stats-and-data-viz",
    "title": "Travels with R and Python",
    "section": "Stats and data viz",
    "text": "Stats and data viz\n\nML leans a bit more towards atheoretical prediction\nStats leans a bit more towards inference (but they both do both)\nData scientists may use different visualisations\n\nInteractive web based tools\nDashboard based visualisers e.g.¬†{stminsights}"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#software-engineering",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#software-engineering",
    "title": "Travels with R and Python",
    "section": "Software engineering",
    "text": "Software engineering\n\nProgramming\n\nNo/ low code data science?\n\nSQL/ data\n\nTend to use reproducible automated processes\n\nDevOps\n\nPlan, code, build, test, release, deploy, operate, monitor\n\nRAP\n\nI will come back to this"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#domain-knowledge",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#domain-knowledge",
    "title": "Travels with R and Python",
    "section": "Domain knowledge",
    "text": "Domain knowledge\n\nDo stuff that matters\n\nThe best minds of my generation are thinking about how to make people click ads. That sucks. Jeffrey Hammerbacher\n\nConvince other people that it matters\nThis is the hardest part of data science"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#rap",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#rap",
    "title": "Travels with R and Python",
    "section": "RAP",
    "text": "RAP\n\nData science isn‚Äôt RAP\nRAP isn‚Äôt data science\nThey are firm friends"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#reproducibility",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#reproducibility",
    "title": "Travels with R and Python",
    "section": "Reproducibility",
    "text": "Reproducibility\n\nReproducibility in science\nThe $6B spreadsheet error\nGeorge Osbourne‚Äôs austerity was based on a spreadsheet error\nFor us, reproducibility also means we can do the same analysis 50 times in one minute\n\nWhich is why I started down the road of data science"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-is-rap",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-is-rap",
    "title": "Travels with R and Python",
    "section": "What is RAP",
    "text": "What is RAP\n\na process in which code is used to minimise manual, undocumented steps, and a clear, properly documented process is produced in code which can reliably give the same result from the same dataset\nRAP should be:\n\n\nthe core working practice that must be supported by all platforms and teams; make this a core focus of NHS analyst training\n\n\nGoldacre review"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--baseline",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--baseline",
    "title": "Travels with R and Python",
    "section": "Levels of RAP- Baseline",
    "text": "Levels of RAP- Baseline\n\nData produced by code in an open-source language (e.g., Python, R, SQL)\nCode is version controlled\nRepository includes a README.md file that clearly details steps a user must follow to reproduce the code\nCode has been peer reviewed\nCode is published in the open and linked to & from accompanying publication (if relevant)\n\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--silver",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--silver",
    "title": "Travels with R and Python",
    "section": "Levels of RAP- Silver",
    "text": "Levels of RAP- Silver\n\nCode is well-documented‚Ä¶\nCode is well-organised following standard directory format\nReusable functions and/or classes are used where appropriate\nPipeline includes a testing framework\nRepository includes dependency information (e.g.¬†requirements.txt, PipFile, environment.yml)\nData is handled and output in a Tidy data format\n\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--gold",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--gold",
    "title": "Travels with R and Python",
    "section": "Levels of RAP- Gold",
    "text": "Levels of RAP- Gold\n\nCode is fully packaged\nRepository automatically runs tests etc. via CI/CD or a different integration/deployment tool e.g.¬†GitHub Actions\nProcess runs based on event-based triggers (e.g., new data in database) or on a schedule\nChanges to the RAP are clearly signposted. E.g. a changelog in the package, releases etc. (See gov.uk info on Semantic Versioning)\n\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#data-science-in-healthcare",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#data-science-in-healthcare",
    "title": "Travels with R and Python",
    "section": "Data science in healthcare",
    "text": "Data science in healthcare\n\nForecasting\n\nStats versus ML\n\nText mining\n\nR versus Python\n\nDemand modelling\n\nDevOps as a way of life"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#get-involved",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#get-involved",
    "title": "Travels with R and Python",
    "section": "Get involved!",
    "text": "Get involved!\n\nNHS-R community\n\nWebinars, training, conference, Slack\n\nNHS Pycom\n\nditto‚Ä¶\n\nMLCSU GitHub?\nBuild links with the other CSUs"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#contact",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#contact",
    "title": "Travels with R and Python",
    "section": "Contact",
    "text": "Contact\n\n\n\n\n strategy.unit@nhs.net\n The-Strategy-Unit\n\n\n\n\n\n chris.beeley1@nhs.net\n chrisbeeley"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#intro",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#intro",
    "title": "A gentle introduction to databricks",
    "section": "Intro",
    "text": "Intro\n\nI insisted on having this slot in C&C\nI think some people want to know what a thing does; others want to know what it is\nThis is the is part of the session"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-is-spark",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-is-spark",
    "title": "A gentle introduction to databricks",
    "section": "What the heck is Spark?",
    "text": "What the heck is Spark?\n\nDatabricks is everything now, and confusingly so\nLet‚Äôs look at the story of databricks- which starts with Spark\nSpark was an attempt to improve on MapReduce (primarily Hadoop)"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-is-mapreduce",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-is-mapreduce",
    "title": "A gentle introduction to databricks",
    "section": "What the heck is MapReduce",
    "text": "What the heck is MapReduce\n\nMapReduce is a less analytically specific version of Split, apply, combine\nWhat the heck is Split, apply, combine? (last layer of the onion I promise!)\nHadley Wickham wrote about Split, apply, combine in the intro to {plyr}\n\n(For the young people plyr is what we had in the olden days before dplyr- which is Dataframe plyr- dplyr)"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-is-split-apply-combine",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-is-split-apply-combine",
    "title": "A gentle introduction to databricks",
    "section": "What the heck is Split, apply, combine?",
    "text": "What the heck is Split, apply, combine?\n\nVery often in an analysis you want to do the same thing to different groups\nSplit: divide a dataset up by age group\nApply: find the mean number of A&E attendances for 2023/4 (e.g.) for each group\nCombine: bring the results back together and put them in a table"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-was-i-talking-about-again",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-was-i-talking-about-again",
    "title": "A gentle introduction to databricks",
    "section": "What the heck was I talking about again?",
    "text": "What the heck was I talking about again?\n\nMapReduce is essentially an algorithm that relies on massive parallelisation to get jobs done quickly\nSpark was a proposed improvement"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#spark-hadoop",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#spark-hadoop",
    "title": "A gentle introduction to databricks",
    "section": "Spark > Hadoop",
    "text": "Spark &gt; Hadoop\n\nIn-memory processing- this is much faster, especially for certain data science applications\nMore tools and toys- APIs, built in modules for SQL, ML‚Ä¶\nFault tolerance- maintains all the fault tolerance of Hadoop, but works in-memory\nMuch greater flexibility on the way computation is done"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#the-advent-of-databricks",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#the-advent-of-databricks",
    "title": "A gentle introduction to databricks",
    "section": "The advent of databricks",
    "text": "The advent of databricks\n\nSpark was open sourced in 2010 and moved to Apache Foundation in 2013 (Apache Spark)\nDatabricks was set up as the commercial version of Apache Spark (databricks still contributes to the open source version)"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#commercial-spark",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#commercial-spark",
    "title": "A gentle introduction to databricks",
    "section": "Commercial spark",
    "text": "Commercial spark\n\nDatabricks does the enterprise-y stuff you‚Äôd expect (think Posit)\n\nProvides support to enterprises\nCurates, manages, and verifies the code in a commercial version of Spark\nProvide a platform to deploy and manage Spark, which is not simple"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#the-advent-of-delta-lake",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#the-advent-of-delta-lake",
    "title": "A gentle introduction to databricks",
    "section": "The advent of Delta Lake",
    "text": "The advent of Delta Lake\n\nThe other important thing to know about databricks is Delta Lake\nDelta lake is open source and was developed by databricks to improve on existing data lakes"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-is-a-data-lake",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-is-a-data-lake",
    "title": "A gentle introduction to databricks",
    "section": "What the heck is a data lake?",
    "text": "What the heck is a data lake?\n\nOkay, one more\nLike a data warehouse, but less structured\nWidely used in data science and analytics\n\nAs opposed to data warehouses which are more for BI\n\nNot either/ or- often orgs have both"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#what-does-delta-lake-bring",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#what-does-delta-lake-bring",
    "title": "A gentle introduction to databricks",
    "section": "What does Delta lake bring?",
    "text": "What does Delta lake bring?\n\nScalability (particularly around simultaneous processing)\nACID transactions\nWhat the heck is ACID? (Some will know- for those who don‚Äôt)"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-are-acid-transactions",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-are-acid-transactions",
    "title": "A gentle introduction to databricks",
    "section": "What the heck are ACID transactions?",
    "text": "What the heck are ACID transactions?\n\nAtomicity - each statement in a transaction (to read, write, update or delete data) is treated as a single unit\nConsistency - ensures that transactions only make changes to tables in predefined, predictable ways\nIsolation - isolation of user transactions ensures that concurrent transactions don‚Äôt interfere with or affect one another\nDurability - ensures that changes to your data made by successfully executed transactions will be saved, even in the event of system failure\n\n(Source)"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#build-a-little-lakehouse-in-your-soul",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#build-a-little-lakehouse-in-your-soul",
    "title": "A gentle introduction to databricks",
    "section": "Build a little lakehouse in your soul",
    "text": "Build a little lakehouse in your soul\n\nDatabricks enables a ‚Äúlakehouse‚Äù- warehouse and lake together\nLots of whizzy toys are available on databricks\n\nThere are so many now that it‚Äôs just confusing- ‚ÄúGenerative AI‚Äù?\n‚Äúyou can search and discover data by asking a question in your own words‚Äù\n\nEqually you can just write SQL against it"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#why-might-we-as-an-su-want-databricks",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#why-might-we-as-an-su-want-databricks",
    "title": "A gentle introduction to databricks",
    "section": "Why might we as an SU want databricks?",
    "text": "Why might we as an SU want databricks?\n\nWe started using it because it was fast\nWe can use databricks on UDAL\nIt provides a way that we can jointly organise and share data and data architecture in a RAP compliant way"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#text-vectorisation-turning-words-into-numbers",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#text-vectorisation-turning-words-into-numbers",
    "title": "Text mining",
    "section": "Text vectorisation: Turning words into numbers",
    "text": "Text vectorisation: Turning words into numbers\n\nComputers cannot do statistics with words as raw text\nThe basic foundation of all Natural Language Processing!\nHuge range in complexity"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#bag-of-words-each-word-is-represented-by-1-number",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#bag-of-words-each-word-is-represented-by-1-number",
    "title": "Text mining",
    "section": "Bag of words: Each word is represented by 1 number",
    "text": "Bag of words: Each word is represented by 1 number\n\n‚ÄòI love to run‚Äô\n‚Äòthe cat does not eat fruit‚Äô\n‚Äòrun to the cat‚Äô\n‚ÄòI love to eat fruit. fruit fruit fruit fruit‚Äô"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#word-embeddings-50-300-numbers-per-word",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#word-embeddings-50-300-numbers-per-word",
    "title": "Text mining",
    "section": "Word embeddings (50-300 numbers per word)",
    "text": "Word embeddings (50-300 numbers per word)\n1"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#attention-mechanism-768-3-numbers-per-word",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#attention-mechanism-768-3-numbers-per-word",
    "title": "Text mining",
    "section": "Attention mechanism: 768 * 3 numbers per word",
    "text": "Attention mechanism: 768 * 3 numbers per word\n\nBasis of Large Language Models!\nAttempts to capture the relationship between words\nHuge computational power required"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#text-classification",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#text-classification",
    "title": "Text mining",
    "section": "Text classification",
    "text": "Text classification\n\nSupervised learning - we need examples that have already been labelled\nSentiment analysis - whether a review is positive or negative\n\nOver to the code!"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#how-do-we-know-how-good-a-model-is",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#how-do-we-know-how-good-a-model-is",
    "title": "Text mining",
    "section": "How do we know how good a model is?",
    "text": "How do we know how good a model is?\n\n\nWe use performance metrics like:\n\nAccuracy\nPrecision\nRecall\n\n\n2"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#whats-the-accuracy-for-this-model",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#whats-the-accuracy-for-this-model",
    "title": "Text mining",
    "section": "What‚Äôs the accuracy for this model?",
    "text": "What‚Äôs the accuracy for this model?\n\n\n\n\n\nThe model‚Äôs accuracy is 91%!\nBut is there something wrong with this model?"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#different-metrics-for-different-purposes",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#different-metrics-for-different-purposes",
    "title": "Text mining",
    "section": "üêü Different metrics for different purposes3",
    "text": "üêü Different metrics for different purposes3\n\n\nRecall\n\nA model for cancer screening (positive = potential cancer)\nCost of false negative is higher than cost of false positive\nü•Ö Fishing with a net (more fish, some rocks are ok)\n\n\nPrecision\n\nA model for identifying safe seatbelts (positive = safe)\nCost of false positive is higher than cost of false negative\nüé£ Fishing with a spear (fewer fish, but fewer rocks too)"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#topic-modelling",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#topic-modelling",
    "title": "Text mining",
    "section": "Topic Modelling",
    "text": "Topic Modelling\n\nUnsupervised learning - the model has no examples to learn from\n\nOver to the code!"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#topic-modelling-pros-and-cons",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#topic-modelling-pros-and-cons",
    "title": "Text mining",
    "section": "Topic Modelling pros and cons",
    "text": "Topic Modelling pros and cons\n\nHow do you evaluate the performance of a topic model?\nCan work well sometimes\nBlack box"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#conclusion",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#conclusion",
    "title": "Text mining",
    "section": "Conclusion",
    "text": "Conclusion\n\nOur code examples today were very basic\nText mining is not magic\nFancier models = Fancier tasks!"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#footnotes",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#footnotes",
    "title": "Text mining",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOpenclassrooms.com\nEvidently AI\npxtextmining"
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#tldr",
    "href": "presentations/2024-05-23_github-team-sport/index.html#tldr",
    "title": "GitHub as a team sport",
    "section": "tl;dr",
    "text": "tl;dr\n\n\n\n‚ÄòQuality‚Äô isn‚Äôt just good code\nTeamwork makes the dream work\nGitHub is a communication tool\n\n\n\n\n\n\n‚ÄòToo long; didn‚Äôt read‚Äô.\nGitHub isn‚Äôt just a dumping ground for code and Git history.\nIt‚Äôs a platform for working with teammates to get things done.\nQuality is improved by good communication, organisation and reduction of something called the ‚Äòbus factor‚Äô that I‚Äôll get to in a minute."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#the-strategy-unit-su",
    "href": "presentations/2024-05-23_github-team-sport/index.html#the-strategy-unit-su",
    "title": "GitHub as a team sport",
    "section": "The Strategy Unit (SU)",
    "text": "The Strategy Unit (SU)\n\n\n\nAn ‚Äòinternal consultancy‚Äô\nHosted by NHS Midlands and Lancashire\nGrowing in size and reputation\n\n\n\n\n\n\nInitially a ‚Äòstart-up‚Äô style operation that has expanded to 70+ staff.\n‚ÄòWe produce high-quality, multi-disciplinary analytical work ‚Äì and we help people apply the results.‚Äô\nA lot of our work is on the important New Hospital Programme (NHP).\n‚ÄòOur proposition is simple: better evidence, better decisions, better outcomes.‚Äô\nExpansion is tricky; how can we maintain quality?"
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#the-data-science-team",
    "href": "presentations/2024-05-23_github-team-sport/index.html#the-data-science-team",
    "title": "GitHub as a team sport",
    "section": "The Data Science Team",
    "text": "The Data Science Team\n     \n\nExpanded to 6, all remote\nModelling, Quarto, Shiny\nNew Hospital Programme (NHP)\n\n\n\nA new team, expanding rapidly from 2 to 6 in about a year.\nRemote across England.\nExperience from across the NHS and consultancy. I spent a decade in five central government departments before this.\nWe‚Äôre helping to model and design apps for the NHP to help build hospitals.\nSo: growing team, different experiences, important work, but few standardised processes. What to do?"
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#github-at-the-su",
    "href": "presentations/2024-05-23_github-team-sport/index.html#github-at-the-su",
    "title": "GitHub as a team sport",
    "section": "GitHub at the SU",
    "text": "GitHub at the SU\n\n\nWe should be exemplars\nAiming for open by default\nGitHub is on the homepage and there‚Äôs a Data Science site\n\n\n\nIt‚Äôs not just the DS team.\nWe have many other analysts eager to learn and contribute.\nHow can we set good standards and encourage use across the organisation?\nWe‚Äôre running Coffee & Coding sessions, teaching and encouraging talks and blogs on our site.\nWe want to drive up quality by making code open too.\nIt‚Äôs a statement of intent that the SU homepage links to our GitHub organisation."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#what-this-is",
    "href": "presentations/2024-05-23_github-team-sport/index.html#what-this-is",
    "title": "GitHub as a team sport",
    "section": "What this is",
    "text": "What this is\n\nLow-tech, no code\nTips and etiquette, not directives\nWhat‚Äôs been working for us\n\n\n\nBut this is not a technical talk about how to use Git for version control.\nMostly it‚Äôs about planning, workflows, standards and communication.\nIt‚Äôs things that our team have been doing and the ideas are evolving.\nI‚Äôve worked mostly alone on GitHub projects in my career and never worked in a data science team of even this size. So at worst these slides are a way for me to write down what I‚Äôm learning."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#the-bus-factor",
    "href": "presentations/2024-05-23_github-team-sport/index.html#the-bus-factor",
    "title": "GitHub as a team sport",
    "section": "The ‚Äòbus factor‚Äô üöç",
    "text": "The ‚Äòbus factor‚Äô üöç\n\nWe should maintain quality\nWe need redundancy\nStandardised processes can help\n\n\n\nWhy do we care about discussing and ‚Äòformalising‚Äô these ideas?\nWe should encourage standard practices in case someone is ill or away.\nThis also makes it easier when new team members join.\nThis helps us maintain quality."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#rules",
    "href": "presentations/2024-05-23_github-team-sport/index.html#rules",
    "title": "GitHub as a team sport",
    "section": "‚ÄòRules‚Äô",
    "text": "‚ÄòRules‚Äô\n\nIt‚Äôs the spirit that counts\nDo as I say, not as I do\nKnow why you‚Äôre breaking the rules\n\n\n\nTo be clear though, nothing here is etched into stone.\nThere will be times where rules can be broken.\nBut we shouldn‚Äôt be complacent."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#github-flow",
    "href": "presentations/2024-05-23_github-team-sport/index.html#github-flow",
    "title": "GitHub as a team sport",
    "section": "GitHub flow",
    "text": "GitHub flow\n\nCreate a repository\nWrite issues\nPlan\nCreate a branch\nMake a pull request\nReview\nRelease\n\n\n\nThis is a fairly generic GitHub flow.\nI‚Äôll talk through a few things in each of these categories."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#repositories",
    "href": "presentations/2024-05-23_github-team-sport/index.html#repositories",
    "title": "GitHub as a team sport",
    "section": "Repositories",
    "text": "Repositories\n\nAssign ‚Äòowner‚Äô and ‚Äòdeputy‚Äô roles\nAdd README and .gitignore\nStore data elsewhere\n\n\n\nEasy starter: tell people what the purpose of the repo is and how to use it. This is what a README is for. This is an absolute must to lower the bus factor.\nWe should be prevent accidental file upload immediately. Use a .gitignore to exclude likely data files (as well as other unnecessary files). We‚Äôre thinking about common templates/cookiecutters.\nCommunicative files (README, .gitignores) are good, but so is vigilance (code review).\nOwners/deputies are in charge of ‚ÄòGitHub gardening‚Äô (keeping issues in order, labelling, milestones, etc).\nDeputies help with bus factor.\nThe owner can be auto-selected as the reviewer. We‚Äôre experimenting with this for repos with external contributors, especially.\nData is stored elsewhere, on Azure or Posit Connect, due to sensitivity and size. This should be planned before you begin and recorded in the README."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#issues",
    "href": "presentations/2024-05-23_github-team-sport/index.html#issues",
    "title": "GitHub as a team sport",
    "section": "Issues",
    "text": "Issues\n\n\n\nAren‚Äôt just ‚Äòproblems‚Äô\nUse labels, including MoSCoW\nExplain the need, be informative\n\n\n\n\n\n\nIssues can be reminders or questions for further discussion, not just features to build.\nTickets should get two labels. We use a topic like ‚Äòenhancement‚Äô, ‚Äòbug‚Äô, ‚Äòdocumentation‚Äô, ‚Äòtechdebt‚Äô, etc, plus MoSCoW (must, should, could, won‚Äôt) to help prioritisation.\nIssue templates can ensure certain info is provided, which is especially good for external contributors.\nRefer to other related commits by number (e.g.¬†#1), which stops you repeating the same information.\nPrefer to reopen an issue if it doesn‚Äôt actually work.\nIssues can track separate sub-issues.\nYou can add checklists with markdown checkbox: - [ ] (these appear in the issue preview).\nYou can ‚Äòhide‚Äô comments if they‚Äôre out of date, etc."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#plan",
    "href": "presentations/2024-05-23_github-team-sport/index.html#plan",
    "title": "GitHub as a team sport",
    "section": "Plan",
    "text": "Plan\n\n\nTalk, review and reflect\nUse labels to prioritise\nSort into milestones\n\n\n\nWe have a repo and issues, what do we do now? Where to start?\nWe‚Äôve begun working in sprints of about 4 weeks. We have sprint planning meetings to plan things out.\nConsider what needs to be done in the sprint period, what other issues support those goals?\nIs there time for other tasks, like clearing techdebt?\nAll issues should be assigned to a milestone.\nIssues in milestones should be sorted in priority order/order of expected completion (MoSCoW labels will help with this).\nThis helps focus the goals of the sprint and keep us on track."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#branches",
    "href": "presentations/2024-05-23_github-team-sport/index.html#branches",
    "title": "GitHub as a team sport",
    "section": "Branches",
    "text": "Branches\n\n\nOne issue, one branch, one assigned person\nName them sensibly\nBurn them\n\n\n\nOnly one person works on a branch at a time. This person is the one assigned to the relevant issue.\nBranch names should be numbered to match their issue, e.g.¬†‚Äò123-add-filter‚Äô. This makes it obvious what issue is being fixed by that branch and should help identify if more than one person has a branch open for the same issue.\nIf commits from someone else are required, then all parties must communicate about the current state of the branch to ensure they pull changes and avoid merge conflicts.\nBranches are ephemeral and die when the PR is merged. They should be deleted (this can be done automatically).\nThe only branches to exist at all times should be main and a deployment branch, if necessary. All others should be active branches so it‚Äôs clear what‚Äôs being worked on."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#commits",
    "href": "presentations/2024-05-23_github-team-sport/index.html#commits",
    "title": "GitHub as a team sport",
    "section": "Commits",
    "text": "Commits\n\n\nDon‚Äôt commit to main!\n‚ÄòSmall, early and often‚Äô\nMake messages meaningful\n\n\n\nThere‚Äôs not a lot of earth-shattering advice to give here; this stuff is fairly standard.\nDo not commit directly to main. Your work must be independently checked first to limit the chance of mistakes.\nMake your commits small in terms of code and files touched, if possible. This makes the Git history easier to read and makes reviews easier too.\nCommit and push early and often into your branch. This can help others see progress and helps reduce the bus factor.\nDon‚Äôt dump your work into a commit because it‚Äôs the end of the day.\nMake your commit messages meaningful. What does the commit do? Start with a verb in present tense (‚Äòadds‚Äô, not ‚Äòadded‚Äô). Or maybe use ‚Äòconventional‚Äô commits."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#pull-requests-prs",
    "href": "presentations/2024-05-23_github-team-sport/index.html#pull-requests-prs",
    "title": "GitHub as a team sport",
    "section": "Pull requests (PRs)",
    "text": "Pull requests (PRs)\n\n\n\nSmall and closes an issue\nSelect the assignee and reviewer\nThe assignee merges\n\n\n\n\n\n\nPRs should solve the issue they‚Äôre related to. Occasionally one fix may solve another.\nThey should be named to explain what they do. The issue might be ‚Äòthe red button doesn‚Äôt work‚Äô; the PR might be ‚Äòfix the red button‚Äô.\nThey should be small in terms of lines of code and files touched. This will make it easier and faster to understand and assess the changes.\nThe submitter should mark themself as the ‚Äòassignee‚Äô and choose a reviewer. You may want to chat with the reviewer to let them know if they have time.\nFor context, link to the issue(s) being closed with the magic words (‚Äòcloses‚Äô, ‚Äòfixes‚Äô, etc), which will also close those issues as completed.\nInclude a short explanation or bullet-points of what the PR does. Provide any extra information to make the reviewer‚Äôs life easier (areas of focus, maybe) or to ask a question about some aspect of what you‚Äôve written.\nThe PR submitter is the one who clicks the merge button. This is in case the submitter realises there‚Äôs something they need to add or change before the merge."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#reviewing-prs",
    "href": "presentations/2024-05-23_github-team-sport/index.html#reviewing-prs",
    "title": "GitHub as a team sport",
    "section": "Reviewing PRs",
    "text": "Reviewing PRs\n\n\n\nBe helpful, be kind\nUse GitHub suggestions\nDiscuss if unclear\n\n\n\n\n\n\nThe reviewer should typically check that the changes result in the issue being fixed. This may require pulling the branch and then testing it, but may not be necessary for small changes.\nThe reviewer should seek clarification and add comments where something isn‚Äôt clear.\nUse ‚Äòsuggestions‚Äô as a reviewer rather than committing to someone else‚Äôs branch.\nWhen working at pace (when aren‚Äôt we?), we should err towards approval if the issue is completed rather than an endless cycle of asking for small changes. The submitter and reviewer should decide whether smaller things like code style or change in approach should be added as a new issue with a ‚Äòtechdebt‚Äô label."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#releases",
    "href": "presentations/2024-05-23_github-team-sport/index.html#releases",
    "title": "GitHub as a team sport",
    "section": "Releases",
    "text": "Releases\n\nUse semantic versioning (1.2.3)\nAutofill notes with PR names\nDon‚Äôt release on a Friday üôÉ\n\n\n\nTag the history and release on GitHub concurrently to keep them in sync (this is done automatically if the release is done from the GitHub interface).\nSemantic (x.y.z where x is breaking, y is new features and z is patches for bugs).\nWe typically just autofill the release description with the constituent PR titles. Which means it‚Äôs important to give them meaningful names.\nWe align releases with sprints, though patches may occur more frequently.\nWe link releases to deployment in many cases. Don‚Äôt release to prod on a Friday, lol."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#github-is-a-team-member",
    "href": "presentations/2024-05-23_github-team-sport/index.html#github-is-a-team-member",
    "title": "GitHub as a team sport",
    "section": "GitHub is a team member",
    "text": "GitHub is a team member\n\n\nAutomate with Actions\nProvide issue and repo templates\nAn all-in-one planner?\n\n\n\nI lied: we have 6 human team members. GitHub itself has features that can automate away some boring things and help prevent accidents or forgetfulness.\nGitHub Actions for continuous integration. R-CMD check at least for R projects. Start with r-lib examples as a basis.\nWe‚Äôre looking towards things like templates at the issue and repo levels; again to remove drudgery.\nWe use Trello to plan things and have to link to GitHub repos and issues in Trello cards. Can we use GitHub as our planner across multiple repos instead? Seems possible."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#are-we-curling",
    "href": "presentations/2024-05-23_github-team-sport/index.html#are-we-curling",
    "title": "GitHub as a team sport",
    "section": "Are we curling? ü•å",
    "text": "Are we curling? ü•å\n\n\nWe:\n\nare a small team\nassume specialist roles\nwork in sync\n\n\n\n\n\n\nYou have been wondering: if this is a ‚Äòteam sport‚Äô, what sport is it?\nThis is a terrible metaphor. But think about it."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#the-bottom-line-actually",
    "href": "presentations/2024-05-23_github-team-sport/index.html#the-bottom-line-actually",
    "title": "GitHub as a team sport",
    "section": "The bottom line, actually",
    "text": "The bottom line, actually\n\n\n\n\n\nCommunicate\nHelp each other\nBe kind\n\n\n\n\nThe ideas in this talk are things that have helped us, and could help you, to drive up and maintain quality. Some were obvious, some were specific features you might not have known about.\nBut none of these are replacements for being good team members.\nGitHub just provides some affordances to help you.\nI am the guy falling over, the stones are tasks, my team mates are picking me up and dusting me off.\nDid you learn at least one thing? What has your team been doing? What works for you?"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#what-is-targets",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#what-is-targets",
    "title": "Coffee and Coding",
    "section": "What is {targets}?",
    "text": "What is {targets}?\n\nThe targets package is a Make-like pipeline tool for Statistics and data science in R. With targets, you can maintain a reproducible workflow without repeating yourself. targets learns how your pipeline fits together, skips costly runtime for tasks that are already up to date, runs only the necessary computation, supports implicit parallel computing, abstracts files as R objects, and shows tangible evidence that the results match the underlying code and data.\n\n\n\nData analysis can be slow. A round of scientific computation can take several minutes, hours, or even days to complete. After it finishes, if you update your code or data, your hard-earned results may no longer be valid. Unchecked, this invalidation creates chronic Sisyphean loop:\n\n\nLaunch the code.\nWait while it runs.\nDiscover an issue.\nRestart from scratch.\n\n\nsource: The {targets} R package user manual"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#what-is-it-actually-trying-to-do",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#what-is-it-actually-trying-to-do",
    "title": "Coffee and Coding",
    "section": "What is it actually trying to do?",
    "text": "What is it actually trying to do?\n\n\nYour analysis is built up of a number of steps that build one on top of another\nbut these steps need to run in a particular order\nsome of these steps may take a long time to run\nso you only want to run the steps that have changed"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#typical-solution",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#typical-solution",
    "title": "Coffee and Coding",
    "section": "Typical solution",
    "text": "Typical solution\n\n\nSteps\nYou have a folder with numbered scripts, such as:\n\n1. get data.R\n2. process data.R\n3. produce charts.R\n4. build model.R\n5. report.qmd\n\n\nDownsides\n\n\nit‚Äôs easy to accidentally skip a step: what happens if you went from 1 to 3?\nperforming one of the steps may take a long time, so you may want to skip it if it‚Äôs already been run‚Ä¶ but how do you know that it‚Äôs already been run?\nperhaps step 4 doesn‚Äôt depend on step 3, but is this obvious that you could skip step 4 if step 3 is updated?\nwhat if someone labels the files terribly, or doesn‚Äôt number them at all?\nwhat if the numbers become out of date and are in the wrong order?\ndo you need to create a procedure document that describes what to do, step-by-step?"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#targets-to-the-rescue",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#targets-to-the-rescue",
    "title": "Coffee and Coding",
    "section": "{targets} to the rescue?",
    "text": "{targets} to the rescue?\n\n\nUsing the previous example, if we were to create functions for each of the steps (all saved in the folder R/), we can start using targets using the function use_targets() which will create a file called _targets.R.\nWe can then modify the file to match our pipeline, for example:\nNote that:\n\nprocessed_data depends upon raw_data,\nchart and model depend upon processed_data,\nreport depends upon chart and model.\n\nWe can visualise our pipeline using tar_visnetwork().\n\n\nlibrary(targets)\n\ntar_option_set(\n  packages = c(\"tibble\", \"dplyr\", \"ggplot2\"),\n)\n\ntar_source()\n\nlist(\n  tar_target(raw_data, get_data()),\n  tar_target(processed_data, process_data(raw_data)),\n  tar_target(chart, produce_chart(processed_data)),\n  tar_target(model, build_model(processed_data)),\n  tar_target(report, generate_report(chart, model))\n)"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#running-the-pipeline",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#running-the-pipeline",
    "title": "Coffee and Coding",
    "section": "Running the pipeline",
    "text": "Running the pipeline\nRunning this pipeline is as simple as: tar_make().\nThis will output the following:\n‚Ä¢ start target raw_data\n‚Ä¢ built target raw_data [1.05 seconds]\n‚Ä¢ start target processed_data\n‚Ä¢ built target processed_data [0.03 seconds]\n‚Ä¢ start target chart\n‚Ä¢ built target chart [0.02 seconds]\n‚Ä¢ start target model\n‚Ä¢ built target model [0.01 seconds]\n‚Ä¢ start target report\n‚Ä¢ built target report [0 seconds]\n‚Ä¢ end pipeline [1.75 seconds]\n\nRunning tar_make() again will show these step‚Äôs being skipped:\n‚úî skip target raw_data\n‚úî skip target processed_data\n‚úî skip target chart\n‚úî skip target model\n‚úî skip target report\n‚úî skip pipeline [0.12 seconds]"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#changing-one-of-the-files",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#changing-one-of-the-files",
    "title": "Coffee and Coding",
    "section": "Changing one of the files",
    "text": "Changing one of the files\nIf we change produce_chart.R slightly, this will cause chart and report to be invalidated, but it will skip over the other steps.\n\n\n&gt; targets::tar_make()\n\n‚úî skip target raw_data\n‚úî skip target processed_data\n‚Ä¢ start target chart\n‚Ä¢ built target chart [0.03 seconds]\n‚úî skip target model\n‚Ä¢ start target report\n‚Ä¢ built target report [0 seconds]\n‚Ä¢ end pipeline [1.71 seconds]"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#using-the-results-of-our-pipeline",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#using-the-results-of-our-pipeline",
    "title": "Coffee and Coding",
    "section": "Using the results of our pipeline",
    "text": "Using the results of our pipeline\nWe can view the results of any step using tar_read() and tar_load(). These will either directly give you the results of a step, or load that step into your environment (as a variable with the same name as the step).\nThis allows us to view intermediate steps as well as the final outputs of our pipelines.\nOne thing you may want to consider doing is as a final step in a pipeline is to generate a quarto document, or save call a function like saveRDS to generate more useful outputs."
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#current-examples-of-targets-in-action",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#current-examples-of-targets-in-action",
    "title": "Coffee and Coding",
    "section": "Current examples of {targets} in action",
    "text": "Current examples of {targets} in action\n\ncode used in this presentation\nNHP Inputs (all of the data processing steps are a targets pipeline)\nNHP Strategies (runs Sql scripts to update tables in the data warehouse)\nNHP Model (all of the data extraction, processing, and uploading for the model is a targets pipeline)\nMacmillan on NCDR - Jacqueline has been using {targets} for her current project\n\nThe {targets} documentation is exceptionally detailed and easy to follow, and goes into more complex examples (such as dynamic branching of steps in a pipeline and high performance computing setups)"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#a-note-on-richard-stallman",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#a-note-on-richard-stallman",
    "title": "Open source licensing",
    "section": "A note on Richard Stallman",
    "text": "A note on Richard Stallman\n\nRichard Stallman has been heavily criticised for some of this views\nHe is hard to ignore when talking about open source so I am going to talk about him\nNothing in this talk should be read as endorsing any of his comments outside (or inside) the world of open source"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#the-origin-of-open-source",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#the-origin-of-open-source",
    "title": "Open source licensing",
    "section": "The origin of open source",
    "text": "The origin of open source\n\nIn the 50s and 60s source code was routinely shared with hardware and users were often expected to modify to run on their hardware\nBy the late 1960s the production cost of software was rising relative to hardware and proprietary licences became more prevalent\nIn 1980 Richard Stallman‚Äôs department at MIT took delivery of a printer they were not able to modify the source code for\nRichard Stallman launched the GNU project in 1983 to fight for software freedoms\nMIT licence was launched in the late 1980s\nCathedral and the bazaar was released in 1997 (more on which later)"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#what-is-open-source",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#what-is-open-source",
    "title": "Open source licensing",
    "section": "What is open source?",
    "text": "What is open source?\n\nThink free as in free speech, not free beer (Stallman)\n\n\nOpen source does not mean free of charge! Software freedom implies the ability to sell code\nFree of charge does not mean open source! Many free to download pieces of software are not open source (Zoom, for example)\n\n\nBy Chao-Kuei et al.¬†- https://www.gnu.org/philosophy/categories.en.html, GPL, Link"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#the-four-freedoms",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#the-four-freedoms",
    "title": "Open source licensing",
    "section": "The four freedoms",
    "text": "The four freedoms\n\nFreedom 0: The freedom to use the program for any purpose.\nFreedom 1: The freedom to study how the program works, and change it to make it do what you wish.\nFreedom 2: The freedom to redistribute and make copies so you can help your neighbor.\nFreedom 3: The freedom to improve the program, and release your improvements (and modified versions in general) to the public, so that the whole community benefits."
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#cathedral-and-the-bazaar",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#cathedral-and-the-bazaar",
    "title": "Open source licensing",
    "section": "Cathedral and the bazaar",
    "text": "Cathedral and the bazaar\n\nEvery good work of software starts by scratching a developer‚Äôs personal itch.\nGood programmers know what to write. Great ones know what to rewrite (and reuse).\nPlan to throw one [version] away; you will, anyhow (copied from Frederick Brooks‚Äôs The Mythical Man-Month).\nIf you have the right attitude, interesting problems will find you.\nWhen you lose interest in a program, your last duty to it is to hand it off to a competent successor.\nTreating your users as co-developers is your least-hassle route to rapid code improvement and effective debugging.\nRelease early. Release often. And listen to your customers.\nGiven a large enough beta-tester and co-developer base, almost every problem will be characterized quickly and the fix obvious to someone.\nSmart data structures and dumb code works a lot better than the other way around.\nIf you treat your beta-testers as if they‚Äôre your most valuable resource, they will respond by becoming your most valuable resource."
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#cathedral-and-the-bazaar-cont.",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#cathedral-and-the-bazaar-cont.",
    "title": "Open source licensing",
    "section": "Cathedral and the bazaar (cont.)",
    "text": "Cathedral and the bazaar (cont.)\n\nThe next best thing to having good ideas is recognizing good ideas from your users. Sometimes the latter is better.\nOften, the most striking and innovative solutions come from realizing that your concept of the problem was wrong.\nPerfection (in design) is achieved not when there is nothing more to add, but rather when there is nothing more to take away. (Attributed to Antoine de Saint-Exup√©ry)\nAny tool should be useful in the expected way, but a truly great tool lends itself to uses you never expected.\nWhen writing gateway software of any kind, take pains to disturb the data stream as little as possible‚Äîand never throw away information unless the recipient forces you to!\nWhen your language is nowhere near Turing-complete, syntactic sugar can be your friend.\nA security system is only as secure as its secret. Beware of pseudo-secrets.\nTo solve an interesting problem, start by finding a problem that is interesting to you.\nProvided the development coordinator has a communications medium at least as good as the Internet, and knows how to lead without coercion, many heads are inevitably better than one."
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#the-disciplines-of-open-source-are-the-disciplines-of-good-data-science",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#the-disciplines-of-open-source-are-the-disciplines-of-good-data-science",
    "title": "Open source licensing",
    "section": "The disciplines of open source are the disciplines of good data science",
    "text": "The disciplines of open source are the disciplines of good data science\n\nMeaningful README\nMeaningful commit messages\nModularity\nSeparating data code from analytic code from interactive code\nAssigning issues and pull requests for action/ review\nDon‚Äôt forget one of the most lazy and incompetent developers you will ever work with is yourself, six months later"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#what-licences-exist",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#what-licences-exist",
    "title": "Open source licensing",
    "section": "What licences exist?",
    "text": "What licences exist?\n\nPermissive\n\nSuch as MIT but there are others. Recommended by NHSX draft guidelines on open source\nApache is a notable permissive licence- includes a patent licence\nIn our work the OGL is also relevant- civil servant publish stuff under OGL (and MIT- it isn‚Äôt particularly recommended for code)\n\nCopyleft\n\nGPL2, GPL3, AGPL (‚Äúthe GPL of the web‚Äù)\nNote that the provisions of the GPL only apply when you distribute the code\nAt a certain point it all gets too complicated and you need a lawyer\nMPL is a notable copyleft licence- can combine with proprietary code as long as kept separate\n\nArguments for permissive/ copyleft- getting your code used versus preserving software freedoms for other people\nNote that most of the licences are impossible to read! There is a website to explain tl;dr"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#what-is-copyright-and-why-does-it-matter",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#what-is-copyright-and-why-does-it-matter",
    "title": "Open source licensing",
    "section": "What is copyright and why does it matter",
    "text": "What is copyright and why does it matter\n\nCopyright is assigned at the moment of creation\nIf you made it in your own time, it‚Äôs yours (usually!)\nIf you made it at work, it belongs to your employer\nIf someone paid you to make it (‚Äúwork for hire‚Äù) it belongs to them\nCrucially, the copyright holder can relicence software\n\nIf it‚Äôs jointly authored it depends if it‚Äôs a ‚Äúcollective‚Äù or ‚Äújoint‚Äù work\nHonestly it‚Äôs pretty complicated. Just vest copyright in an organisation or group of individuals you trust\nGoldacre review suggests using Crown copyright for copyright in the NHS because it‚Äôs a ‚Äúshoal, not a big fish‚Äù (with apologies to Ben whom I am misquoting)"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#iceweasel",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#iceweasel",
    "title": "Open source licensing",
    "section": "Iceweasel",
    "text": "Iceweasel\n\nIceweasel is a story of trademark rather than copyright\nDebian (a Linux flavour) had the permission to use the source code of Firefox, but not the logo\nSo they took the source code and made their own version\nThis sounds very obscure and unimportant but it could become important in future projects of ours, like‚Ä¶"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#what-we-have-learned-in-recent-projects",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#what-we-have-learned-in-recent-projects",
    "title": "Open source licensing",
    "section": "What we have learned in recent projects",
    "text": "What we have learned in recent projects\n\nThe huge benefits of being open\n\nTransparency\nWorking with customers\nGoodwill\n\nNonfree mitigators\nDifferent licences for different repos"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#software-freedom-means-allowing-people-to-do-stuff-you-dont-like",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#software-freedom-means-allowing-people-to-do-stuff-you-dont-like",
    "title": "Open source licensing",
    "section": "Software freedom means allowing people to do stuff you don‚Äôt like",
    "text": "Software freedom means allowing people to do stuff you don‚Äôt like\n\nFreedom 0: The freedom to use the program for any purpose.\nFreedom 3: The freedom to improve the program, and release your improvements (and modified versions in general) to the public, so that the whole community benefits.\nThe code isn‚Äôt the only thing with worth in the project\nThis is why there are whole businesses founded on ‚Äúhere‚Äôs the Linux source code‚Äù\nSo when we‚Äôre sharing code we are letting people do stupid things with it but we‚Äôre not recommending that they do stupid things with it\nPeople do stupid things with Excel and Microsoft don‚Äôt accept liability for that, and neither should we\nThis issue of sharing analytic code and merchantability for a particular purpose is poorly understood and I think everyone needs to be clearer on it (us, and our customers)\nIn my view a world where consultants are selling our code is better than a world where they‚Äôre selling their spreadsheets"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#open-source-as-in-piano",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#open-source-as-in-piano",
    "title": "Open source licensing",
    "section": "‚ÄúOpen source as in piano‚Äù",
    "text": "‚ÄúOpen source as in piano‚Äù\n\nThe patient experience QDC project\nOur current project\nOpen source code is not necessarily to be run, but understood and learned from\nBuilding a group of people who can use and contribute to your code is arguably as important as writing it"
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "Data Science Blog",
    "section": "",
    "text": "Mapping my R journey so far: ten things that I have done along the way\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\nMar 10, 2025\n\n\nSheila Ali\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to text vectorization\n\n\n\n\n\n\nNLP\n\n\nPython\n\n\nTutorial\n\n\n\nWhat is it and why do we care? First steps in Natural Language Processing\n\n\n\n\n\nJan 3, 2025\n\n\nYiWen Hon\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy previews with GitHub pages\n\n\n\n\n\n\nGitHub\n\n\nlearning\n\n\ndeployment\n\n\n\nWouldn‚Äôt it be nice if when a PR is created, you automatically got a deployed version of your changes to look at?\n\n\n\n\n\nDec 4, 2024\n\n\nRhian Davies\n\n\n\n\n\n\n\n\n\n\n\n\nUsing GitHub to plan and organise Coffee & Coding\n\n\n\n\n\n\nGitHub\n\n\nlearning\n\n\n\n\n\n\n\n\n\nNov 12, 2024\n\n\nYiWen Hon\n\n\n\n\n\n\n\n\n\n\n\n\nMap and Nest\n\n\n\n\n\n\npurrr\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nAug 8, 2024\n\n\nRhian Davies\n\n\n\n\n\n\n\n\n\n\n\n\nStoring data safely\n\n\n\n\n\n\nlearning\n\n\nR\n\n\nPython\n\n\n\n\n\n\n\n\n\nMay 22, 2024\n\n\nYiWen Hon, Matt Dray, Claire Welsh\n\n\n\n\n\n\n\n\n\n\n\n\nOne year of coffee & coding\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\nRhian Davies\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio Tips and Tricks\n\n\n\n\n\n\nlearning\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 21, 2024\n\n\nMatt Dray\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising participant recruitment in R using Sankey plots\n\n\n\n\n\n\nlearning\n\n\ntutorial\n\n\nvisualisation\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nCraig Parylo\n\n\n\n\n\n\n\n\n\n\n\n\nNearest neighbour imputation\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nJacqueline Grout\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code and Test Driven Development\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\nJan 10, 2024\n\n\nYiWen Hon\n\n\n\n\n\n\n\n\n\n\n\n\nAlternative remote repositories\n\n\n\n\n\n\ngit\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nApr 26, 2023\n\n\nTom Jemmett\n\n\n\n\n\n\n\n\n\n\n\n\nReinstalling R Packages\n\n\n\n\n\n\ngit\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nApr 26, 2023\n\n\nTom Jemmett\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a hotfix with git\n\n\n\n\n\n\ngit\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMar 24, 2023\n\n\nTom Jemmett\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs/posts/2024-12-04-gha-branch-preview/index.html",
    "href": "blogs/posts/2024-12-04-gha-branch-preview/index.html",
    "title": "Deploy previews with GitHub pages",
    "section": "",
    "text": "When reviewing a pull request (PR) for a Quarto website, it‚Äôs good practice to check the rendered output, as well as the code. This is useful for ensuring that the changes look as expected, for example, ensuring that bullet points have rendered correctly, or that images are well sized.\nHowever, it‚Äôs a pain for the reviewer to clone the repository and render the Quarto site locally just to check it looks correct. Wouldn‚Äôt it be nice if when the PR was created, you automatically got a deployed version of your changes to look at?\nOther development platforms like Netlify and Vercel have offered deploy previews for a while, and although these are free for individual users in public repos they aren‚Äôt free for organisations.\nThere has been discussion of GitHub deploy preview for a few years, but there is currently no ETA for this feature. However, there is a popular GitHub marketplace action deploy-pr-preview by rossjrw which does just what we need.\nThis features of this action are:"
  },
  {
    "objectID": "blogs/posts/2024-12-04-gha-branch-preview/index.html#how-to-use-deploy-pr-preview",
    "href": "blogs/posts/2024-12-04-gha-branch-preview/index.html#how-to-use-deploy-pr-preview",
    "title": "Deploy previews with GitHub pages",
    "section": "How to use deploy-pr-preview",
    "text": "How to use deploy-pr-preview\nWe weren‚Äôt doing any CI/CD on PRs initially, so first I need to define a new workflow. Workflows are defined in .yaml files in the .github/workflows folder. At the top of the workflow, I need to give it a name and tell it when to trigger. In this case I want it to trigger on any PR.\nname: Quarto Preview\n\non:\n  pull_request:\n    types:\n      - opened\n      - reopened\n      - synchronize\n      - closed\nOnce I‚Äôve defined when it should run, I need to specify what it should run. That tends to involve a number of steps such as\n\nChecking out the repository\nInstalling system dependencies\nInstalling packages (via {renv})\nRendering the Quarto site\n\nWe already have a publish.yml workflow for main which has a number of relevant steps which I‚Äôve borrowed.\nThe file looks a bit like this:\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    defaults:\n      run:\n        working-directory: ./.github/workflows\n    permissions:\n      contents: write\n      pull-requests: write\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n\n      - name: Use cache\n        uses: actions/cache@v4\n        with:\n          key: freeze\n          path: _freeze\n\n      - name: Install System Dependencies\n        run:  bash install_system_deps.sh\n\n      - name: Set up R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          use-public-rspm: true\n      \n      - name: Set up renv\n        uses: r-lib/actions/setup-renv@v2\n\n      - name: Setup Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Render\n        uses: quarto-dev/quarto-actions/render@v2\nOnce the site has rendered, we just need to add a step to deploy the PR.\n      - name: Deploy PR Preview\n        uses: rossjrw/pr-preview-action@v1.4.8\n        with:\n          source-dir: ./_site/\nNow everytime a PR is created, or updated, the GitHub Action bot will spring into action."
  },
  {
    "objectID": "blogs/posts/2024-12-04-gha-branch-preview/index.html#adjusting-the-publish.yml",
    "href": "blogs/posts/2024-12-04-gha-branch-preview/index.html#adjusting-the-publish.yml",
    "title": "Deploy previews with GitHub pages",
    "section": "Adjusting the publish.yml",
    "text": "Adjusting the publish.yml\nThat‚Äôs all working and looking good! However, there is one last change we need to make. When someone merges into main, the publish action is triggered, and sometimes in that process the pr-previews can get wiped. This means that your preview link would no longer work if someone merges another branch to main whilst you‚Äôre working on your PR.\nWe were using the standard Quarto publish action which as far as I know, does not have an inbuilt configuration to exclude folders from the clean up process.\nInstead, I‚Äôve replaced the Quarto action with another marketplace action, github-pages-deploy-action by JamesIves which has an inbuilt way to exclude certain folders from the clean-up. Since it‚Äôs not a specific Quarto publish action, we need to remember to add a Quarto render step first.\n      - name: Publish to GitHub Pages (and render)\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n      - name: Render Quarto\n        uses: quarto-dev/quarto-actions/render@v2\n\n      - name: Deploy üöÄ\n        uses: JamesIves/github-pages-deploy-action@v4\n        with:\n          folder: _site/\n          clean-exclude: pr-preview/\n          force: false"
  },
  {
    "objectID": "blogs/posts/2024-12-04-gha-branch-preview/index.html#spring-cleaning",
    "href": "blogs/posts/2024-12-04-gha-branch-preview/index.html#spring-cleaning",
    "title": "Deploy previews with GitHub pages",
    "section": "Spring cleaning",
    "text": "Spring cleaning\nNow that we have a preview workflow (for branches) and a publish workflow (for main), we have a number of duplicated steps. As any good programmer knows, this is not ideal as it goes against the DRY prinicipal and makes code harder to maintain.\nI couldn‚Äôt find a quick way for two workflows to share steps1, so as a small improvement, I moved the lengthy system depencies call into it‚Äôs own script. I then ran the script using the following snippet. and called it via the following step.\n      - name: Install System Dependencies\n        run:  bash install_system_deps.sh\nwhich I think is a bit nicer to read than the original.\n - name: Install System Dependencies\n        run: |\n          sudo apt update\n          sudo apt install -y cmake\n          sudo apt install -y gdal-bin\n          sudo apt install -y git\n          sudo apt install -y libcurl4-openssl-dev\n          ...\nIn order for the actions to find the script, I also specified the path when I defined the job.\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    defaults:\n      run:\n        working-directory: ./.github/workflows\nNow we‚Äôve got this working for GitHub pages üéâ we‚Äôd also like to start using it for some of our deployments on Posit Connect. But that‚Äôs for another day."
  },
  {
    "objectID": "blogs/posts/2024-12-04-gha-branch-preview/index.html#footnotes",
    "href": "blogs/posts/2024-12-04-gha-branch-preview/index.html#footnotes",
    "title": "Deploy previews with GitHub pages",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you know how to do this, do let me know, or make a PR.‚Ü©Ô∏é"
  },
  {
    "objectID": "blogs/posts/2024-11-29-mapping-my-r-learning/index.html",
    "href": "blogs/posts/2024-11-29-mapping-my-r-learning/index.html",
    "title": "Mapping my R journey so far: ten things that I have done along the way",
    "section": "",
    "text": "This blog post follows up from a talk I gave last year at coffee and coding about my experiences of learning how to code using Rstudio. Here I build on that talk to share some more reflections and advice for others who are starting out on their R learning journey.\nI have tried to learn R a few times over several years, with mixed success. When I first tried learning it a few years ago, I only managed to learn some basics. The second time, I was going through a crisis of confidence about my ability, and so when I had difficulties with learning R, I thought it was more evidence to show that I couldn‚Äôt do it. I tried again, and got to the stage of making a plot with some of the data that was included with Rstudio. Soon after that I got swept up in the demands of everyday life, and gradually my work moved away from the world of quantitative data into qualitative research, and I had fewer opportunities to use R. Still, in the back of my mind I had this strange feeling of both wanting to avoid R, but also wondering what it would have been like if I had persisted with learning it.\nA couple of years later, when I started my current job, I heard about the NHS-R community, and felt encouraged to learn R again. I tried to join my colleagues who were participating in Advent of Code. But I couldn‚Äôt understand a lot of what was going on, and when I tried to participate in some of the exercises, I immediately hit some hurdles with the basics, which was discouraging.\nIt seemed important to try and change my approach, so that learning R didn‚Äôt seem so daunting. I came across the aRtsy package and was amazed by the colourful and intricate artwork that it could produce. But better still, all of the code was open-source. I experimented with the code, making very small changes to see what kind of images it would create.\nI also discovered colour palettes such as those in the wesanderson package, and tried experimenting with those along with the generative art functions. I soon found that my fear of R was quickly replaced by a geeky fascination with all of the beautiful artwork that could be created with only a few lines of code. It felt like a low-stakes situation, because the worst that could happen was that the code wouldn‚Äôt work. Suddenly, the process of coding felt less intimidating, and it had opened up a wealth of possibilities1.\nThe great thing about R is that it is free and open source. I believe this lends itself well to a culture of shared learning. When I joined the SU‚Äôs Coffee and Coding sessions and NHS-R community‚Äôs Coffee and Code, I felt like a child asking very silly questions, but to my surprise, all of the people I have met have been keen to answer my questions. I learned to recognise and value the people in those communities who would encourage me and fellow learners by making time to answer our questions and help us learn.\nThis meant learning some of the key words and phrases, and getting exposure to the language in various ways: reading learning materials, watching tutorials, and spending time with people who were using it, and writing my own code. This had an incremental effect and over time, the more information I absorbed, the more familiar I became with the terminology.\nIn my day job, I was working on a qualitative case study and wanted to illustrate my findings using geospatial and population density data in the form of a choropleth map. Unfortunately this was one of the most challenging tasks I could have chosen as an R novice, but luckily, I had kind mentors who both believed I could achieve the task and were also on hand to help me learn the skills I needed. So I set myself the goal of trying to learn how to create a choropleth map by the end of the year. This involved breaking the task down into steps, and learning skills which I could build on along the way. I celebrated my small wins, even the tiny ones, until I achieved the goals I set for myself.\nThis involved watching tutorials on YouTube, working through books (such as R for Data Science and R for non-programmers, trying out online coding courses, using search engines and forums, and asking my colleagues and mentors for advice about what resources I should look at as well as what to avoid.\nAlthough learning resources were plentiful, I faced some common barriers when trying to use them. Often tutorials were not always written in a way that I could reproduce the code or access the data they cited, or were written in very technical language, which meant that I had to go away and learn some key concepts to be able to understand them properly. Therefore an important part of the learning journey for me has been to gradually build up a vocabulary of words and concepts in Rstudio. This has enabled me to better understand what key concepts I need to learn, and to understand the content of any training materials or tutorials. I realised that chipping away at it, spending an hour here and there, several times a week, was the best approach for me specifically, with some bigger blocks of time set aside occasionally for more difficult tasks where I could just spend a couple of hours trying out different things or understanding the problem in more depth.\nWhen I became more confident with trying out some packages and functions in R, I decided to find opportunities to apply my learning to real data. I practiced using the inbuilt datasets in Rstudio, the palmerpenguins dataset, and the datasets that were referred to in the books and learning resources I was using. For creating my choropleth maps, I then used data from the UK Census as well as geographical data about local authority geographical boundaries. Applying my learning to real data was an essential step in learning some of the key data wrangling skills.\nOver time I understood that failure is part of the learning journey, and a helpful tool for the learning process itself. If I could figure out what didn‚Äôt work, that often gave me information about what had gone wrong. This was useful as it either pointed me towards what I needed to fix, or gave me the words and concepts I could look into to help me solve the problem. Sometimes the process of trying to learn different functions accidentally produced hilariously terrible results2\nAs well as providing some humour to contrast with the often frustrating process of learning to code, these failures also helped me to get unstuck. More often than not, they were a catalyst for problem-solving as they provided useful information about what specific aspect of the code had gone wrong, which would give me a clue about what I needed to look into to fix the problem.\nOne of my worries about trying to learn R was that learning new things took more time, now I was years older than the last time I tried. But I was fairly confident that there must have been other people out there who had successfully learned how to code when they were my age or older. This led to a fascinating rabbit hole of learning about people who had successfully learned to code later in their life and the hidden history of women in coding. I bookmarked these stories so that I could revisit them on the days where I was having a difficult time understanding a particular concept or getting my code to work.\nThroughout my R learning journey, I have found that coding has been a useful conduit for my creativity, and similarly, my creative projects outside of work have been a catalyst for learning some key concepts related to coding3.\nI realised this a few months ago when my friend got me a beginner‚Äôs embroidery kit, and as I followed the pattern and learned how to create the different types of embroidery stitch, I reflected that just like with the embroidery pattern I was working on, I needed to structure the coding for the map in layers. This led me to approach the process like I would for an art project4 to identify what I needed to do to adequately visualise both types of data that I wanted to include in the map.\nAs I write this, it has been over a year since I re-started my R learning journey in earnest. Early on in the journey, I remember feeling overwhelmed by the kindness and helpfulness of the community. I decided to channel these feelings into learning as best I could, so that I could then pass the learning on. I was reminded of this when I attended the most recent RPYSOC conference where I once again experienced the warm sense of collaboration and community in NHS-R and NHS.pycom. Therefore my aim for 2025 and beyond is to continue my R learning journey (and become more familiar with GitHub), so that I can give back to the wonderful communities that helped me to find my way."
  },
  {
    "objectID": "blogs/posts/2024-11-29-mapping-my-r-learning/index.html#footnotes",
    "href": "blogs/posts/2024-11-29-mapping-my-r-learning/index.html#footnotes",
    "title": "Mapping my R journey so far: ten things that I have done along the way",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf this topic is of interest, I would recommend getting involved in the Tidy Tuesday community activity and also having a look at Nicola Rennie‚Äôs data visualisations.‚Ü©Ô∏é\nThis amused me greatly, as a fan of the Terrible Maps social media pages.‚Ü©Ô∏é\nThis has also worked the other way around, with my R learning journey helping me with learning new crafts. I have recently begun learning sewing and dressmaking. I have quickly found that the learning journey is just as intimidating, meticulous and complicated as it was for learning R. I have also unintentionally chosen a very complicated project for a beginner, which has resulted in a very steep learning curve and lots of failures and mistakes along the way. Throughout the process, I have applied some of the same principles as I did for learning coding. For example, one of the key parts of my journey of learning sewing and dressmaking has been the process of embracing and learning from failure. This has been essential both in terms of knowing what not to do next time, but also to learn how to fix mistakes, ideally early on in a practice situation (e.g.¬†when creating a mock-up). Luckily there is a large community of supportive fellow learners and patient mentors, who are keen to help with fixing mistakes and to pass on their knowledge to new learners. I‚Äôm pleased to say, with a lot of help (and many failures) along the way, I did eventually manage to produce three choropleth maps and submitted them with the report late last year.‚Ü©Ô∏é\nThroughout the journey I have realised that thinking about the problem like an artist has been very helpful, because it allows me to use a similarly iterative approach. I wanted my choropleth maps to show both the population density and the underlying terrain when superimposed. To do this, I used the colorbrewer2 tool to test out different colour palettes, and changed the opacity and terrain to identify which colours would clearly to show the population data and the terrain underneath. The tool let me test this on an example map and showed me the hexadecimal colour codes for the colours in the palettes. Once I had found some combinations that would likely work for my particular map, I then iteratively adjusted the aesthetics in my R code until I found a combination that worked for my data. ¬†‚Ü©Ô∏é"
  },
  {
    "objectID": "blogs/posts/2024-08-08_map-and-nest/index.html",
    "href": "blogs/posts/2024-08-08_map-and-nest/index.html",
    "title": "Map and Nest",
    "section": "",
    "text": "I want to share a framework that I like using occasionally for data analysis. It‚Äôs the nest-and-map and it‚Äôs helped me countless times when I‚Äôm working with related datasets. By combining {purrr} mapping with {tidyr} nesting, I can keep my analysis steps linked, allowing me to easily track from a summary or plot, back to the original data.\nThe main funtions we‚Äôll need are"
  },
  {
    "objectID": "blogs/posts/2024-08-08_map-and-nest/index.html#example-on-nhs-workforce-statistics",
    "href": "blogs/posts/2024-08-08_map-and-nest/index.html#example-on-nhs-workforce-statistics",
    "title": "Map and Nest",
    "section": "Example on NHS workforce statistics",
    "text": "Example on NHS workforce statistics\nThe NHS workforce statistics are official statistics published monthly for England.\n\nstaff_group &lt;- readRDS(file = \"workforce_staff_group.rds\")\n\nI want to perform an analysis for each of the 42 integrated care systems (ICS). The {tidyr} nest() function creates a list-column, where each cell contains a mini dataframe for each grouping.\nLet‚Äôs group by ICS, and call the nested data column raw_data.\n\ngroup_by_ics &lt;- staff_group |&gt;\n    tidyr::nest(raw_data = -ics_name)\n\nThe new column is a list-column, with each cell containing an entire tibble of data relating to that individual ICS.\n\n#' echo: false\nhead(group_by_ics)\n\n# A tibble: 6 √ó 2\n  ics_name             raw_data         \n  &lt;chr&gt;                &lt;list&gt;           \n1 South East London    &lt;tibble [8 √ó 6]&gt; \n2 North East London    &lt;tibble [7 √ó 6]&gt; \n3 North Central London &lt;tibble [12 √ó 6]&gt;\n4 North West London    &lt;tibble [10 √ó 6]&gt;\n5 South West London    &lt;tibble [8 √ó 6]&gt; \n6 Devon                &lt;tibble [7 √ó 6]&gt; \n\n\nWe can grab these mini datasets in the usual way and explore them interactively.\n\ngroup_by_ics$raw_data[[1]]\n\n# A tibble: 8 √ó 6\n  organisation_name           total hchs_doctors nurses_health_visitors midwives\n  &lt;chr&gt;                       &lt;dbl&gt;        &lt;dbl&gt;                  &lt;dbl&gt;    &lt;dbl&gt;\n1 Total                       58394         7108                  14939      926\n2 Guy's and St Thomas' NHS F‚Ä¶ 21361         3003                   6196      281\n3 King's College Hospital NH‚Ä¶ 13158         2443                   4202      375\n4 Lewisham and Greenwich NHS‚Ä¶  6617          979                   2103      271\n5 London Ambulance Service N‚Ä¶  7050            4                     44        0\n6 NHS South East London ICB     617            9                     43        0\n7 Oxleas NHS Foundation Trust  4094          200                   1196        0\n8 South London and Maudsley ‚Ä¶  5496          471                   1155        0\n# ‚Ñπ 1 more variable: ambulance_staff &lt;dbl&gt;\n\n\nNext, let‚Äôs apply some simple processing, say converting absolute numbers into percentages, to each of the ICSs in turn.\nWe use mutate() to create a new list-column staff_percent and map() to apply the processing function to each cell in turn. 1\n\n\nSee function definition for convert_percent()\n\n\n#' Convert percent\n#' @param raw_staff Tibble containing organisation_name, total and a number of staff categories\n#' @return Tibble like raw_staff but with staff categories represented as percentages rather than absolute numbers\nconvert_percent &lt;- function(staff){\n    staff |&gt;\n    dplyr::mutate(dplyr::across(.cols = -c(organisation_name, total),\n                  .fns =  \\(x)x/total)) |&gt;\n    dplyr::rename(\"Doctors\" = \"hchs_doctors\",\n                  \"Nurses\" = \"nurses_health_visitors\",\n                  \"Ambulance staff\" = \"ambulance_staff\",\n                  \"Midwives\" = \"midwives\")\n}\n\n\n\nprocessed_staff &lt;-\ngroup_by_ics |&gt;\n    dplyr::mutate(\n        staff_percent = purrr::map(raw_data, convert_percent)\n    )\n\nWhere I think this map-and-nest process really comes into its own is creating plots. Often, I find myself wanting to create a couple of different plots for each grouping, and then optionally save the plots with sensible names. Particularly in the analysis stage, I like having these plots in the same row as the raw data, so I can quickly compare and validate.\nI‚Äôve created two functions, plot_barchart() and plot_waffle() which take the data and create charts.\n\n\nSee definition for plot_barchart() & plot_waffle()\n\n\n#' Plot barchart\n#' Makes a bar chart of staff perentages by organisation\n#' @param df tibble of staff data in percent format\nplot_barchart &lt;- function(df) {\n  df |&gt;\n    dplyr::filter(organisation_name != \"Total\") |&gt;\n    dplyr::select(-total) |&gt;\n    tidyr::pivot_longer(cols = -c(organisation_name), names_to = \"job\", values_to = \"percent\") |&gt;\n    ggplot2::ggplot(ggplot2::aes(x = percent, y = organisation_name, fill = job)) +\n    ggplot2::geom_col(position = \"dodge\") + \n    ggplot2::scale_x_continuous(labels = scales::percent_format(scale = 100)) +\n    ggplot2::labs(x = \"\", y = \"\") +\n    StrategyUnitTheme::scale_fill_su() + \n    ggplot2::theme_minimal() + \n    ggplot2::theme(legend.title = ggplot2::element_blank())\n}\n\n#' Plot waffle\n#' Makes a waffle chart to visualise staff breakdown at an ICS level\n#' @param raw_staff count data of staff\n#' @param title Title for the graphic\nplot_waffle &lt;- function(raw_staff, title) {\nwaffle_data &lt;-\nraw_staff |&gt;\n    dplyr::filter(organisation_name == \"Total\") |&gt;\n    dplyr::select(-total, -organisation_name) |&gt;\n    tidyr::pivot_longer(cols = dplyr::everything(), names_to = \"names\", values_to = \"vals\") |&gt;\n    dplyr::mutate(vals = round(vals / 100))\n\nggplot2::ggplot(waffle_data, ggplot2::aes(fill = names, values = vals)) +\n  waffle::geom_waffle(n_rows = 8, size = 0.33, colour = \"white\") +\n  ggplot2::coord_equal() +\n  ggplot2::theme_void() + \n  ggplot2::theme(legend.title = ggplot2::element_blank()) +\n  ggplot2::ggtitle(title)\n}\n\n\nAgain, using mutate() I can create a new column called barchart and I can map() the function plot_barchart(), applying it to each row at a time.\n\ngraphs &lt;-\nprocessed_staff |&gt;\n    dplyr::mutate(\n        barchart =  purrr::map(staff_percent, plot_barchart)\n    ) \n\nThe resulting column barchart is again a list-column, but this time instead of containing a tibble, it holds a ggplot object. A whole ggplot in a single cell. 2\nIf we want to pass two arguments to our function, we can replace map() with map2(). Here we‚Äôre using map2() to pass the ics_name column to use as a title in our waffle plot. 3\n\ngraphs &lt;-\nprocessed_staff |&gt;\n    dplyr::mutate(\n        waffle =  purrr::map2(raw_data, ics_name, \n            \\(data, title) plot_waffle(data, title)\n        )\n    ) \n\n\n\n\nAn example bar chart plot"
  },
  {
    "objectID": "blogs/posts/2024-08-08_map-and-nest/index.html#putting-it-all-together",
    "href": "blogs/posts/2024-08-08_map-and-nest/index.html#putting-it-all-together",
    "title": "Map and Nest",
    "section": "Putting it all together",
    "text": "Putting it all together\nAll of these mutate() steps can actually be called in one step. Here‚Äôs the full workflow again in full after a little refactor. I‚Äôve also used pivot_longer() to move the two plotting columns into a single plot column. This will make it easier for me to generate nice filenames, and save the plots.\n\nresults &lt;-\nstaff_group |&gt;\n    tidyr::nest(raw_data = -ics_name) |&gt;\n    dplyr::mutate(\n        staff_percent = purrr::map(raw_data, convert_percent),\n        barchart =  purrr::map(staff_percent, plot_barchart),\n        waffle =  purrr::map2(raw_data, ics_name, \\(data, title) plot_waffle(data, title)) \n    )     |&gt;\n    tidyr::pivot_longer(cols = c(barchart, waffle), names_to = \"plot_type\", values_to = \"plot\") |&gt;\n    dplyr::mutate(filename = glue::glue(\"{snakecase::to_snake_case(ics_name)}_{plot_type}.png\"))\n\nThe walk() family of functions in {purrr} are used when the function you‚Äôre applying does not return an object, but is being used for it‚Äôs side-effect, for example reading or writing files.\nHere we call walk2(), passing in both the filename column and the plots column are arguments to save all the plots.\n\npurrr::walk2(\n  results$filename,\n  results$plot,\n  \\(filename, plot) ggplot2::ggsave(file.path(\"plots\", filename), plot, width = 10, height = 6)\n)\n\nBy keeping everything together in one nested structure, I personally find it much easier to keep track of my analyses. If you‚Äôre doing a more complex or permenant analysis, you might want to consider setting up a more formal data processing pipeline, and following RAP principals."
  },
  {
    "objectID": "blogs/posts/2024-08-08_map-and-nest/index.html#footnotes",
    "href": "blogs/posts/2024-08-08_map-and-nest/index.html#footnotes",
    "title": "Map and Nest",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn this example, we actually didn‚Äôt need to nest first. We could have performed the mutate() step on the full dataset.‚Ü©Ô∏é\nThis totally blew my mind the first time I saw it ü§Ø.‚Ü©Ô∏é\nWe‚Äôre mapping the relationship between the two inputs and the plot_waffle() with an anonymous function. This shorthand syntax for anonymous functions came in R v 4.1.0. For compatibility with older versions of R, you‚Äôll need the ~ operator. For the different ways you can specify functions in {purrr} see the help file.‚Ü©Ô∏é"
  },
  {
    "objectID": "blogs/posts/2024-01-17_nearest_neighbour/index.html",
    "href": "blogs/posts/2024-01-17_nearest_neighbour/index.html",
    "title": "Nearest neighbour imputation",
    "section": "",
    "text": "Recently I have been gathering data by GP practice, from a variety of different sources. The ultimate purpose of my project is to be able to report at an ICB/sub-ICB level1. The various datasets cover different timescales and consequently changes in GP practices over time have left me with mismatching datasets.\n1¬†An ICB (Integrated Care Board) is a statutory NHS organisation responsible for planning health services for their local populationsMy approach has been to take as the basis of my project a recent GP List. Later in my project I want to perform calculations at a GP practice level based on an underlying health need and the data for this need is a CHD prevalence value from a dataset that is around 8 years old, and for which there is no update or alternative. From my recent list of 6454 practices, when I match to the need dataset, I am left with 151 practices without a value for need. If I remove these practices from the analysis then this could impact the analysis by sub-ICB since often a group of practices in the same area could be subject to changes, mergers and reorganisation.\nHere‚Äôs the packages and some demo objects to work with to create an example for two practices:\n\n\nCode\n# Packages\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidygeocoder)\nlibrary(leaflet)\nlibrary(viridisLite)\nlibrary(gt)\n\n# Create some data with two practices with no need data\n# and a selection of practices locally with need data\npractices &lt;- tribble(\n  ~practice_code, ~postcode, ~has_orig_need, ~value,\n  \"P1\", \"CV1 4FS\", 0, NA,\n  \"P2\", \"CV1 3GB\", 1, 7.3,\n  \"P3\", \"CV11 5TW\", 1, 6.9,\n  \"P4\", \"CV6 3HZ\", 1, 7.1,\n  \"P5\", \"CV6 1HS\", 1, 7.7,\n  \"P6\", \"CV6 5DF\", 1, 8.2,\n  \"P7\", \"CV6 3FA\", 1, 7.9,\n  \"P8\", \"CV1 2DL\", 1, 7.5,\n  \"P9\", \"CV1 4JH\", 1, 7.7,\n  \"P10\", \"CV10 0GQ\", 1, 7.5,\n  \"P11\", \"CV10 0JH\", 1, 7.8,\n  \"P12\", \"CV11 5QT\", 0, NA,\n  \"P13\", \"CV11 6AB\", 1, 7.6,\n  \"P14\", \"CV6 4DD\", 1, 7.9\n)\n\n# get domain of numeric data\n(domain &lt;- range(practices$has_orig_need))\n\n# make a colour palette\npal &lt;- colorNumeric(palette = viridis(2), domain = domain)\n\n\nTo provide a suitable estimate of need for the newer practices without values, all the practices in the dataset were geocoded2 using the geocode function from the {tidygeocoder} package.\n2¬†Geocoding is the process of converting addresses (often the postcode) into geographic coordinates (such as latitude and longitude) that can be plotted on a map.\npractices &lt;- practices |&gt;\n  mutate(id = row_number()) |&gt;\n  geocode(postalcode = postcode) |&gt;\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326)\n\n\n\nCode\npractices |&gt;\n  gt()\n\n\n\n\n\n\n\n\npractice_code\npostcode\nhas_orig_need\nvalue\nid\ngeometry\n\n\n\n\nP1\nCV1 4FS\n0\nNA\n1\nc(-1.50672203333333, 52.4140662333333)\n\n\nP2\nCV1 3GB\n1\n7.3\n2\nc(-1.51888, 52.4034199)\n\n\nP3\nCV11 5TW\n1\n6.9\n3\nc(-1.46746, 52.519)\n\n\nP4\nCV6 3HZ\n1\n7.1\n4\nc(-1.52231, 52.42367)\n\n\nP5\nCV6 1HS\n1\n7.7\n5\nc(-1.52542, 52.41989)\n\n\nP6\nCV6 5DF\n1\n8.2\n6\nc(-1.498344825, 52.4250186)\n\n\nP7\nCV6 3FA\n1\n7.9\n7\nc(-1.51787, 52.43135)\n\n\nP8\nCV1 2DL\n1\n7.5\n8\nc(-1.49105, 52.40582)\n\n\nP9\nCV1 4JH\n1\n7.7\n9\nc(-1.5069566, 52.4191685)\n\n\nP10\nCV10 0GQ\n1\n7.5\n10\nc(-1.52197, 52.54074)\n\n\nP11\nCV10 0JH\n1\n7.8\n11\nc(-1.5163199, 52.53723)\n\n\nP12\nCV11 5QT\n0\nNA\n12\nc(-1.46927, 52.51899)\n\n\nP13\nCV11 6AB\n1\n7.6\n13\nc(-1.45822, 52.52682)\n\n\nP14\nCV6 4DD\n1\n7.9\n14\nc(-1.50832, 52.44104)\n\n\n\n\n\n\n\nThis map shows the practices, purple are the practices with no need data and yellow are practices with need data available.\n\n\nCode\n# make map to display practices\nleaflet(practices) |&gt;\n  addTiles() |&gt;\n  addCircleMarkers(color = ~ pal(has_orig_need))\n\n\n\n\n\n\nThe data was split into those with, and without, a value for need. Using st_join from the {sf} package to join those without, and those with, a value for need, using the geometry to find all those within 1500m (1.5km).\n\nno_need &lt;- practices |&gt;\n  filter(has_orig_need == 0)\n\nwith_need &lt;- practices |&gt;\n  filter(has_orig_need == 1)\n\n\nneighbours &lt;- no_need |&gt;\n  select(no_need_postcode = postcode, no_need_prac_code = practice_code) |&gt;\n  st_join(with_need, st_is_within_distance, 1500) |&gt;\n  st_drop_geometry() |&gt;\n  select(id, no_need_postcode, no_need_prac_code) |&gt;\n  inner_join(x = with_need, by = join_by(\"id\"))\n\n\n\nCode\nleaflet(neighbours) |&gt;\n  addTiles() |&gt;\n  addCircleMarkers(color = \"purple\") |&gt;\n  addMarkers(-1.50686326666667, 52.4141089666667, popup = \"Practice with no data\") |&gt;\n  addCircles(-1.50686326666667, 52.4141089666667, radius = 1500) |&gt;\n  addMarkers(-1.46927, 52.51899, popup = \"Practice with no data\") |&gt;\n  addCircles(-1.46927, 52.51899, radius = 1500)\n\n\n\n\n\n\nThe data for the ‚Äúneighbours‚Äù was grouped by the practice code of those without need data and a mean value was calculated for each practice to generate an estimated value.\n\nneighbours_estimate &lt;- neighbours |&gt;\n  group_by(no_need_prac_code) |&gt;\n  summarise(need_est = mean(value)) |&gt;\n  st_drop_geometry(select(no_need_prac_code, need_est))\n\nThe original data was joined back to the ‚Äúneighbours‚Äù.\n\npractices_with_neighbours_estimate &lt;- practices |&gt;\n  left_join(neighbours_estimate, join_by(practice_code == no_need_prac_code)) |&gt;\n  st_drop_geometry(select(practice_code, need_est))\n\n\n\nCode\npractices_with_neighbours_estimate |&gt;\n  select(-has_orig_need, -id) |&gt;\n  gt()\n\n\n\n\n\n\n\n\npractice_code\npostcode\nvalue\nneed_est\n\n\n\n\nP1\nCV1 4FS\nNA\n7.68\n\n\nP2\nCV1 3GB\n7.3\nNA\n\n\nP3\nCV11 5TW\n6.9\nNA\n\n\nP4\nCV6 3HZ\n7.1\nNA\n\n\nP5\nCV6 1HS\n7.7\nNA\n\n\nP6\nCV6 5DF\n8.2\nNA\n\n\nP7\nCV6 3FA\n7.9\nNA\n\n\nP8\nCV1 2DL\n7.5\nNA\n\n\nP9\nCV1 4JH\n7.7\nNA\n\n\nP10\nCV10 0GQ\n7.5\nNA\n\n\nP11\nCV10 0JH\n7.8\nNA\n\n\nP12\nCV11 5QT\nNA\n7.25\n\n\nP13\nCV11 6AB\n7.6\nNA\n\n\nP14\nCV6 4DD\n7.9\nNA\n\n\n\n\n\n\n\nFinally, an updated data frame was created of the need data using the actual need for the practice where available, otherwise using estimated need.\n\npractices_with_neighbours_estimate &lt;- practices_with_neighbours_estimate |&gt;\n  mutate(need_to_use = case_when(value &gt;= 0 ~ value,\n    .default = need_est\n  )) |&gt;\n  select(practice_code, need_to_use)\n\n\n\n\n\n\n\n\n\npractice_code\nneed_to_use\n\n\n\n\nP1\n7.68\n\n\nP2\n7.30\n\n\nP3\n6.90\n\n\nP4\n7.10\n\n\nP5\n7.70\n\n\nP6\n8.20\n\n\nP7\n7.90\n\n\nP8\n7.50\n\n\nP9\n7.70\n\n\nP10\n7.50\n\n\nP11\n7.80\n\n\nP12\n7.25\n\n\nP13\n7.60\n\n\nP14\n7.90\n\n\n\n\n\n\n\nFor my project, this method has successfully generated a prevalence for 125 of the 151 practices without a need value, leaving just 26 practices without a need. This is using a 1.5 km radius. In each use case there will be a decision to make regarding a more accurate estimate (smaller radius) and therefore fewer matches versus a less accurate estimate (using a larger radius) and therefore more matches.\nThis approach could be replicated for other similar uses/purposes. A topical example from an SU project is the need to assign population prevalence for hypertension and compare it to current QOF3 data. Again, the prevalence data is a few years old so we have to move the historical data to fit with current practices and this leaves missing data that can be estimated using this method.\n\n\n3¬†QOF (Quality and Outcomes Framework) is a voluntary annual reward and incentive programme for all GP practices in England, detailing practice achievement results."
  },
  {
    "objectID": "blogs/posts/2024-05-22_storing-data-safely/azure_python.html",
    "href": "blogs/posts/2024-05-22_storing-data-safely/azure_python.html",
    "title": "Data Science @ The Strategy Unit",
    "section": "",
    "text": "import os\nimport io\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom azure.identity import DefaultAzureCredential\nfrom azure.storage.blob import ContainerClient\n\n\n# Load all environment variables\nload_dotenv()\naccount_url = os.getenv('AZ_STORAGE_EP')\ncontainer_name = os.getenv('AZ_STORAGE_CONTAINER')\n\n\n# Authenticate\ndefault_credential = DefaultAzureCredential()\n\nFor the first time, you might need to authenticate via the Azure CLI\nDownload it from https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-windows?tabs=azure-cli\nInstall then run az login in your terminal. Once you have logged in with your browser try the DefaultAzureCredential() again!\n\n# Connect to container\ncontainer_client = ContainerClient(account_url, container_name, default_credential)\n\n\n# List files in container - should be empty\nblob_list = container_client.list_blob_names()\nfor blob in blob_list:\n    if blob.startswith('newdir'):\n        print(blob)\n\nnewdir/cats.parquet\nnewdir/ronald.jpeg\n\n\n\n# Upload file to container\nwith open(file='data/cats.csv', mode=\"rb\") as data:\n    blob_client = container_client.upload_blob(name='newdir/cats.csv', \n                                               data=data, \n                                               overwrite=True)\n\n\n# # Check files have uploaded - List files in container again\nblob_list = container_client.list_blobs()\nfor blob in blob_list:\n    if blob['name'].startswith('newdir'):\n        print(blob['name'])\n\nnewdir/cats.csv\nnewdir/cats.parquet\nnewdir/ronald.jpeg\n\n\n\n# Download file from Azure container to temporary filepath\n\n# Connect to blob\nblob_client = container_client.get_blob_client('newdir/cats.csv')\n\n# Write to local file from blob\ntemp_filepath = os.path.join('temp_data', 'cats.csv')\nwith open(file=temp_filepath, mode=\"wb\") as sample_blob:\n    download_stream = blob_client.download_blob()\n    sample_blob.write(download_stream.readall())\ncat_data = pd.read_csv(temp_filepath)\ncat_data.head()\n\n\n\n\n\n\n\n\nName\nPhysical_characteristics\nBehaviour\n\n\n\n\n0\nRonald\nWhite and ginger\nLazy and greedy but undoubtedly cutest and best\n\n\n1\nKaspie\nSmall calico\nSweet and very shy but adventurous\n\n\n2\nHennimore\nPale orange\nUnhinged and always in a state of panic\n\n\n3\nThug cat\nBlack and white - very large\nLocal bully\n\n\n4\nSon of Stripey\nGrey tabby\nVery vocal\n\n\n\n\n\n\n\n\n# Load directly from Azure - no local copy\n\ndownload_stream = blob_client.download_blob()\nstream_object = io.BytesIO(download_stream.readall())\ncat_data = pd.read_csv(stream_object)\ncat_data\n\n\n\n\n\n\n\n\nName\nPhysical_characteristics\nBehaviour\n\n\n\n\n0\nRonald\nWhite and ginger\nLazy and greedy but undoubtedly cutest and best\n\n\n1\nKaspie\nSmall calico\nSweet and very shy but adventurous\n\n\n2\nHennimore\nPale orange\nUnhinged and always in a state of panic\n\n\n3\nThug cat\nBlack and white - very large\nLocal bully\n\n\n4\nSon of Stripey\nGrey tabby\nVery vocal\n\n\n\n\n\n\n\n\n# !!!!!!!!! Delete from Azure container !!!!!!!!!\nblob_client = container_client.get_blob_client('newdir/cats.csv')\nblob_client.delete_blob()\n\n\nblob_list = container_client.list_blobs()\nfor blob in blob_list:\n    if blob['name'].startswith('newdir'):\n        print(blob['name'])\n\nnewdir/cats.parquet\nnewdir/ronald.jpeg"
  },
  {
    "objectID": "blogs/posts/2024-01-10_advent-of-code-and-test-driven-development/index.html",
    "href": "blogs/posts/2024-01-10_advent-of-code-and-test-driven-development/index.html",
    "title": "Advent of Code and Test Driven Development",
    "section": "",
    "text": "Advent of Code is an annual event, where daily coding puzzles are released from 1st ‚Äì 24th December. We ran one of our fortnightly Coffee & Coding sessions introducing Advent of Code to people who code in the Strategy Unit, as well as the concept of test-driven development as a potential way of approaching the puzzles.\nTest-driven development (TDD) is an approach to coding which involves writing the test for a function BEFORE we write the function. This might seem quite counterintuitive, but it makes it easier to identify bugs üêõ when they are introduced to our code, and ensures that our functions meet all necessary criteria. From my experience, this takes quite a long time to implement and can be quite tedious, but it is definitely worth it overall, especially as your project develops. Testing is also recommended in the NHS Reproducible Analytical Pipeline (RAP) guidelines.\nAn interesting thing to note about TDD is that we‚Äôre always expecting our first test to fail, and indeed failing tests are useful and important! If we wrote tests that just passed all the time, this would not be useful at all for our code.\nThe way that Advent of Code is structured, with test data for each puzzle and an expected test result, makes it very amenable to a test-driven approach. In order to support this, Matt and I created template repositories for a test-driven approach to Advent of Code, in Python and in R.\nOur goal when setting this up was to introduce others in the Strategy Unit to both TDD and Advent of Code. Advent of code can be challenging and I personally struggle to get past the first week, but it encourages creative (and maybe even fun?!) approaches to coding problems. I‚Äôm glad that we had the chance to explore some of the puzzles together in Coffee & Coding ‚Äì it was interesting to see so many different approaches to the same problem, and hopefully it also gave us all the chance to practice writing tests."
  },
  {
    "objectID": "blogs/posts/2023-04-26_reinstalling-r-packages/index.html",
    "href": "blogs/posts/2023-04-26_reinstalling-r-packages/index.html",
    "title": "Reinstalling R Packages",
    "section": "",
    "text": "R 4.3.0 was released last week. Anytime you update R you will probably find yourself in the position where no packages are installed. This is by design - the packages that you have installed may need to be updated and recompiled to work under new versions of R.\nYou may find yourself wanting to have all of the packages that you previously used, so one approach that some people take is to copy the previous library folder to the new versions folder. This isn‚Äôt a good idea and could potentially break your R install.\nAnother approach would be to export the list of packages in R before updating and then using that list after you have updated R. This can cause issues though if you install from places other than CRAN, e.g.¬†bioconductor, or from GitHub.\nSome of these approaches are discussed on the RStudio Community Forum. But I prefer an approach of having a ‚Äúspring clean‚Äù, instead only installing the packages that I know that I need.\nI maintain a list of the packages that I used as a gist. Using this, I can then simply run this script on any new R install. In fact, if you click the ‚Äúraw‚Äù button on the gist, and copy that url, you can simply run\nsource(\"https://gist.githubusercontent.com/tomjemmett/c105d3e0fbea7558088f68c65e68e1ed/raw/a1db4b5fa0d24562d16d3f57fe8c25fb0d8aa53e/setup.R\")\nGenerally, sourcing a url is a bad idea - the reason for this is if it‚Äôs not a link that you control, then someone could update the contents and run arbritary code on your machine. In this case, I‚Äôm happy to run this as it‚Äôs my own gist, but you should be mindful if running it yourself!\nIf you look at the script I first install a number of packages from CRAN, then I install packages that only exist on GitHub."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science @ The Strategy Unit",
    "section": "",
    "text": "This is the home of Data Science activities at The Strategy Unit.\nHere, we host information about how we work, links to presentations, and blogposts relating to how we utilise data science tools.\nAll members of the Strategy Unit are welcome to contribute."
  }
]