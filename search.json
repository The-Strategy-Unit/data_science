[
  {
    "objectID": "blogs/posts/2023-04-26_alternative_remotes.html",
    "href": "blogs/posts/2023-04-26_alternative_remotes.html",
    "title": "Alternative remote repositories",
    "section": "",
    "text": "It‚Äôs great when someone send‚Äôs you a pull request on GitHub to fix bugs or add new features to your project, but you probably always want to check the other persons work in someway before merging that pull request.\nAll of the steps below are intended to be entered via a terminal.\nLet‚Äôs imagine that we have a GitHub account called example and a repository called test, and we use https rather than ssh.\n$ git remote get-url origin\n# https://github.com/example/test.git\nNow, let‚Äôs say we have someone who has submitted a Pull Request (PR), and their username is friend. We can add a new remote for their fork with\n$ git remote add friend https://github.com/friend/test.git\nHere, I name the remote exactly as per the persons GitHub username for no other reason than making it easier to track things later on. You could name this remote whatever you like, but you will need to make sure that the remote url matches their repository correctly.\nWe are now able to checkout their remote branch. First, we will want to fetch their work:\n# make sure to replace the remote name to what you set it to before\n$ git fetch friend\nNow, hopefully they have commited to a branch with a name that you haven‚Äôt used. Let‚Äôs say they created a branch called my_work. You can then simply run\n$ git switch friend/my_work\nThis should checkout the my_work branch locally for you.\nNow, if they have happened to use a branch name that you are already using, or more likely, directly commited to their own main branch, you will need to do checkout to a new branch:\n# replace friend as above to be the name of the remote, and main to be the branch\n# that they have used\n# replace their_work with whatever you want to call this branch locally\n$ git checkout friend/main -b their_work\nYou are now ready to run their code and check everything is good to merge!\nFinally, If you want to clean up your local repository you can remove the new branch that you checked out and the new remote with the following steps:\n# switch back to one of your branches, e.g. main\n$ git checkout main\n\n# then remove the branch that you created above\n$ git branch -D their_work\n\n# you can remove the remote\n$ git remote remove friend"
  },
  {
    "objectID": "blogs/posts/2023-04-26-reinstalling-r-packages.html",
    "href": "blogs/posts/2023-04-26-reinstalling-r-packages.html",
    "title": "Reinstalling R Packages",
    "section": "",
    "text": "R 4.3.0 was released last week. Anytime you update R you will probably find yourself in the position where no packages are installed. This is by design - the packages that you have installed may need to be updated and recompiled to work under new versions of R.\nYou may find yourself wanting to have all of the packages that you previously used, so one approach that some people take is to copy the previous library folder to the new versions folder. This isn‚Äôt a good idea and could potentially break your R install.\nAnother approach would be to export the list of packages in R before updating and then using that list after you have updated R. This can cause issues though if you install from places other than CRAN, e.g.¬†bioconductor, or from GitHub.\nSome of these approaches are discussed on the RStudio Community Forum. But I prefer an approach of having a ‚Äúspring clean‚Äù, instead only installing the packages that I know that I need.\nI maintain a list of the packages that I used as a gist. Using this, I can then simply run this script on any new R install. In fact, if you click the ‚Äúraw‚Äù button on the gist, and copy that url, you can simply run\nsource(\"https://gist.githubusercontent.com/tomjemmett/c105d3e0fbea7558088f68c65e68e1ed/raw/a1db4b5fa0d24562d16d3f57fe8c25fb0d8aa53e/setup.R\")\nGenerally, sourcing a url is a bad idea - the reason for this is if it‚Äôs not a link that you control, then someone could update the contents and run arbritary code on your machine. In this case, I‚Äôm happy to run this as it‚Äôs my own gist, but you should be mindful if running it yourself!\nIf you look at the script I first install a number of packages from CRAN, then I install packages that only exist on GitHub."
  },
  {
    "objectID": "blogs/posts/2023-03-24_hotfix-with-git.html",
    "href": "blogs/posts/2023-03-24_hotfix-with-git.html",
    "title": "Creating a hotfix with git",
    "section": "",
    "text": "I recently discovered a bug in a code-base which needed to be fixed and deployed back to production A.S.A.P., but since the last release the code has moved on significantly. The history looks something a bit like:\nThat is, we have a tag which is the code that is currently in production (which we need to patch), a number of commits after that tag to main (which were separate branches merged via pull requests), and a current development branch.\nI need to somehow: 1. go back to the tagged release, 2. check that code out, 3. patch that code, 4. commit this change, but insert the commit before all of the new commits after the tag\nThere are at least two ways that I know to do this, one would be with an interactive rebase, but I used a slightly longer method, but one I feel is a little less likely to get wrong.\nBelow are the step‚Äôs that I took. One thing I should note is this worked well for my particular issue because the change didn‚Äôt cause any merge conflicts later on."
  },
  {
    "objectID": "blogs/posts/2023-03-24_hotfix-with-git.html#fixing-my-codebase",
    "href": "blogs/posts/2023-03-24_hotfix-with-git.html#fixing-my-codebase",
    "title": "Creating a hotfix with git",
    "section": "Fixing my codebase",
    "text": "Fixing my codebase\nFirst, we need to checkout the tag\ngit checkout -b hotfix v0.2.0\nThis creates a new branch called hotfix off of the tag v0.2.0.\nNow that I have the code base checked out at the point I need to fix, I can make the change that is needed, and commit the change\ngit add [FILENAME]\ngit commit -m \"fixes the code\"\n(Obviously, I used the actual file name and gave a better commit message. I Promise üòù)\nNow my code is fixed, I create a new tag for this ‚Äúrelease‚Äù, as well as push the code to production (this step is omitted here)\ngit tag v0.2.1 -m \"version 0.2.0\"\nAt this point, our history looks something like\n\n\n\n\n\nWhat we want to do is break the link between main and v0.2.0, instead attaching tov0.2.1. First though, I want to make sure that if I make a mistake, I‚Äôm not making it on the main branch.\ngit checkout main\ngit checkout -b apply-hotfix\nThen we can fix our history using the rebase command\ngit rebase hotfix\nWhat this does is it rolls back to the point where the branch that we are rebasing (apply-hotfix) and the hotfix branch both share a common commit (v0.2.0 tag). It then applies the commits in the hotfix branch, before reapplying the commits from apply-hotfix (a.k.a. the main branch).\nOne thing to note, if you have any merge conflicts created by your fix, then the rebase will stop and ask you to fix the merge conflicts. There is some information in the GitHub doc‚Äôs for [resolving merge conflicts after a Git rebase][2].\n[2]: https://docs.github.com/en/get-started/using-git/resolving-merge-conflicts-after-a-git-rebase\nAt this point, we can check that the commit history looks correct\ngit log v0.2.0..HEAD\nIf we are happy, then we can apply this to the main branch. I do this by renaming the apply-hotfix branch as main. First, you have to delete the main branch to allow us to rename the branch.\ngit branch -D main\ngit branch -m main\nWe also need to update the other branches to use the new main branch\ngit checkout branch\ngit rebase main\nNow, we should have a history like"
  },
  {
    "objectID": "style/git_and_github.html",
    "href": "style/git_and_github.html",
    "title": "Using Git and GitHub",
    "section": "",
    "text": "All commits should be atomic, in short,\n\nEach commit does one, and only one simple thing, that can be summed up in a simple sentence.\n\nAll of your commits should be in branches, the only changes that are made to the main branch would be via pull requests (PR) that have been reviewed by at minimum one colleague.\n\n\n\n\n\n\nImportant\n\n\n\nNever push to main!\n\n\n\n\n\n\nIf you haven‚Äôt already, file an issue that describes what you are doing - whether it be fixing a bug, adding a feature, or something else. Issues help to keep track of both the work to be done, and the work that has been done.\nYou should ensure that the issue is as detailed as possible as it may need to be picked up by someone else. Even if only you intend to work on the issue, over time it can be easy to forget things if there isn‚Äôt enough information. They can also help you to remember what changes you have made to your code, and why you had to make these changes, which can be useful if you need to provide change logs when you release your code.\n\n\nIssues are a good record to explain what a PR is for. It‚Äôs also a useful area for input from other members of the team.\n\n\n\nOnce the issue is created and you have decided that you are going to work on it, first assign yourself to the issue in GitHub. This is an indication to others and helps to prevent multiple people from independently working on the same issue.\nOnce you have assigned yourself, you must make a new branch to work on that feature. GitHub offers a button to do this automatically on the issue page, on the right-hand side.\n\n\n\nScreenshot links to specific GitHub help page\n\n\n\n\nIf you used the create branch button on GitHub it will automatically close the issue when merged.\nLocally you can then work on the branch, pushing your code regularly to GitHub so it can be run and inspected when you are not around.\nWhen you think that your changes are ready to be merged, it‚Äôs time to create a PR and request a code review.\n\n\n\nWhen you create a PR, you must do two things:\n\nimmediately make someone an assignee - this is the person who will merge the PR. Typically, this should be the person creating the PR (you)\nselect a person (or people) to review the PR\n\nIf your code is not yet ready to be merged then you should use a draft PR.\n\n\nNote that draft PRs are only available on public GitHub repos.\nWait until the reviewer(s) has completed the review and marked it as ready to merge. At this point, the person who is assigned to the PR can complete the merge.\n\n\nMost merges will be the default Create a merge commit but sometimes you may wish to Squash and merge. As the person requesting the PR, you can select whichever option is wanted from the drop-down in GitHub as part of the PR.\n\n\n\nBy using this approach of the assignee completing the merge, it ensures that code quality is maintained and prevents code from being merged when it is not yet ready. For example, you may have started a PR thinking your work is complete, and a reviewer checks the code and agrees to merge, however, you may realise that there are still things to work on, or there are issues that need to be addressed.\nThe person who is assigned to the PR should be the only person making commits to the branch and this will prevent merge conflicts. If you wish for someone else to collaborate on the branch, then you should assign the PR to that person. At that point, they can pull your branch down and work on it, but you must stop using that branch locally.\n\n\n\n\n\n\nImportant\n\n\n\nOnly one person should ever work on changes to a branch at any time, and it is important to communicate with colleagues so they know to pull the latest changes in.\n\n\n\n\nIf the PR is later assigned back to you then you must immediately pull changes.\nThere may be times when you cannot be the assignee on a PR and in those situations you should nominate someone else to be the assignee and in charge of the PR, the same rules as in the paragraph above would then apply.\nIf, as a reviewer, you find that no one is assigned to the branch, you should get in contact with the person who created the branch and decide who is going to be the assigned owner of the PR.\n\n\nIn circumstances where the person who created the PR is an outside collaborator and doesn‚Äôt have permission to merge, then the reviewer should also be the assignee. In these circumstances, the collaborator will be working from their local fork and will be the only person who can push to the branch. The reviewer, once happy to approve the changes, can merge the PR.\n\n\n\n\nWe use semantic versioning."
  },
  {
    "objectID": "style/git_and_github.html#workflow-for-writing-code-with-git-and-github",
    "href": "style/git_and_github.html#workflow-for-writing-code-with-git-and-github",
    "title": "Using Git and GitHub",
    "section": "",
    "text": "If you haven‚Äôt already, file an issue that describes what you are doing - whether it be fixing a bug, adding a feature, or something else. Issues help to keep track of both the work to be done, and the work that has been done.\nYou should ensure that the issue is as detailed as possible as it may need to be picked up by someone else. Even if only you intend to work on the issue, over time it can be easy to forget things if there isn‚Äôt enough information. They can also help you to remember what changes you have made to your code, and why you had to make these changes, which can be useful if you need to provide change logs when you release your code.\n\n\nIssues are a good record to explain what a PR is for. It‚Äôs also a useful area for input from other members of the team.\n\n\n\nOnce the issue is created and you have decided that you are going to work on it, first assign yourself to the issue in GitHub. This is an indication to others and helps to prevent multiple people from independently working on the same issue.\nOnce you have assigned yourself, you must make a new branch to work on that feature. GitHub offers a button to do this automatically on the issue page, on the right-hand side.\n\n\n\nScreenshot links to specific GitHub help page\n\n\n\n\nIf you used the create branch button on GitHub it will automatically close the issue when merged.\nLocally you can then work on the branch, pushing your code regularly to GitHub so it can be run and inspected when you are not around.\nWhen you think that your changes are ready to be merged, it‚Äôs time to create a PR and request a code review.\n\n\n\nWhen you create a PR, you must do two things:\n\nimmediately make someone an assignee - this is the person who will merge the PR. Typically, this should be the person creating the PR (you)\nselect a person (or people) to review the PR\n\nIf your code is not yet ready to be merged then you should use a draft PR.\n\n\nNote that draft PRs are only available on public GitHub repos.\nWait until the reviewer(s) has completed the review and marked it as ready to merge. At this point, the person who is assigned to the PR can complete the merge.\n\n\nMost merges will be the default Create a merge commit but sometimes you may wish to Squash and merge. As the person requesting the PR, you can select whichever option is wanted from the drop-down in GitHub as part of the PR.\n\n\n\nBy using this approach of the assignee completing the merge, it ensures that code quality is maintained and prevents code from being merged when it is not yet ready. For example, you may have started a PR thinking your work is complete, and a reviewer checks the code and agrees to merge, however, you may realise that there are still things to work on, or there are issues that need to be addressed.\nThe person who is assigned to the PR should be the only person making commits to the branch and this will prevent merge conflicts. If you wish for someone else to collaborate on the branch, then you should assign the PR to that person. At that point, they can pull your branch down and work on it, but you must stop using that branch locally.\n\n\n\n\n\n\nImportant\n\n\n\nOnly one person should ever work on changes to a branch at any time, and it is important to communicate with colleagues so they know to pull the latest changes in.\n\n\n\n\nIf the PR is later assigned back to you then you must immediately pull changes.\nThere may be times when you cannot be the assignee on a PR and in those situations you should nominate someone else to be the assignee and in charge of the PR, the same rules as in the paragraph above would then apply.\nIf, as a reviewer, you find that no one is assigned to the branch, you should get in contact with the person who created the branch and decide who is going to be the assigned owner of the PR.\n\n\nIn circumstances where the person who created the PR is an outside collaborator and doesn‚Äôt have permission to merge, then the reviewer should also be the assignee. In these circumstances, the collaborator will be working from their local fork and will be the only person who can push to the branch. The reviewer, once happy to approve the changes, can merge the PR.\n\n\n\n\nWe use semantic versioning."
  },
  {
    "objectID": "style/data_storage.html",
    "href": "style/data_storage.html",
    "title": "Data Storage",
    "section": "",
    "text": "All projects should be commited to version control, with a repository created in the Strategy Unit‚Äôs GitHub organisation.\nIdeally, any data that is used within the project should be part of a targets pipeline.\nThere are a number of considerations about whether to add the data to version control or not. At a high level:\n\nis the data OK to release publicly?\nis the data in a text-based (non-binary) format, such as .csv, .json (rather than say .xlsx)?\nis the data relatively small in size?\n\n\n\nIf data is grabbed from a website, or via an API, create code to download the file/data. Consider whether this is likely to be a stable way of getting the data (does the data change over time? do you suspect that the location of the resource may disappear? is it quick to retrieve the data?). If so, then it doesn‚Äôt make much sense to commit the data to version control as it can always be quickly regenerated.\n\n\n\nLarge files tend not to work particularly well with version control. Specifically, files larger than 100MB will be blocked by GitHub, and files larger than 50MB will generate a warning. But you may even want to class any file over a few MB as large.\nAlternatives for storing large files:\n\nif the file is something that is generated (and reproducible) from other sources, then do not bother tracking the file\nif the file is something that you want tracking with version control, look at git LFS\nif the file needs to be shared publicly, but LFS is not suitable, the file could be stored in Azure blob storage\nif the file needs to be shared privately, also consider Azure blob storage (using something like SAS tokens)\nif the file needs to only be shared within the Strategy Unit then store in SharePoint (i.e.¬†within a teams channel)\n\nUse of network drives should be deprecated and avoided at all costs due to issues of lack of versioning of files and the performance bottleneck that is created by using a network share. If a network share is truly the only way of storing the data for sharing with colleagues, then look at using ways of syncing the file to local storage to avoid performance bottlenecks, such as robocopy."
  },
  {
    "objectID": "style/data_storage.html#data-from-websites",
    "href": "style/data_storage.html#data-from-websites",
    "title": "Data Storage",
    "section": "",
    "text": "If data is grabbed from a website, or via an API, create code to download the file/data. Consider whether this is likely to be a stable way of getting the data (does the data change over time? do you suspect that the location of the resource may disappear? is it quick to retrieve the data?). If so, then it doesn‚Äôt make much sense to commit the data to version control as it can always be quickly regenerated."
  },
  {
    "objectID": "style/data_storage.html#filesize",
    "href": "style/data_storage.html#filesize",
    "title": "Data Storage",
    "section": "",
    "text": "Large files tend not to work particularly well with version control. Specifically, files larger than 100MB will be blocked by GitHub, and files larger than 50MB will generate a warning. But you may even want to class any file over a few MB as large.\nAlternatives for storing large files:\n\nif the file is something that is generated (and reproducible) from other sources, then do not bother tracking the file\nif the file is something that you want tracking with version control, look at git LFS\nif the file needs to be shared publicly, but LFS is not suitable, the file could be stored in Azure blob storage\nif the file needs to be shared privately, also consider Azure blob storage (using something like SAS tokens)\nif the file needs to only be shared within the Strategy Unit then store in SharePoint (i.e.¬†within a teams channel)\n\nUse of network drives should be deprecated and avoided at all costs due to issues of lack of versioning of files and the performance bottleneck that is created by using a network share. If a network share is truly the only way of storing the data for sharing with colleagues, then look at using ways of syncing the file to local storage to avoid performance bottlenecks, such as robocopy."
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#packages-we-are-using-today",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#packages-we-are-using-today",
    "title": "Coffee and Coding",
    "section": "Packages we are using today",
    "text": "Packages we are using today\n\nlibrary(tidyverse)\n\nlibrary(sf)\n\nlibrary(tidygeocoder)\nlibrary(PostcodesioR)\n\nlibrary(osrm)\n\nlibrary(leaflet)"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#getting-boundary-data",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#getting-boundary-data",
    "title": "Coffee and Coding",
    "section": "Getting boundary data",
    "text": "Getting boundary data\nWe can use the ONS‚Äôs Geoportal we can grab boundary data to generate maps\n\n\n\nicb_url &lt;- paste0(\n  \"https://services1.arcgis.com\",\n  \"/ESMARspQHYMw9BZ9/arcgis\",\n  \"/rest/services\",\n  \"/Integrated_Care_Boards_April_2023_EN_BGC\",\n  \"/FeatureServer/0/query\",\n  \"?outFields=*&where=1%3D1&f=geojson\"\n)\nicb_boundaries &lt;- read_sf(icb_url)\n\nicb_boundaries |&gt;\n  ggplot() +\n  geom_sf() +\n  theme_void()"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-is-the-icb_boundaries-data",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-is-the-icb_boundaries-data",
    "title": "Coffee and Coding",
    "section": "What is the icb_boundaries data?",
    "text": "What is the icb_boundaries data?\n\nicb_boundaries |&gt;\n  select(ICB23CD, ICB23NM)\n\nSimple feature collection with 42 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -6.418667 ymin: 49.86479 xmax: 1.763706 ymax: 55.81112\nGeodetic CRS:  WGS 84\n# A tibble: 42 √ó 3\n   ICB23CD   ICB23NM                                                    geometry\n   &lt;chr&gt;     &lt;chr&gt;                                            &lt;MULTIPOLYGON [¬∞]&gt;\n 1 E54000008 NHS Cheshire and Merseyside Integrated C‚Ä¶ (((-3.083264 53.2559, -3‚Ä¶\n 2 E54000010 NHS Staffordshire and Stoke-on-Trent Int‚Ä¶ (((-1.950489 53.21188, -‚Ä¶\n 3 E54000011 NHS Shropshire, Telford and Wrekin Integ‚Ä¶ (((-2.380794 52.99841, -‚Ä¶\n 4 E54000013 NHS Lincolnshire Integrated Care Board    (((0.2687853 52.81584, 0‚Ä¶\n 5 E54000015 NHS Leicester, Leicestershire and Rutlan‚Ä¶ (((-0.7875237 52.97762, ‚Ä¶\n 6 E54000018 NHS Coventry and Warwickshire Integrated‚Ä¶ (((-1.577608 52.67858, -‚Ä¶\n 7 E54000019 NHS Herefordshire and Worcestershire Int‚Ä¶ (((-2.272042 52.43972, -‚Ä¶\n 8 E54000022 NHS Norfolk and Waveney Integrated Care ‚Ä¶ (((1.666741 52.31366, 1.‚Ä¶\n 9 E54000023 NHS Suffolk and North East Essex Integra‚Ä¶ (((0.8997023 51.7732, 0.‚Ä¶\n10 E54000024 NHS Bedfordshire, Luton and Milton Keyne‚Ä¶ (((-0.4577115 52.32009, ‚Ä¶\n# ‚Ñπ 32 more rows"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-dataframes",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-dataframes",
    "title": "Coffee and Coding",
    "section": "Working with geospatial dataframes",
    "text": "Working with geospatial dataframes\nWe can simply join sf data frames and ‚Äúregular‚Äù data frames together\n\n\n\nicb_metrics &lt;- icb_boundaries |&gt;\n  st_drop_geometry() |&gt;\n  select(ICB23CD) |&gt;\n  mutate(admissions = rpois(n(), 1000000))\n\nicb_boundaries |&gt;\n  inner_join(icb_metrics, by = \"ICB23CD\") |&gt;\n  ggplot() +\n  geom_sf(aes(fill = admissions)) +\n  scale_fill_viridis_c() +\n  theme_void()"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-data-frames",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-data-frames",
    "title": "Coffee and Coding",
    "section": "Working with geospatial data frames",
    "text": "Working with geospatial data frames\nWe can manipulate sf objects like other data frames\n\n\n\nlondon_icbs &lt;- icb_boundaries |&gt;\n  filter(ICB23NM |&gt; stringr::str_detect(\"London\"))\n\nggplot() +\n  geom_sf(data = london_icbs) +\n  geom_sf(data = st_centroid(london_icbs)) +\n  theme_void()"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-data-frames-1",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-data-frames-1",
    "title": "Coffee and Coding",
    "section": "Working with geospatial data frames",
    "text": "Working with geospatial data frames\nSummarising the data will combine the geometries.\n\nlondon_icbs |&gt;\n  summarise(area = sum(Shape__Area)) |&gt;\n  # and use geospatial functions to create calculations using the geometry\n  mutate(new_area = st_area(geometry), .before = \"geometry\")\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -0.5102803 ymin: 51.28676 xmax: 0.3340241 ymax: 51.69188\nGeodetic CRS:  WGS 84\n# A tibble: 1 √ó 3\n         area    new_area                                               geometry\n*       &lt;dbl&gt;       [m^2]                                     &lt;MULTIPOLYGON [¬∞]&gt;\n1 1573336388. 1567995610. (((-0.3314819 51.43935, -0.3306676 51.43889, -0.33118‚Ä¶\n\n\n Why the difference in area?\n\n We are using a simplified geometry, so calculating the area will be slightly inaccurate. The original area was calculated on the non-simplified geometries."
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#creating-our-own-geospatial-data",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#creating-our-own-geospatial-data",
    "title": "Coffee and Coding",
    "section": "Creating our own geospatial data",
    "text": "Creating our own geospatial data\n\nlocation_raw &lt;- postcode_lookup(\"B2 4BJ\")\nglimpse(location_raw)\n\nRows: 1\nColumns: 38\n$ postcode                        &lt;chr&gt; \"B2 4BJ\"\n$ quality                         &lt;int&gt; 1\n$ eastings                        &lt;int&gt; 406866\n$ northings                       &lt;int&gt; 286775\n$ country                         &lt;chr&gt; \"England\"\n$ nhs_ha                          &lt;chr&gt; \"West Midlands\"\n$ longitude                       &lt;dbl&gt; -1.90033\n$ latitude                        &lt;dbl&gt; 52.47887\n$ european_electoral_region       &lt;chr&gt; \"West Midlands\"\n$ primary_care_trust              &lt;chr&gt; \"Heart of Birmingham Teaching\"\n$ region                          &lt;chr&gt; \"West Midlands\"\n$ lsoa                            &lt;chr&gt; \"Birmingham 138A\"\n$ msoa                            &lt;chr&gt; \"Birmingham 138\"\n$ incode                          &lt;chr&gt; \"4BJ\"\n$ outcode                         &lt;chr&gt; \"B2\"\n$ parliamentary_constituency      &lt;chr&gt; \"Birmingham, Ladywood\"\n$ admin_district                  &lt;chr&gt; \"Birmingham\"\n$ parish                          &lt;chr&gt; \"Birmingham, unparished area\"\n$ admin_county                    &lt;lgl&gt; NA\n$ date_of_introduction            &lt;chr&gt; \"198001\"\n$ admin_ward                      &lt;chr&gt; \"Ladywood\"\n$ ced                             &lt;lgl&gt; NA\n$ ccg                             &lt;chr&gt; \"NHS Birmingham and Solihull\"\n$ nuts                            &lt;chr&gt; \"Birmingham\"\n$ pfa                             &lt;chr&gt; \"West Midlands\"\n$ admin_district_code             &lt;chr&gt; \"E08000025\"\n$ admin_county_code               &lt;chr&gt; \"E99999999\"\n$ admin_ward_code                 &lt;chr&gt; \"E05011151\"\n$ parish_code                     &lt;chr&gt; \"E43000250\"\n$ parliamentary_constituency_code &lt;chr&gt; \"E14000564\"\n$ ccg_code                        &lt;chr&gt; \"E38000258\"\n$ ccg_id_code                     &lt;chr&gt; \"15E\"\n$ ced_code                        &lt;chr&gt; \"E99999999\"\n$ nuts_code                       &lt;chr&gt; \"TLG31\"\n$ lsoa_code                       &lt;chr&gt; \"E01033620\"\n$ msoa_code                       &lt;chr&gt; \"E02006899\"\n$ lau2_code                       &lt;chr&gt; \"E08000025\"\n$ pfa_code                        &lt;chr&gt; \"E23000014\"\n\n\n\n\n\nlocation &lt;- location_raw |&gt;\n  st_as_sf(coords = c(\"eastings\", \"northings\"), crs = 27700) |&gt;\n  select(postcode, ccg) |&gt;\n  st_transform(crs = 4326)\n\nlocation\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -1.900335 ymin: 52.47886 xmax: -1.900335 ymax: 52.47886\nGeodetic CRS:  WGS 84\n  postcode                         ccg                   geometry\n1   B2 4BJ NHS Birmingham and Solihull POINT (-1.900335 52.47886)"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#creating-a-geospatial-data-frame-for-all-nhs-trusts",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#creating-a-geospatial-data-frame-for-all-nhs-trusts",
    "title": "Coffee and Coding",
    "section": "Creating a geospatial data frame for all NHS Trusts",
    "text": "Creating a geospatial data frame for all NHS Trusts\n\n\n\n# using the NHSRtools package\n# remotes::install_github(\"NHS-R-Community/NHSRtools\")\ntrusts &lt;- ods_get_trusts() |&gt;\n  filter(status == \"Active\") |&gt;\n  select(name, org_id, post_code) |&gt;\n  geocode(postalcode = \"post_code\") |&gt;\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326)\n\n\ntrusts |&gt;\n  leaflet() |&gt;\n  addProviderTiles(\"Stamen.TonerLite\") |&gt;\n  addMarkers(popup = ~name)"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-are-the-nearest-trusts-to-our-location",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-are-the-nearest-trusts-to-our-location",
    "title": "Coffee and Coding",
    "section": "What are the nearest trusts to our location?",
    "text": "What are the nearest trusts to our location?\n\nnearest_trusts &lt;- trusts |&gt;\n  mutate(\n    distance = st_distance(geometry, location)[, 1]\n  ) |&gt;\n  arrange(distance) |&gt;\n  head(5)\n\nnearest_trusts\n\nSimple feature collection with 5 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -1.9384 ymin: 52.4533 xmax: -1.886282 ymax: 52.48764\nGeodetic CRS:  WGS 84\n# A tibble: 5 √ó 5\n  name                       org_id post_code             geometry distance\n  &lt;chr&gt;                      &lt;chr&gt;  &lt;chr&gt;              &lt;POINT [¬∞]&gt;      [m]\n1 BIRMINGHAM WOMEN'S AND CH‚Ä¶ RQ3    B4 6NH     (-1.894241 52.4849)     789.\n2 BIRMINGHAM AND SOLIHULL M‚Ä¶ RXT    B1 3RB    (-1.917663 52.48416)    1313.\n3 BIRMINGHAM COMMUNITY HEAL‚Ä¶ RYW    B7 4BN    (-1.886282 52.48754)    1356.\n4 SANDWELL AND WEST BIRMING‚Ä¶ RXK    B18 7QH   (-1.930203 52.48764)    2246.\n5 UNIVERSITY HOSPITALS BIRM‚Ä¶ RRK    B15 2GW      (-1.9384 52.4533)    3838."
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#lets-find-driving-routes-to-these-trusts",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#lets-find-driving-routes-to-these-trusts",
    "title": "Coffee and Coding",
    "section": "Let‚Äôs find driving routes to these trusts",
    "text": "Let‚Äôs find driving routes to these trusts\n\nroutes &lt;- nearest_trusts |&gt;\n  mutate(\n    route = map(geometry, ~ osrmRoute(location, st_coordinates(.x)))\n  ) |&gt;\n  st_drop_geometry() |&gt;\n  rename(straight_line_distance = distance) |&gt;\n  unnest(route) |&gt;\n  st_as_sf()\n\nroutes\n\nSimple feature collection with 5 features and 8 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -1.93846 ymin: 52.45316 xmax: -1.88527 ymax: 52.49279\nGeodetic CRS:  WGS 84\n# A tibble: 5 √ó 9\n  name     org_id post_code straight_line_distance src   dst   duration distance\n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;                        [m] &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 BIRMING‚Ä¶ RQ3    B4 6NH                      789. 1     dst       5.77     3.09\n2 BIRMING‚Ä¶ RXT    B1 3RB                     1313. 1     dst       6.84     4.14\n3 BIRMING‚Ä¶ RYW    B7 4BN                     1356. 1     dst       7.59     4.29\n4 SANDWEL‚Ä¶ RXK    B18 7QH                    2246. 1     dst       8.78     4.95\n5 UNIVERS‚Ä¶ RRK    B15 2GW                    3838. 1     dst      10.6      4.67\n# ‚Ñπ 1 more variable: geometry &lt;LINESTRING [¬∞]&gt;"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#lets-show-the-routes",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#lets-show-the-routes",
    "title": "Coffee and Coding",
    "section": "Let‚Äôs show the routes",
    "text": "Let‚Äôs show the routes\n\nleaflet(routes) |&gt;\n  addTiles() |&gt;\n  addMarkers(data = location) |&gt;\n  addPolylines(color = \"black\", weight = 3, opacity = 1) |&gt;\n  addCircleMarkers(data = nearest_trusts, radius = 4, opacity = 1, fillOpacity = 1)"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#we-can-use-osrm-to-calculate-isochrones",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#we-can-use-osrm-to-calculate-isochrones",
    "title": "Coffee and Coding",
    "section": "We can use {osrm} to calculate isochrones",
    "text": "We can use {osrm} to calculate isochrones\n\n\n\niso &lt;- osrmIsochrone(location, breaks = seq(0, 60, 15), res = 10)\n\nisochrone_ids &lt;- unique(iso$id)\n\npal &lt;- colorFactor(\n  viridis::viridis(length(isochrone_ids)),\n  isochrone_ids\n)\n\nleaflet(location) |&gt;\n  addProviderTiles(\"Stamen.TonerLite\") |&gt;\n  addMarkers() |&gt;\n  addPolygons(\n    data = iso,\n    fillColor = ~ pal(id),\n    color = \"#000000\",\n    weight = 1\n  )"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones",
    "title": "Coffee and Coding",
    "section": "What trusts are in the isochrones?",
    "text": "What trusts are in the isochrones?\nThe summarise() function will ‚Äúunion‚Äù the geometry\n\nsummarise(iso)\n\nSimple feature collection with 1 feature and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -2.913575 ymin: 51.98062 xmax: -0.8502164 ymax: 53.1084\nGeodetic CRS:  WGS 84\n                        geometry\n1 POLYGON ((-1.541014 52.9693..."
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones-1",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones-1",
    "title": "Coffee and Coding",
    "section": "What trusts are in the isochrones?",
    "text": "What trusts are in the isochrones?\nWe can use this with a geo-filter to find the trusts in the isochrone\n\n# also works\ntrusts_in_iso &lt;- trusts |&gt;\n  st_filter(\n    summarise(iso),\n    .predicate = st_within\n  )\n\ntrusts_in_iso\n\nSimple feature collection with 31 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -2.793386 ymin: 52.19205 xmax: -1.10302 ymax: 53.01015\nGeodetic CRS:  WGS 84\n# A tibble: 31 √ó 4\n   name                               org_id post_code             geometry\n * &lt;chr&gt;                              &lt;chr&gt;  &lt;chr&gt;              &lt;POINT [¬∞]&gt;\n 1 BIRMINGHAM AND SOLIHULL MENTAL HE‚Ä¶ RXT    B1 3RB    (-1.917663 52.48416)\n 2 BIRMINGHAM COMMUNITY HEALTHCARE N‚Ä¶ RYW    B7 4BN    (-1.886282 52.48754)\n 3 BIRMINGHAM WOMEN'S AND CHILDREN'S‚Ä¶ RQ3    B4 6NH     (-1.894241 52.4849)\n 4 BIRMINGHAM WOMEN'S NHS FOUNDATION‚Ä¶ RLU    B15 2TG   (-1.942861 52.45325)\n 5 BURTON HOSPITALS NHS FOUNDATION T‚Ä¶ RJF    DE13 0RB  (-1.656667 52.81774)\n 6 COVENTRY AND WARWICKSHIRE PARTNER‚Ä¶ RYG    CV6 6NY    (-1.48692 52.45659)\n 7 DERBYSHIRE HEALTHCARE NHS FOUNDAT‚Ä¶ RXM    DE22 3LZ  (-1.512896 52.91831)\n 8 DUDLEY INTEGRATED HEALTH AND CARE‚Ä¶ RYK    DY5 1RU    (-2.11786 52.48176)\n 9 GEORGE ELIOT HOSPITAL NHS TRUST    RLT    CV10 7DJ   (-1.47844 52.51258)\n10 HEART OF ENGLAND NHS FOUNDATION T‚Ä¶ RR1    B9 5ST     (-1.828759 52.4781)\n# ‚Ñπ 21 more rows"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones-2",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones-2",
    "title": "Coffee and Coding",
    "section": "What trusts are in the isochrones?",
    "text": "What trusts are in the isochrones?\n\n\n\nleaflet(trusts_in_iso) |&gt;\n  addProviderTiles(\"Stamen.TonerLite\") |&gt;\n  addMarkers() |&gt;\n  addPolygons(\n    data = iso,\n    fillColor = ~pal(id),\n    color = \"#000000\",\n    weight = 1\n  )"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#doing-the-same-but-within-a-radius",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#doing-the-same-but-within-a-radius",
    "title": "Coffee and Coding",
    "section": "Doing the same but within a radius",
    "text": "Doing the same but within a radius\n\n\n\nr &lt;- 25000\n\ntrusts_in_radius &lt;- trusts |&gt;\n  st_filter(\n    location,\n    .predicate = st_is_within_distance,\n    dist = r\n  )\n\n# transforming gives us a pretty smooth circle\nradius &lt;- location |&gt;\n  st_transform(crs = 27700) |&gt;\n  st_buffer(dist = r) |&gt;\n  st_transform(crs = 4326)\n\nleaflet(trusts_in_radius) |&gt;\n  addProviderTiles(\"Stamen.TonerLite\") |&gt;\n  addMarkers() |&gt;\n  addPolygons(\n    data = radius,\n    color = \"#000000\",\n    weight = 1\n  )"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#further-reading",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#further-reading",
    "title": "Coffee and Coding",
    "section": "Further reading",
    "text": "Further reading\n\nGeocomputation with R\nr-spatial\n{sf} documentation\nLeaflet documentation\nTidy Geospatial Networks in R\n\n\n\nview slides at the-strategy-unit.github.io/data_science/presentations"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#section",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#section",
    "title": "Conference Check-in App",
    "section": "",
    "text": "digital.library.unt.edu/ark:/67531/metadc1039451/m1/1/\n\n\nClark, Junebug. [Registration Desk for the LPC Conference], photograph, 2016-03-17/2016-03-19; (https://digital.library.unt.edu/ark:/67531/metadc1039451/m1/1/: accessed October 16, 2023), University of North Texas Libraries, UNT Digital Library, https://digital.library.unt.edu; crediting UNT Libraries Special Collections."
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#qr-codes-are-great",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#qr-codes-are-great",
    "title": "Conference Check-in App",
    "section": "QR codes are great",
    "text": "QR codes are great"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#and-can-be-easily-generated-in-r",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#and-can-be-easily-generated-in-r",
    "title": "Conference Check-in App",
    "section": "and can be easily generated in R",
    "text": "and can be easily generated in R\ninstall.packages(\"qrcode\")\nlibrary(qrcode)\n\nqr_code(\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\")"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#why-not",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#why-not",
    "title": "Conference Check-in App",
    "section": "Why not?",
    "text": "Why not?\n\n{shiny} would be doing all the processing on the server side\nwe would need to read from a camera client side\nthen stream video to the server for {shiny} to detect and decode the QR codes"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work",
    "title": "Conference Check-in App",
    "section": "How does this work?",
    "text": "How does this work?\n\n\nFront-end\n\n\nuses the React JavaScript framework\n@yidel/react-qr-scanner\nApp scan‚Äôs a QR code, then sends this to our backend\nA window pops up to say who has checked in, or shows an error message"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work-1",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work-1",
    "title": "Conference Check-in App",
    "section": "How does this work?",
    "text": "How does this work?\nBack-end\nUses the {plumber} R package to build the API, with endpoints for\n\ngetting the list of all of the attendees for that day\nuploading a list of attendees in bulk\nadding an attendee individually\ngetting an attendee\nchecking the attendee in"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work-2",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work-2",
    "title": "Conference Check-in App",
    "section": "How does this work?",
    "text": "How does this work?\nMore Back-end Stuff\n\nuses a simple SQLite DB that will be thrown away at the end of the conference\nwe send personalised emails using {blastula} to the attendees with their QR codes\nthe QR codes are just random ids (UUIDs) that identify each attendee\nuses websockets to update all of the clients when a user checks in (to update the list of attendees)"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#learning-different-tools-can-show-you-the-light",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#learning-different-tools-can-show-you-the-light",
    "title": "Conference Check-in App",
    "section": "Learning different tools can show you the light",
    "text": "Learning different tools can show you the light\n\nunsplash.com/photos/tMGMINwFOtI"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#the-team",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#the-team",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "The team",
    "text": "The team"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#a-hospital-is-a-place-where-you-can-find-people",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#a-hospital-is-a-place-where-you-can-find-people",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "A hospital is a place where you can find people‚Ä¶",
    "text": "A hospital is a place where you can find people‚Ä¶\n\n\nhaving the best day of their life,\nthe worst day of their life,\nthe first day of their life,\nand the last day of their life."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#planning-is-hard",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#planning-is-hard",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Planning is hard",
    "text": "Planning is hard\n\n\n\n\n\n\n\n\n\nbuilt with enough capacity to replace the existing school\nfailed to take into account a new housing estate\nlikely needs double the number of spaces within the next decade\n\nBBC article"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#review-of-existing-models",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#review-of-existing-models",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Review of existing models",
    "text": "Review of existing models\n\nSteven Wyatt - NHS-R 2022"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#review-of-existing-models-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#review-of-existing-models-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Review of existing models",
    "text": "Review of existing models\n\nlots of models\nlots of external consultancies\nlots of similarities\n\n\n\nlots of repetition/duplication\nsufficiently different that comparing results is difficult\nmethodological progress slow\nno base to build from\n\n\n\nconsultancies don‚Äôt tend to offer products, but services\ndifficult to compare different models to understand if differences are methodological or due to assumptions\nsame issues seen 20/30 years ago\nlearning and expertise gathered tends to be trapped within trusts, or kept secret by consultancies"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#common-issues",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#common-issues",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Common issues",
    "text": "Common issues\n\nhandling uncertainty\nunnecessary/early aggregation\npoor coverage of some changes\nlack of ownership & auditability of assumptions\nconflating demand forecasting with affordability\n\n\n\nmost models handle changes like demographic changes and the impact of changes in occupancy rates\nbut few try to handle addressing inequities, health status adjustment"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#our-model",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#our-model",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Our model",
    "text": "Our model\n\nopen source (not quite yet‚Ä¶)\nuses standard, well-known datasets (e.g.¬†HES, ONS population projections)\ncurrently handles Inpatient admissions, Outpatient attendances, and A&E arrivals\nextensible and adaptable\ncovering all of the change factors\nstochastic Monte-Carlo model to handle uncertainty"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#project-structure",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#project-structure",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Project Structure",
    "text": "Project Structure\n\n\n\nData Extraction (R + {targets} & Sql)\nInputs App (R + {shiny})\nOutputs App (R + {shiny})\nModel Engine (Python & Docker)\nAzure Infrastructure (VM/ACR/ACI/Storage Accounts)\nAll of the code is stored on GitHub (currently, private repos üòî)\n\n\n\n\n\n\nflowchart TB\n  classDef orange fill:#f9bf07,stroke:#2c2825,color:#2c2825;\n  classDef lightslate fill:#b2b7b9,stroke:#2c2825,color:#2c2825;\n\n  A[Data Extraction]\n  B[Inputs App]\n  C[Model]\n  D[Outputs App]\n\n\n  SB[(input app data)]\n  SC[(model data)]\n  SD[(results data)]\n\n  A ---&gt; SB\n  A ---&gt; SC\n  \n  SB ---&gt; B\n  SC ---&gt; C\n\n  B ---&gt; C\n\n  C ---&gt; SD\n  SD ---&gt; D\n\n  B -.-&gt; D\n\n  class A,B,C,D orange\n  class SB,SC,SD lightslate"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-overview",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-overview",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Overview",
    "text": "Model Overview\n\n\nthe baseline data is a year worth of a provider‚Äôs HES data\neach row in the baseline data is run through a series of steps\neach step creates a factor that says how many times (on average) to sample that row\nthe factors are multiplied together and used to create a random Poisson value\nwe resample the rows using this random values\nefficiencies are then applied, e.g.¬†LoS reductions, type conversions\n\n\n\n\nIP/OP/A&E data\ncomplex, but not complicated"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-diagram",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-diagram",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Diagram",
    "text": "Model Diagram\n\n\n\n\nflowchart TB\n    classDef blue fill:#5881c1,stroke:#2c2825,color:#2c2825;\n    classDef orange fill:#f9bf07,stroke:#2c2825,color:#2c2825;\n    classDef red fill:#ec6555,stroke:#2c2825,color:#2c2825;\n    classDef lightslate fill:#b2b7b9,stroke:#2c2825,color:#2c2825;\n    classDef slate fill:#e0e2e3,stroke:#2c2825,color:#2c2825;\n\n    S[Baseline Activity]\n    T[Future Activity]\n\n    class S,T red\n\n    subgraph rr[Row Resampling]\n        direction LR\n\n        subgraph pop[Population Changes]\n            direction TB\n            pop_p[Population Growth]\n            pop_a[Age/Sex Structure]\n            pop_h[Population Specific Health Status]\n\n            class pop_p,pop_a,pop_h orange\n\n            pop_p --- pop_a --- pop_h\n        end\n\n        subgraph dsi[Demand Supply Imbalances]\n            direction TB\n            dsi_w[Waiting List Adjustment]\n            dsi_r[Repatriation/Expatriation]\n            dsi_p[Private Healthcare Dynamics]\n\n            class dsi_w,dsi_r,dsi_p orange\n\n            dsi_w --- dsi_r --- dsi_p\n        end\n\n        subgraph nsi[Need Supply Imbalances]\n            direction TB\n            nsi_g[Gaps in Care]\n            nsi_i[Inequalities]\n            nsi_t[Threshold Imbalances]\n\n            class nsi_g,nsi_i,nsi_t orange\n\n            nsi_g --- nsi_i --- nsi_t\n        end\n\n        subgraph nda [Non-Demographic Adjustment]\n            direction TB\n            nda_m[Medical Interventions]\n            nda_c[Changes to National Standards]\n            nda_p[Patient Expectations]\n\n            class nda_m,nda_c,nda_p orange\n\n            nda_m --- nda_c --- nda_p\n        end\n\n        subgraph mit[Activity Mitigators]\n            direction TB\n            mit_a[Activity Avoidance]\n            mit_t[Type Conversion]\n            mit_e[Efficiencies]\n            \n            class mit_a,mit_t,mit_e orange\n\n            mit_a --- mit_t --- mit_e\n        end\n\n        pop --- dsi --- nsi --- nda --- mit\n\n        class dsi,nsi,pop,nda,mit lightslate\n    end\n\n    class rr slate\n    \n    S --&gt; rr --&gt; T\n\n\n\n\n\n\n\nuses either patient-level data, or minimal aggregation\nrow resampling grouped into 5 broad groups\n\npopulation changes address the changes to the structure of the population and health status over the medium term\ndemand supply imbalances: hospitals are currently struggling to keep pace with demand, so we correct for this to not carry forwards these into the future\nneed supply imbalance: addressing gaps in care that currently exist\nnon-demographic: such as the development of new medical technologies\nactivity mitigators: strategies trusts adopt for reducing activity, or delivering activity more efficiently\n\nsome assumptions set nationally, such as population growth via ONS population projections\nother assumptions set locally, with support from a Shiny app"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-diagram-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-diagram-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Diagram",
    "text": "Model Diagram\n\n\n\n\nflowchart TB\n    classDef blue fill:#5881c1,stroke:#2c2825,color:#2c2825;\n    classDef orange fill:#f9bf07,stroke:#2c2825,color:#2c2825;\n    classDef red fill:#ec6555,stroke:#2c2825,color:#2c2825;\n    classDef lightslate fill:#b2b7b9,stroke:#2c2825,color:#2c2825;\n    classDef slate fill:#e0e2e3,stroke:#2c2825,color:#2c2825;\n\n    S[Baseline Activity]\n    T[Future Activity]\n\n    ORANGE[Implemented]\n    BLUE[Not yet implemented]\n\n    class ORANGE orange\n    class BLUE blue\n\n    class S,T red\n\n    subgraph rr[Row Resampling]\n        direction LR\n\n        subgraph pop[Population Changes]\n            direction TB\n            pop_p[Population Growth]\n            pop_a[Age/Sex Structure]\n            pop_h[Population Specific Health Status]\n\n            class pop_p,pop_a,pop_h orange\n\n            pop_p --- pop_a --- pop_h\n        end\n\n        subgraph dsi[Demand Supply Imbalances]\n            direction TB\n            dsi_w[Waiting List Adjustment]\n            dsi_r[Repatriation/Expatriation]\n            dsi_p[Private Healthcare Dynamics]\n\n            class dsi_w,dsi_r orange\n            class dsi_p blue\n\n            dsi_w --- dsi_r --- dsi_p\n        end\n\n        subgraph nsi[Need Supply Imbalances]\n            direction TB\n            nsi_g[Gaps in Care]\n            nsi_i[Inequalities]\n            nsi_t[Threshold Imbalances]\n\n            class nsi_g,nsi_i,nsi_t blue\n\n            nsi_g --- nsi_i --- nsi_t\n        end\n\n        subgraph nda [Non-Demographic Adjustment]\n            direction TB\n            nda_m[Medical Interventions]\n            nda_c[Changes to National Standards]\n            nda_p[Patient Expectations]\n\n            class nda_m,nda_c,nda_p blue\n\n            nda_m --- nda_c --- nda_p\n        end\n\n        subgraph mit[Activity Mitigators]\n            direction TB\n            mit_a[Activity Avoidance]\n            mit_t[Type Conversion]\n            mit_e[Efficiencies]\n            \n            class mit_a,mit_t,mit_e orange\n\n            mit_a --- mit_t --- mit_e\n        end\n\n        pop --- dsi --- nsi --- nda --- mit\n\n        class dsi,nsi,pop,nda,mit lightslate\n    end\n\n    class rr slate\n    \n    S --&gt; rr --&gt; T"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#monte-carlo-simulation",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#monte-carlo-simulation",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Monte Carlo Simulation",
    "text": "Monte Carlo Simulation\n\n\n\nWe run the model N times, varying the input parameters each time slightly to handle the uncertainty.\nThe results of the model are aggregated at the end of each model run\nThe aggregated results are combined at the end into a single file\n\n\n\n\n\n\nflowchart LR\n  classDef orange fill:#f9bf07,stroke:#2c2825,color:#2c2825;\n  classDef red fill:#ec6555,stroke:#2c2825,color:#2c2825;\n  \n  A[Baseline Activity]\n  Ba[Model Run 0]\n  Bb[Model Run 1]\n  Bc[Model Run 2]\n  Bd[Model Run 3]\n  Bn[Model Run n]\n  C[Results]\n\n  A ---&gt; Ba ---&gt; C\n  A ---&gt; Bb ---&gt; C\n  A ---&gt; Bc ---&gt; C\n  A ---&gt; Bd ---&gt; C\n  A ---&gt; Bn ---&gt; C\n  \n  class A,C red\n  class Ba,Bb,Bc,Bd,Bn orange\n  \n\n\n\n\n\n\nInspired by\n\nMapReduce (Google, 2004)\nSplit, Apply, Combine (H. Wickham, 2011)"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Parameters",
    "text": "Model Parameters\n\nWe ask users to provide parameters in the form of 90% confidence intervals\nWe can then convert these confidence intervals into distributions\nDuring the model we sample values from these distributions for each model parameter\nAll of the parameters represent the average rate to sample a row of data from the baseline"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Parameters",
    "text": "Model Parameters\n\n‚ÄúWe expect in the future to see between a 25% reduction and a 25% increase in this activity‚Äù\n\n\n\n\ngrey highlighted section: 90% confidence intervals\nblack line: confidence intervals into distributions\nyellow points: sampled parameter for a model run"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-2",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-2",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Parameters",
    "text": "Model Parameters\n\n‚ÄúWe expect in the future to see between a 20% reduction and a 90% reduction in this activity‚Äù\n\n\n\n\ngrey highlighted section: 90% confidence intervals\nblack line: confidence intervals into distributions\nyellow points: sampled parameter for a model run"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-3",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-3",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Parameters",
    "text": "Model Parameters\n\n‚ÄúWe expect in the future to see between a 2% reduction and an 18% reduction in this activity‚Äù\n\n\n\n\ngrey highlighted section: 90% confidence intervals\nblack line: confidence intervals into distributions\nyellow points: sampled parameter for a model run"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Run Example (1)",
    "text": "Model Run Example (1)\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\nf\n\n\n\n\n1\n50\nm\n100\n4\n1.00\n\n\n2\n50\nm\n110\n3\n1.00\n\n\n3\n51\nm\n120\n5\n1.00\n\n\n4\n50\nf\n100\n1\n1.00\n\n\n5\n50\nf\n110\n2\n1.00\n\n\n6\n52\nf\n120\n0\n1.00\n\n\n\n\n\n\n\n\n\n\nStart with baseline data - we are going to sample each row exactly once (column f)."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-2",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-2",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Run Example (2)",
    "text": "Model Run Example (2)\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\nf\n\n\n\n\n1\n50\nm\n100\n4\n1.00\n\n\n2\n50\nm\n110\n3\n1.00\n\n\n3\n51\nm\n120\n5\n1.00\n\n\n4\n50\nf\n100\n1\n1.00\n\n\n5\n50\nf\n110\n2\n1.00\n\n\n6\n52\nf\n120\n0\n1.00\n\n\n\n\n\n\n\nage\nsex\nf\n\n\n\n\n50\nm\n0.90\n\n\n51\nm\n1.10\n\n\n52\nm\n1.20\n\n\n50\nf\n0.80\n\n\n51\nf\n0.70\n\n\n52\nf\n1.30\n\n\n\n\n\n\n\nf\n\n\n\n\n1.00 √ó 0.90 = 0.90\n\n\n1.00 √ó 0.90 = 0.90\n\n\n1.00 √ó 1.10 = 1.10\n\n\n1.00 √ó 0.80 = 0.80\n\n\n1.00 √ó 0.80 = 0.80\n\n\n1.00 √ó 1.30 = 1.30\n\n\n\n\n\n\nWe perform a step where we join based on age and sex, then update the f column."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-3",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-3",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Run Example (3)",
    "text": "Model Run Example (3)\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\nf\n\n\n\n\n1\n50\nm\n100\n4\n0.90\n\n\n2\n50\nm\n110\n3\n0.90\n\n\n3\n51\nm\n120\n5\n1.10\n\n\n4\n50\nf\n100\n1\n0.80\n\n\n5\n50\nf\n110\n2\n0.80\n\n\n6\n52\nf\n120\n0\n1.30\n\n\n\n\n\n\n\nspecialty\nf\n\n\n\n\n100\n0.90\n\n\n110\n1.10\n\n\n\n\n\n\n\nf\n\n\n\n\n0.90 √ó 0.90 = 0.81\n\n\n0.90 √ó 1.10 = 0.99\n\n\n1.10 √ó 1.00 = 1.10\n\n\n0.80 √ó 0.90 = 0.72\n\n\n0.80 √ó 1.10 = 0.88\n\n\n1.30 √ó 1.00 = 1.30\n\n\n\n\n\n\nThe next step joins on the specialty column, again updating f. Note, if there is no value to join on, then we multiply by 1."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-4",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-4",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Run Example (4)",
    "text": "Model Run Example (4)\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\nf\nn\n\n\n\n\n1\n50\nm\n100\n4\n0.90\n1\n\n\n2\n50\nm\n110\n3\n0.90\n0\n\n\n3\n51\nm\n120\n5\n1.10\n2\n\n\n4\n50\nf\n100\n1\n0.80\n1\n\n\n5\n50\nf\n110\n2\n0.80\n0\n\n\n6\n52\nf\n120\n0\n1.30\n3\n\n\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\n\n\n\n\n1\n50\nm\n100\n4\n\n\n3\n51\nm\n120\n5\n\n\n3\n51\nm\n120\n5\n\n\n4\n50\nf\n100\n1\n\n\n6\n52\nf\n120\n0\n\n\n6\n52\nf\n120\n0\n\n\n6\n52\nf\n120\n0\n\n\n\n\n\n\nOnce all of the steps are performed, sample a random value n from a Poisson distribution with Œª=f, then we select each row n times."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-5",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-5",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Run Example (5)",
    "text": "Model Run Example (5)\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\ng\n\n\n\n\n1\n50\nm\n100\n4\n0.75\n\n\n3\n51\nm\n120\n5\n0.50\n\n\n3\n51\nm\n120\n5\n1.00\n\n\n4\n50\nf\n100\n1\n0.90\n\n\n6\n52\nf\n120\n0\n0.80\n\n\n6\n52\nf\n120\n0\n0.80\n\n\n6\n52\nf\n120\n0\n0.80\n\n\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\n\n\n\n\n1\n50\nm\n100\n2\n\n\n3\n51\nm\n120\n1\n\n\n3\n51\nm\n120\n5\n\n\n4\n50\nf\n100\n0\n\n\n6\n52\nf\n120\n0\n\n\n6\n52\nf\n120\n0\n\n\n6\n52\nf\n120\n0\n\n\n\n\n\n\nAfter resampling, we apply efficiency steps. E.g., similar joins are used to create column g, which is then used to sample a new LOS from a binomial distribution."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-built",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-built",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "How the model is built",
    "text": "How the model is built\n\nThe model is built in Python and can be run on any machine you can install Python on\nUses various packages, such as numpy and pandas\nReads data in .parquet format for efficiency\nReturns aggregated results as a .json file\nCould also output full row level results if needed"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-built-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-built-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "How the model is built",
    "text": "How the model is built\n\nCode is built in a modular approach\nEach activity type (Inpatients/Outpatients/A&E) has its own model code\nCode is reused where possible (e.g.¬†all three models share the code for demographic adjustment)"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-deployed",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-deployed",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "How the model is deployed",
    "text": "How the model is deployed\n\nDeployed as a Docker Container\nRuns in Azure Container Instances\nEach model run creates a new container, and the container is destroyed when the model run completes"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#data-extraction",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#data-extraction",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Data Extraction",
    "text": "Data Extraction\n\nUses principles of RAP, using R + {targets} and Sql\nAll of the data required to run the model\nData is extracted from various sources\n\nSql Datawarehouse (HES data)\nONS population projections + life expectancy tables\nCentral returns, e.g.¬†KH03\nODS data (organisation names, successors)\n\nExtracted data is uploaded to Azure storage containers"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#inputs-app",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#inputs-app",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Inputs App",
    "text": "Inputs App\nA {shiny} app that allows the user to set parameters, and submit as a job to run the model with those values."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#inputs-app-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#inputs-app-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Inputs App",
    "text": "Inputs App"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#outputs-app",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#outputs-app",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Outputs App",
    "text": "Outputs App\nA {shiny} app that allows the user to view the results of model runs."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#outputs-app-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#outputs-app-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Outputs App",
    "text": "Outputs App"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#questions",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#questions",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Questions?",
    "text": "Questions?\n\nContact The Strategy Unit\n\n\n strategy.unit@nhs.net\n The-Strategy-Unit\n\n\nContact Me\n\n\n thomas.jemmett@nhs.net\n tomjemmett\n\n\n\n\nview slides at https://tinyurl.com/haca23nhp"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#what-is-data-science",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#what-is-data-science",
    "title": "Everything you ever wanted to know about data science",
    "section": "What is data science?",
    "text": "What is data science?\n\n‚ÄúA data scientist knows more about computer science than the average statistician, and more about statistics than the average computer scientist‚Äù"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#drew-conways-famous-venn-diagram",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#drew-conways-famous-venn-diagram",
    "title": "Everything you ever wanted to know about data science",
    "section": "Drew Conway‚Äôs famous Venn diagram",
    "text": "Drew Conway‚Äôs famous Venn diagram\n\nSource"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#around-the-web",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#around-the-web",
    "title": "Everything you ever wanted to know about data science",
    "section": "Around the web‚Ä¶",
    "text": "Around the web‚Ä¶\n\n\n\nThe difference between a statitician and a data scientist? About $30,000\n‚Ä¶ an actual definition of data science. Taking a database and making it do something else. (warning: this quote is me! :wink:)\nStatistics done on a Mac"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#what-are-the-skills-of-data-science",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#what-are-the-skills-of-data-science",
    "title": "Everything you ever wanted to know about data science",
    "section": "What are the skills of data science?",
    "text": "What are the skills of data science?\n\nAnalysis\n\nML\nStats\nData viz\n\nSoftware engineering\n\nProgramming\nSQL/ data\nDevOps\nRAP"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#what-are-the-skills-of-data-science-1",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#what-are-the-skills-of-data-science-1",
    "title": "Everything you ever wanted to know about data science",
    "section": "What are the skills of data science?",
    "text": "What are the skills of data science?\n\nDomain knowledge\n\nCommunication\nProblem formulation\nDashboards and reports"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#ml",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#ml",
    "title": "Everything you ever wanted to know about data science",
    "section": "ML",
    "text": "ML\n\nSource"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#inevitable-xkcd",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#inevitable-xkcd",
    "title": "Everything you ever wanted to know about data science",
    "section": "Inevitable XKCD",
    "text": "Inevitable XKCD\n\n\n\nSource\n\n\nGoogle flu trends"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#stats-and-data-viz",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#stats-and-data-viz",
    "title": "Everything you ever wanted to know about data science",
    "section": "Stats and data viz",
    "text": "Stats and data viz\n\nML leans a bit more towards atheoretical prediction\nStats leans a bit more towards inference (but they both do both)\nData scientists may use different visualisations\n\nInteractive web based tools\nDashboard based visualisers e.g.¬†{stminsights}"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#software-engineering",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#software-engineering",
    "title": "Everything you ever wanted to know about data science",
    "section": "Software engineering",
    "text": "Software engineering\n\nProgramming\n\nNo/ low code data science?\n\nSQL/ data\n\nTend to use reproducible automated processes\n\nDevOps\n\nPlan, code, build, test, release, deploy, operate, monitor\n\nRAP\n\nI will come back to this"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#domain-knowledge",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#domain-knowledge",
    "title": "Everything you ever wanted to know about data science",
    "section": "Domain knowledge",
    "text": "Domain knowledge\n\nDo stuff that matters\n\nThe best minds of my generation are thinking about how to make people click ads. That sucks. Jeffrey Hammerbacher\n\nConvince other people that it matters\nThis is the hardest part of data science\nCommunicate, communicate, communicate!\nMany of you are expert at this"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#reproducibility",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#reproducibility",
    "title": "Everything you ever wanted to know about data science",
    "section": "Reproducibility",
    "text": "Reproducibility\n\nReproducibility in science\nThe $6B spreadsheet error\nGeorge Osbourne‚Äôs austerity was based on a spreadsheet error\nFor us, reproducibility also means we can do the same analysis 50 times in one minute\n\nWhich is why I started down the road of data science"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#what-is-rap",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#what-is-rap",
    "title": "Everything you ever wanted to know about data science",
    "section": "What is RAP",
    "text": "What is RAP\n\na process in which code is used to minimise manual, undocumented steps, and a clear, properly documented process is produced in code which can reliably give the same result from the same dataset\nRAP should be:\n\n\nthe core working practice that must be supported by all platforms and teams; make this a core focus of NHS analyst training\n\n\nGoldacre review"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--baseline",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--baseline",
    "title": "Everything you ever wanted to know about data science",
    "section": "Levels of RAP- Baseline",
    "text": "Levels of RAP- Baseline\n\nData produced by code in an open-source language (e.g., Python, R, SQL).\nCode is version controlled (see Git basics and using Git collaboratively guides).\nRepository includes a README.md file (or equivalent) that clearly details steps a user must follow to reproduce the code\nCode has been peer reviewed.\nCode is published in the open and linked to & from accompanying publication (if relevant).\n\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--silver",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--silver",
    "title": "Everything you ever wanted to know about data science",
    "section": "Levels of RAP- Silver",
    "text": "Levels of RAP- Silver\n\nCode is well-documented‚Ä¶\nCode is well-organised following standard directory format\nReusable functions and/or classes are used where appropriate\nPipeline includes a testing framework\nRepository includes dependency information (e.g.¬†requirements.txt, PipFile, environment.yml\nData is handled and output in a Tidy data format\n\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--gold",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--gold",
    "title": "Everything you ever wanted to know about data science",
    "section": "Levels of RAP- Gold",
    "text": "Levels of RAP- Gold\n\nCode is fully packaged\nRepository automatically runs tests etc. via CI/CD or a different integration/deployment tool e.g.¬†GitHub Actions\nProcess runs based on event-based triggers (e.g., new data in database) or on a schedule\nChanges to the RAP are clearly signposted. E.g. a changelog in the package, releases etc. (See gov.uk info on Semantic Versioning)\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#the-data-science-unicorn",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#the-data-science-unicorn",
    "title": "Everything you ever wanted to know about data science",
    "section": "The data science ‚ÄúUnicorn‚Äù",
    "text": "The data science ‚ÄúUnicorn‚Äù\n\nThe maybe-mythical data science ‚ÄúUnicorn‚Äù has mastered:\n\nDomain knowledge\nStats and ML\nSoftware engineering"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-a-team-sport",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-a-team-sport",
    "title": "Everything you ever wanted to know about data science",
    "section": "Data science is a team sport",
    "text": "Data science is a team sport\n\nIn my extended DS team I have:\nStats and DevOps (and rabble rousing) [this one is me :wink:]\nSQL, data, and training\nDevOps and programming\nText mining, Python, and APIs\nBilingual R/ Python, Shiny dashboards"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-an-mmo",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-an-mmo",
    "title": "Everything you ever wanted to know about data science",
    "section": "Data science is an MMO",
    "text": "Data science is an MMO\n\nData scientists need help with:\n\nStakeholder communication and engagement\nQualitative analysis\nTranslating models and prediction into the real world\nEvidence review and problem definition"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-an-mmo-1",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-an-mmo-1",
    "title": "Everything you ever wanted to know about data science",
    "section": "Data science is an MMO",
    "text": "Data science is an MMO\n\nData scientists are an excellent help when you:\n\nNeed a lot of pretty graphs\nNeed the same analysis done 50+ times with different data\nHave too much text and not enough time to analyse it\nWant to carefully document your analysis and make it reproducible\nHave a hideously messy, large dataset that you can‚Äôt hack together yourself"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#the-team",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#the-team",
    "title": "Everything you ever wanted to know about data science",
    "section": "The team",
    "text": "The team\n\nWe will be organising code review and pair coding sessions\nWe will be running coffee and coding sessions\nWe can be relied on to get very excited about thorny data problems, especially if they involve:\n\nDrawing pretty graphs\nNHS-R and other communities and events\nSpending long hours in a bunker writing open source code\nProcessing text\nDocumenting and version controlling analyses"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#note",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#note",
    "title": "Everything you ever wanted to know about data science",
    "section": "Note",
    "text": "Note\nAll copyrighted material is reused under Fair Dealing\n\n\nview slides at the-strategy-unit.github.io/data_science/presentations"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#health-data-in-the-headlines",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#health-data-in-the-headlines",
    "title": "System Dynamics in health and care",
    "section": "Health Data in the Headlines",
    "text": "Health Data in the Headlines\n\n\n\n\nUsed to seeing headlines that give a snapshot figure but doesn‚Äôt say much about the system.\nNow starting to see headlines that recognise flow through the system rather than snapshot in time of just one part.\nCan get better understanding of the issues in a system if we can map it as stocks and flows, but our datasets not designed to give up this information very readily. This talk is how I have tried to meet that challenge."
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#through-the-system-dynamics-lens",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#through-the-system-dynamics-lens",
    "title": "System Dynamics in health and care",
    "section": "Through the System Dynamics lens",
    "text": "Through the System Dynamics lens\n\nStock-flow model\nDynamic behaviour, feedback loops\n\nIn a few seconds, what is SD?\nAn approach to understanding the behaviour of complex systems over time. A method of mapping a system as stocks, whose levels can only change due to flows in and flows out. Stocks could be people on a waiting list, on a ward, money, ‚Ä¶\nFlows are the rate at which things change in a given time period e.g.¬†admissions per day, referrals per month.\nBehaviour of the system is determined by how the components interact with each other, not what each component does. Mapping the structure of a system like this leads us to identify feedback loops, and consequences of an action - both intended and unintended.\nIn this capacity-constrained model we only need 3 parameters to run the model (exogenous). All the behaviour within the grey box is determined by the interactions of those components (indogenous).\nHow do we get a value/values for referrals per day?\n(currently use specialist software to build and run our models, aim is to get to a point where we can run in open source.)"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#determining-flows",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#determining-flows",
    "title": "System Dynamics in health and care",
    "section": "Determining flows",
    "text": "Determining flows\n\n\n\n\n‚Äòadmissions per day‚Äô is needed to populate the model.\n‚Äòdischarged‚Äô could be used to verify the model against known data\n\nHow many admissions per day (or week, month‚Ä¶)\n\n\n\n\n\n\n\n\n   \n\n\nGoing to use very simple model shown to explain how to extract flow data for admissions. Will start with visual explainer before going into the code.\n1. generate list of key dates (in this case daily, could be weekly, monthly)\n2. take our patient-level ID with admission and discharge dates\n3. count of admissions on that day/week"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#determining-occupancy",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#determining-occupancy",
    "title": "System Dynamics in health and care",
    "section": "Determining occupancy",
    "text": "Determining occupancy\n\n\n\n\n‚Äòon ward‚Äô is used to verify the model against known data\n\nLogic statement testing if the key date is wholly between admission and discharge dates\nflag for a match \n\n\n\n\n\n\n\n     \n\n\nMight also want to generate occupancy, to compare the model output with actual data to verify/validate.\n1. generate list of key dates\n2. take our patient-level ID with admission and discharge dates\n3. going to take each date in our list of keydates, and see if there is an admission before that date and discharge after 4. this creates a wide data frame, the same length as patient data.\n5. once run through all the dates in the list, sum each column\nPatient A admitted on 2nd, so only starts being classed as resident on 3rd."
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---flows",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---flows",
    "title": "System Dynamics in health and care",
    "section": "in R - flows",
    "text": "in R - flows\nEasy to do with count, or group_by and summarise\n\n\n  admit_d &lt;- spell_dates |&gt; \n  group_by(date_admit) |&gt;\n  count(date_admit)\n\nhead(admit_d)\n\n\n# A tibble: 6 √ó 2\n# Groups:   date_admit [6]\n  date_admit     n\n  &lt;date&gt;     &lt;int&gt;\n1 2022-01-01    27\n2 2022-01-02    27\n3 2022-01-03    19\n4 2022-01-04    18\n5 2022-01-05    29\n6 2022-01-06    22"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---occupancy",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---occupancy",
    "title": "System Dynamics in health and care",
    "section": "in R - occupancy",
    "text": "in R - occupancy\nGenerate list of key dates\n\n\n\ndate_start &lt;- dmy(01012022) \ndate_end &lt;- dmy(31012022)\nrun_len &lt;- length(seq(from = date_start, to = date_end, by = \"day\"))\n\nkeydates &lt;- data.frame(\n  date = c(seq(date_start, by = \"day\", length.out=run_len)))  \n\n\n\n\n        date\n1 2022-01-01\n2 2022-01-02\n3 2022-01-03\n4 2022-01-04\n5 2022-01-05\n6 2022-01-06\n\n\n\n\n\nStart by generating the list of keydates. In this example we‚Äôre running the model in days, and checking each day in 2022.\nNeed the run length for the next step, to know how many times to iterate over"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---occupancy-1",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---occupancy-1",
    "title": "System Dynamics in health and care",
    "section": "in R - occupancy",
    "text": "in R - occupancy\nIterate over each date - need to have been admitted before, and discharged after\n\noccupancy_flag &lt;- function(df) {\n\n # pre-allocate tibble size to speed up iteration in loop\n  activity_all &lt;- tibble(nrow = nrow(df))  |&gt;  \n    select()\n \n   for (i in 1:run_len) {\n     \n      activity_period &lt;-  case_when(\n     \n      # creates 1 flag if resident for complete day\n      df$date_admit &lt; keydates$keydate[i] & \n        df$date_discharge &gt; keydates$keydate[i] ~ 1,\n      TRUE ~ 0)\n   \n      # column bind this day's flags to previous\n      activity_all &lt;- bind_cols(activity_all, activity_period)\n \n   }\n  \n    # rename column to match the day being counted\n  activity_all &lt;- activity_all |&gt; \n    setNames(paste0(\"d_\", keydates$date))\n    \n  # bind flags columns to patient data\n  daily_adm &lt;- bind_cols(df, activity_all) |&gt; \n    pivot_longer(\n      cols = starts_with(\"d_\"),\n      names_to = \"date\",\n      values_to = \"count\"\n    ) |&gt; \n    \n    group_by(date) |&gt; \n    summarise(resident = sum(count)) |&gt; \n    ungroup() |&gt; \n  mutate(date = str_remove(date, \"d_\"))\n   \n } \n\n\nIs there a better way than using a for loop?\n\nPre-allocate tibbles\nactivity_all will end up as very wide tibble, with a column for each date in list of keydates.\nFor each date in the list of key dates, compares with admission date & discharge date; need to be admitted before the key date and discharged after the key date. If match, flag = 1.\nCreates a column for each day, then binds this to activity all.\nRename each column with the date it was checking (add a character to start of column name so column doesn‚Äôt start with numeric)\nPivot long, then group by date and sum the flags (other variables could be added here, such as TFC or provider code)"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods---flows",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods---flows",
    "title": "System Dynamics in health and care",
    "section": "Longer Time Periods - flows",
    "text": "Longer Time Periods - flows\nUse lubridate::floor_date to generate the date at start of week/month\n\nadmit_wk &lt;- spell_dates |&gt; \n  mutate(week_start = floor_date(\n    date_admit, unit = \"week\", week_start = 1   # start week on Monday\n  )) |&gt; \n  count(week_start)     # could add other parameters such as provider code, TFC etc\n\nhead(admit_wk)\n\n\n\n# A tibble: 6 √ó 2\n  week_start     n\n  &lt;date&gt;     &lt;int&gt;\n1 2021-12-27    54\n2 2022-01-03   170\n3 2022-01-10   200\n4 2022-01-17   196\n5 2022-01-24   198\n6 2022-01-31   193\n\n\n\nMight run SD model in weeks or months - e.g.¬†months for care homes Use lubridate to create new variable with start date of week/month/year etc"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods---occupancy",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods---occupancy",
    "title": "System Dynamics in health and care",
    "section": "Longer Time Periods - occupancy",
    "text": "Longer Time Periods - occupancy\nKey dates to include the dates at the start and end of each time period\n\n\n\ndate_start &lt;- dmy(03012022) # first Monday of the year\ndate_end &lt;- dmy(01012023)\nrun_len &lt;- length(seq(from = date_start, to = date_end, by = \"week\"))\n\nkeydates &lt;- data.frame(wk_start = c(seq(date_start, \n                                        by = \"week\", \n                                        length.out=run_len))) |&gt;  \n  mutate(\n    wk_end = wk_start + 6)    # last date in time period\n\n\n\n\n    wk_start     wk_end\n1 2022-01-03 2022-01-09\n2 2022-01-10 2022-01-16\n3 2022-01-17 2022-01-23\n4 2022-01-24 2022-01-30\n5 2022-01-31 2022-02-06\n6 2022-02-07 2022-02-13\n\n\n\n\n\nModel might make more sense to run in weeks or months (e.g.¬†care home), so list of keydates need a start date and end date for each time period."
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods",
    "title": "System Dynamics in health and care",
    "section": "Longer Time Periods",
    "text": "Longer Time Periods\nMore logic required if working in weeks or months - can only be in one place at any given time\n\n# flag for occupancy\nactivity_period &lt;-  case_when(\n  \n        # creates 1 flag if resident for complete week\n      df$date_admit &lt; keydates$wk_start[i] & df$date_discharge &gt; keydates$wk_end[i] ~ 1,\n        TRUE ~ 0)\n\n\nAnd a little bit more logic\nOccupancy requires the patient to have been admitted before the start of the week/month, and discharged after the end of the week/month"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#applying-the-data",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#applying-the-data",
    "title": "System Dynamics in health and care",
    "section": "Applying the data",
    "text": "Applying the data\n\n\nHow to apply this wrangling of data to the system dynamic model?\nAdmissions data used as an input to the flow - could be reduced to a single figure (average), or there may be variation by season/day of week etc.\nOccupancy (and discharges) used to verify the model output against known data."
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#next-steps",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#next-steps",
    "title": "System Dynamics in health and care",
    "section": "Next Steps",
    "text": "Next Steps\n\nGeneralise function to a state where it can be used by others - onto Github\nTurn this into a package\nOpen-source SD models and interfaces - R Shiny or Python"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#questions-comments-suggestions",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#questions-comments-suggestions",
    "title": "System Dynamics in health and care",
    "section": "Questions, comments, suggestions?",
    "text": "Questions, comments, suggestions?\n\n\n\nPlease get in touch!\n\nSally.Thompson37@nhs.net\n\n\nNHS-R conference 2023"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#what-is-targets",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#what-is-targets",
    "title": "Coffee and Coding",
    "section": "What is {targets}?",
    "text": "What is {targets}?\n\nThe targets package is a Make-like pipeline tool for Statistics and data science in R. With targets, you can maintain a reproducible workflow without repeating yourself. targets learns how your pipeline fits together, skips costly runtime for tasks that are already up to date, runs only the necessary computation, supports implicit parallel computing, abstracts files as R objects, and shows tangible evidence that the results match the underlying code and data.\n\n\n\nData analysis can be slow. A round of scientific computation can take several minutes, hours, or even days to complete. After it finishes, if you update your code or data, your hard-earned results may no longer be valid. Unchecked, this invalidation creates chronic Sisyphean loop:\n\n\nLaunch the code.\nWait while it runs.\nDiscover an issue.\nRestart from scratch.\n\n\nsource: The {targets} R package user manual"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#what-is-it-actually-trying-to-do",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#what-is-it-actually-trying-to-do",
    "title": "Coffee and Coding",
    "section": "What is it actually trying to do?",
    "text": "What is it actually trying to do?\n\n\nYour analysis is built up of a number of steps that build one on top of another\nbut these steps need to run in a particular order\nsome of these steps may take a long time to run\nso you only want to run the steps that have changed"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#typical-solution",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#typical-solution",
    "title": "Coffee and Coding",
    "section": "Typical solution",
    "text": "Typical solution\n\n\nSteps\nYou have a folder with numbered scripts, such as:\n\n1. get data.R\n2. process data.R\n3. produce charts.R\n4. build model.R\n5. report.qmd\n\n\nDownsides\n\n\nit‚Äôs easy to accidentally skip a step: what happens if you went from 1 to 3?\nperforming one of the steps may take a long time, so you may want to skip it if it‚Äôs already been run‚Ä¶ but how do you know that it‚Äôs already been run?\nperhaps step 4 doesn‚Äôt depend on step 3, but is this obvious that you could skip step 4 if step 3 is updated?\nwhat if someone labels the files terribly, or doesn‚Äôt number them at all?\nwhat if the numbers become out of date and are in the wrong order?\ndo you need to create a procedure document that describes what to do, step-by-step?"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#targets-to-the-rescue",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#targets-to-the-rescue",
    "title": "Coffee and Coding",
    "section": "{targets} to the rescue?",
    "text": "{targets} to the rescue?\n\n\nUsing the previous example, if we were to create functions for each of the steps (all saved in the folder R/), we can start using targets using the function use_targets() which will create a file called _targets.R.\nWe can then modify the file to match our pipeline, for example:\nNote that:\n\nprocessed_data depends upon raw_data,\nchart and model depend upon processed_data,\nreport depends upon chart and model.\n\nWe can visualise our pipeline using tar_visnetwork().\n\n\nlibrary(targets)\n\ntar_option_set(\n  packages = c(\"tibble\", \"dplyr\", \"ggplot2\"),\n)\n\ntar_source()\n\nlist(\n  tar_target(raw_data, get_data()),\n  tar_target(processed_data, process_data(raw_data)),\n  tar_target(chart, produce_chart(processed_data)),\n  tar_target(model, build_model(processed_data)),\n  tar_target(report, generate_report(chart, model))\n)"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#running-the-pipeline",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#running-the-pipeline",
    "title": "Coffee and Coding",
    "section": "Running the pipeline",
    "text": "Running the pipeline\nRunning this pipeline is as simple as: tar_make().\nThis will output the following:\n‚Ä¢ start target raw_data\n‚Ä¢ built target raw_data [1.05 seconds]\n‚Ä¢ start target processed_data\n‚Ä¢ built target processed_data [0.03 seconds]\n‚Ä¢ start target chart\n‚Ä¢ built target chart [0.02 seconds]\n‚Ä¢ start target model\n‚Ä¢ built target model [0.01 seconds]\n‚Ä¢ start target report\n‚Ä¢ built target report [0 seconds]\n‚Ä¢ end pipeline [1.75 seconds]\n\nRunning tar_make() again will show these step‚Äôs being skipped:\n‚úî skip target raw_data\n‚úî skip target processed_data\n‚úî skip target chart\n‚úî skip target model\n‚úî skip target report\n‚úî skip pipeline [0.12 seconds]"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#changing-one-of-the-files",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#changing-one-of-the-files",
    "title": "Coffee and Coding",
    "section": "Changing one of the files",
    "text": "Changing one of the files\nIf we change produce_chart.R slightly, this will cause chart and report to be invalidated, but it will skip over the other steps.\n\n\n&gt; targets::tar_make()\n\n‚úî skip target raw_data\n‚úî skip target processed_data\n‚Ä¢ start target chart\n‚Ä¢ built target chart [0.03 seconds]\n‚úî skip target model\n‚Ä¢ start target report\n‚Ä¢ built target report [0 seconds]\n‚Ä¢ end pipeline [1.71 seconds]"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#using-the-results-of-our-pipeline",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#using-the-results-of-our-pipeline",
    "title": "Coffee and Coding",
    "section": "Using the results of our pipeline",
    "text": "Using the results of our pipeline\nWe can view the results of any step using tar_read() and tar_load(). These will either directly give you the results of a step, or load that step into your environment (as a variable with the same name as the step).\nThis allows us to view intermediate steps as well as the final outputs of our pipelines.\nOne thing you may want to consider doing is as a final step in a pipeline is to generate a quarto document, or save call a function like saveRDS to generate more useful outputs."
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#current-examples-of-targets-in-action",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#current-examples-of-targets-in-action",
    "title": "Coffee and Coding",
    "section": "Current examples of {targets} in action",
    "text": "Current examples of {targets} in action\n\ncode used in this presentation\nNHP Inputs (all of the data processing steps are a targets pipeline)\nNHP Strategies (runs Sql scripts to update tables in the data warehouse)\nNHP Model (all of the data extraction, processing, and uploading for the model is a targets pipeline)\nMacmillan on NCDR - Jacqueline has been using {targets} for her current project\n\nThe {targets} documentation is exceptionally detailed and easy to follow, and goes into more complex examples (such as dynamic branching of steps in a pipeline and high performance computing setups)\n\n\nview slides at the-strategy-unit.github.io/data_science/presentations"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#patient-experience",
    "href": "presentations/2023-05-15_text-mining/index.html#patient-experience",
    "title": "Text mining of patient experience data",
    "section": "Patient experience",
    "text": "Patient experience\n\nThe NHS collects a lot of patient experience data\nRate the service 1-5 (Very poor‚Ä¶ Excellent) but also give written feedback\n\n‚ÄúParking was difficult‚Äù\n‚ÄúDoctor was rude‚Äù\n‚ÄúYou saved my life‚Äù\n\nMany organisations lack the staffing to read all of the feedback in a systematic way"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#text-mining",
    "href": "presentations/2023-05-15_text-mining/index.html#text-mining",
    "title": "Text mining of patient experience data",
    "section": "Text mining",
    "text": "Text mining\n\nWe have built an algorithm to read it\n\nTheme\n‚ÄúCriticality‚Äù\n\nFits alongside other work happening within NHSE\n\nA framework for understanding patient experience"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#patient-experience-101",
    "href": "presentations/2023-05-15_text-mining/index.html#patient-experience-101",
    "title": "Text mining of patient experience data",
    "section": "Patient experience 101",
    "text": "Patient experience 101\n\nTick box scoring is not useful (or accurate)\nText based data is complex and built on human experience\nWe‚Äôre not making word clouds!\nWe‚Äôre not classifying movie reviews or Reddit posts\nThe tool should enhance, not replace, human understanding\n‚ÄúA recommendation engine for feedback data‚Äù"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#everything-open-all-the-time",
    "href": "presentations/2023-05-15_text-mining/index.html#everything-open-all-the-time",
    "title": "Text mining of patient experience data",
    "section": "Everything open, all the time",
    "text": "Everything open, all the time\n\nThis project was coded in the open and is MIT licensed\nEngage with the organisations as we find them\n\nDo they want code or a docker image?\nDo they want to fetch their own themes from an API?\nDo they want to use our dashboard?"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#phase-1",
    "href": "presentations/2023-05-15_text-mining/index.html#phase-1",
    "title": "Text mining of patient experience data",
    "section": "Phase 1",
    "text": "Phase 1\n\n10 categories and moderate performance on criticality analysis\nscikit-learn\nShiny\nReticulate\nR package of Python code"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#golem-all-the-things",
    "href": "presentations/2023-05-15_text-mining/index.html#golem-all-the-things",
    "title": "Text mining of patient experience data",
    "section": "Golem all the things!",
    "text": "Golem all the things!\n\nOpinionated way of building Shiny\nAllows flexibility in deployed versions using YAML\nAgnostic to deployment\nEmphasises dependency management and testing\nSeparate ‚Äúreactive‚Äù and ‚Äúbusiness‚Äù logic (see the accompanying book)"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#phase-2",
    "href": "presentations/2023-05-15_text-mining/index.html#phase-2",
    "title": "Text mining of patient experience data",
    "section": "Phase 2",
    "text": "Phase 2\n\n30-50 categories and excellent criticality performance\nscikit-learn/ BERT\nMore Shiny\nSeparate the code bases\nFastAPI\nInspired by the Royal College of Paediatrics and Child Health API\nDocumentation, documentation, documentation"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#making-it-useful",
    "href": "presentations/2023-05-15_text-mining/index.html#making-it-useful",
    "title": "Text mining of patient experience data",
    "section": "Making it useful",
    "text": "Making it useful\n\nAccurately rating low frequency categories\nPer category precision and recall\nSpeed versus accuracy\nRepresenting the thematic structure"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#the-future",
    "href": "presentations/2023-05-15_text-mining/index.html#the-future",
    "title": "Text mining of patient experience data",
    "section": "The future",
    "text": "The future\n\nOff the shelf, proprietary data collection systems dominate\nThey often offer bundled analytic products of low quality\nThe DS time can‚Äôt and doesn‚Äôt want to offer a complete data system\nHow can we best contribute to improving patient experience for patients in the NHS?\n\nIf the patient experience data won‚Äôt come to the mountain‚Ä¶"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#open-source-ftw",
    "href": "presentations/2023-05-15_text-mining/index.html#open-source-ftw",
    "title": "Text mining of patient experience data",
    "section": "Open source FTW!",
    "text": "Open source FTW!\n\nOften individuals in the NHS don‚Äôt want private companies to ‚Äúbenefit‚Äù from open code\nBut if they make their products better with open code the patients win\nBest practice as code"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#the-projects",
    "href": "presentations/2023-05-15_text-mining/index.html#the-projects",
    "title": "Text mining of patient experience data",
    "section": "The projects",
    "text": "The projects\n\nhttps://github.com/CDU-data-science-team/pxtextmining\nhttps://github.com/CDU-data-science-team/experiencesdashboard\nhttps://github.com/CDU-data-science-team/PatientExperience-QDC"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#the-team",
    "href": "presentations/2023-05-15_text-mining/index.html#the-team",
    "title": "Text mining of patient experience data",
    "section": "The team",
    "text": "The team\n\nYiWen Hon (Python & Machine learning)\nOluwasegun Apejoye (Shiny)\n\nContact:\n\nchris.beeley1@nhs.net\nhttps://fosstodon.org/@chrisbeeley\n\n\n\nview slides at the-strategy-unit.github.io/data_science/presentations"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#introduction",
    "href": "presentations/2023-03-23_collaborative-working/index.html#introduction",
    "title": "Collaborative working",
    "section": "Introduction",
    "text": "Introduction\n\nThis is definitely an art and not a science\nI do not claim to have all, or even most of, the answers\nHow you use these tools is way more important than the tools themselves\nThis is a culture and not a technique"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#costs",
    "href": "presentations/2023-03-23_collaborative-working/index.html#costs",
    "title": "Collaborative working",
    "section": "Costs",
    "text": "Costs\n\nDelay and time\nStress and disagreement\nCommittee thinking\nLearning and effort"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#benefits",
    "href": "presentations/2023-03-23_collaborative-working/index.html#benefits",
    "title": "Collaborative working",
    "section": "Benefits",
    "text": "Benefits\n\n‚ÄúFrom each according to their ability‚Äù\nLearning\nReproducibility and reduced truck factor\nFun!"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#github-as-an-organising-principle-behind-work",
    "href": "presentations/2023-03-23_collaborative-working/index.html#github-as-an-organising-principle-behind-work",
    "title": "Collaborative working",
    "section": "GitHub as an organising principle behind work",
    "text": "GitHub as an organising principle behind work\n\nA project is just a set of milestones\nA milestone is just a set of issues\nAn issue is just a set of commits\nA commit is just text added and removed"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#the-repo-owner",
    "href": "presentations/2023-03-23_collaborative-working/index.html#the-repo-owner",
    "title": "Collaborative working",
    "section": "The repo owner",
    "text": "The repo owner\n\nReview milestones\nReview issues\n\nDiscuss the issue on the issue- NOT on email!\n\nReview pull requests and get your pull requests reviewed!"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#asynchronous-communication",
    "href": "presentations/2023-03-23_collaborative-working/index.html#asynchronous-communication",
    "title": "Collaborative working",
    "section": "Asynchronous communication",
    "text": "Asynchronous communication\n\nInvolve others before you pull request\nInvolve others when you pull request\nRead issues!\nComment on issues!\nFile issues- suggestions/ bug reports/ questions\n\nNOT in emails"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#asynchronous-work",
    "href": "presentations/2023-03-23_collaborative-working/index.html#asynchronous-work",
    "title": "Collaborative working",
    "section": "Asynchronous work",
    "text": "Asynchronous work\n\nEvery piece of work has an issues associated with it\nEvery piece of work associated with an issue lives on its own branch\nEvery branch is incorporated to the main repo by a pull request\nEvery pull request is reviewed"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#iteration-and-documentation",
    "href": "presentations/2023-03-23_collaborative-working/index.html#iteration-and-documentation",
    "title": "Collaborative working",
    "section": "Iteration and documentation",
    "text": "Iteration and documentation\n\nAnalyse early, analyse often (using RAPs!)\nWrite down what you did\nWrite down what you did but then changed your mind about\nFavour Quarto/ RMarkdown\n\nClean sessions\nDocumentation and graphics"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#data-and-.gitignore",
    "href": "presentations/2023-03-23_collaborative-working/index.html#data-and-.gitignore",
    "title": "Collaborative working",
    "section": "Data and .gitignore",
    "text": "Data and .gitignore\n\nYour repo needs to be reproducible but also needs to be safe\nThe main branch should be reproducible by anyone at any time\n\nDocument package dependencies (using renv)\nDocument data loads if the data isn‚Äôt in the repo\n\n\n\n\nview slides at the-strategy-unit.github.io/data_science/presentations"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The Data Science team at the Strategy Unit comprises the following team members:\n\nChris Beeley\nMatt Dray\nOzayr Mohammed\nTom Jemmett\nYiWen Hon\n\nCurrent and previous projects of note include:\n\nWork supporting the New Hospitals Programme, including building a model for predicting the demand and capacity requirements of hospitals in the future, and a tool for mapping the evidence on this topic.\nThe Patient Experience Qualitative Data Categorisation project\nWork supporting the wider analytical community, through events/communities such as NHS-R and HACA."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science @ The Strategy Unit",
    "section": "",
    "text": "This is the home of Data Science activities at The Strategy Unit.\nHere, we host information about how we work, links to presentations, and blogposts relating to how we utilise data science tools.\nAll members of the Strategy Unit are welcome to contribute."
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#patient-experience",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#patient-experience",
    "title": "What good data science looks like",
    "section": "Patient experience",
    "text": "Patient experience\n\nThe NHS collects a lot of patient experience data\nRate the service 1-5 (Very poor‚Ä¶ Excellent) but also give written feedback\n\n‚ÄúParking was difficult‚Äù\n‚ÄúDoctor was rude‚Äù\n‚ÄúYou saved my life‚Äù\n\nMany organisations lack the staffing to read all of the feedback in a systematic way\nProduce an algorithm to rate theme and ‚Äúcriticality‚Äù"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#help-people-to-do-their-jobs",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#help-people-to-do-their-jobs",
    "title": "What good data science looks like",
    "section": "Help people to do their jobs",
    "text": "Help people to do their jobs\n\nText based data is complex and built on human experience\nThe tool should enhance, not replace, human understanding\nEnhancing search and filtering\n\nIf they read 100 comments today, which should they read?\n\n‚ÄúA recommendation engine for feedback data‚Äù"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#reflect-what-users-want",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#reflect-what-users-want",
    "title": "What good data science looks like",
    "section": "Reflect what users want",
    "text": "Reflect what users want\n\nI have worked with this data since before it existed\nI came to realise that people were struggling to read all of their data\nFits alongside other work happening within NHSE\n\nA framework for understanding patient experience"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#useful",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#useful",
    "title": "What good data science looks like",
    "section": "Useful",
    "text": "Useful\n\nA fundamental principle is that everyone can use\nIf you can run the code, run it\nIf you can use the API, use it\nIf you just want the dashboard, use it\nCredit to the growth charts API"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#understandable",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#understandable",
    "title": "What good data science looks like",
    "section": "Understandable",
    "text": "Understandable\n\nTuned to the users needs\nNot simply tuning accuracy scores\nLook at the type of mistake the model is making\nLook at the category it‚Äôs predicting\n\nWe can lose a few of common unimportant categories\nWe need to get every rare and important category"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#iterative",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#iterative",
    "title": "What good data science looks like",
    "section": "Iterative",
    "text": "Iterative\n\nYear one\n\n10 categories\nModerate criticality performance\nNo deep learning\nWeak dashboard\nPositive evaluation"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#iterative-1",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#iterative-1",
    "title": "What good data science looks like",
    "section": "Iterative",
    "text": "Iterative\n\nYear two\n\n30-50 categories\nStrong criticality performance\nDeep learning\nImproved dashboard\nWIP\n\nOverall five minor versions of algorithm and seven of dashboard"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#documented",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#documented",
    "title": "What good data science looks like",
    "section": "Documented",
    "text": "Documented\n\nWe‚Äôve documented in the way you usually would\nWe were asked in year 1 to provide plain English documentation\nWe made a website with all the product details"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#develop-skills-of-the-staff-technical-and-otherwise",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#develop-skills-of-the-staff-technical-and-otherwise",
    "title": "What good data science looks like",
    "section": "Develop skills of the staff, technical and otherwise",
    "text": "Develop skills of the staff, technical and otherwise\n\nYear one created a Python programmer\nYear two created an R/ Shiny programmer\nThe team has learned:\n\nStatic website generation\nText cleaning/ searching/ mining\nCollaborative coding practices\nWorking with and communicating with users\nLinux, databases, APIs‚Ä¶"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#benefits-from-and-benefits-the-community",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#benefits-from-and-benefits-the-community",
    "title": "What good data science looks like",
    "section": "Benefits from, and benefits, the community",
    "text": "Benefits from, and benefits, the community\n\nNHSBSA R Shiny template"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#benefits-from-and-benefits-the-community-1",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#benefits-from-and-benefits-the-community-1",
    "title": "What good data science looks like",
    "section": "Benefits from, and benefits, the community",
    "text": "Benefits from, and benefits, the community\n\nWe benefit and benefit from\n\nNHS-R\nNHS-Pycom\nGovernment Digital Service\nColleagues and friends"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#open-and-reproducible",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#open-and-reproducible",
    "title": "What good data science looks like",
    "section": "Open and reproducible",
    "text": "Open and reproducible\n\nOff the shelf, proprietary data collection systems dominate\nThey often offer bundled analytic products of low quality\nThe DS time can‚Äôt and doesn‚Äôt want to offer a complete data system\nHow can we best contribute to improving patient experience for patients in the NHS?\n\nIf the patient experience data won‚Äôt come to the mountain‚Ä¶"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#open-source-ftw",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#open-source-ftw",
    "title": "What good data science looks like",
    "section": "Open source FTW!",
    "text": "Open source FTW!\n\nOften individuals in the NHS don‚Äôt want private companies to ‚Äúbenefit‚Äù from open code\nBut if they make their products better with open code the patients win\nBest practice as code"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#fun",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#fun",
    "title": "What good data science looks like",
    "section": "Fun!",
    "text": "Fun!\n\nCombing through spreadsheets looking for one comment is not fun\nDoing things the same way you did them last year is not fun\nTrying to implement a project that is too complicated is not fun\n\n¬†\n\nWorking with a diverse team with different skills is fun\nAccessing high quality documentation to understand a project better is fun*"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#team-and-code",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#team-and-code",
    "title": "What good data science looks like",
    "section": "Team and code",
    "text": "Team and code\n\nAndreas Soteriades (Y1)\nYiWen Hon, Oluwasegun Apejoye (Y2)\n\n¬†\n\npxtextmining\nexperiencesdashboard\nDocumentation\n\n\n\nchris.beeley1@nhs.net\nhttps://fosstodon.org/@chrisbeeley\n\n\n\nview slides at the-strategy-unit.github.io/data_science/presentations"
  },
  {
    "objectID": "presentations/index.html",
    "href": "presentations/index.html",
    "title": "Presentations",
    "section": "",
    "text": "title\n      author\n      date\n    \n  \n  \n    Everything you ever wanted to know about data science: but were too afraid to ask\nChris Beeley\n2023-08-02\n    Coffee and coding: Intro session\nChris Beeley\n2023-02-23\n    Coffee and Coding: Good Coding Practices\nTom Jemmett\n2023-03-09\n    RAP: what is it and how can my team start using it effectively?\nChris Beeley\n2023-03-09\n    Coffee and Coding: {targets}\nTom Jemmett\n2023-03-23\n    Collaborative working\nChris Beeley\n2023-03-23\n    Text mining of patient experience data\nChris Beeley\n2023-05-15\n    What good data science looks like\nChris Beeley\n2023-05-23\n    An Introduction to the New Hospital Programme Demand Model: HACA 2023\nTom Jemmett\n2023-07-11\n    Travels with R and Python: the power of data science in healthcare\nChris Beeley\n2023-08-02\n    Unit testing in R: NHS-R Community Webinar\nTom Jemmett\n2023-08-23\n    Coffee and Coding: Working with Geospatial Data in R\nTom Jemmett\n2023-08-24\n    System Dynamics in health and care: fitting square data into round models\nSally Thompson\n2023-10-09\n    Conference Check-in App: NHS-R/NHS.pycom 2023\nTom Jemmett\n2023-10-17"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#what-is-testing",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#what-is-testing",
    "title": "Unit testing in R",
    "section": "What is testing?",
    "text": "What is testing?\n\nSoftware testing is the act of examining the artifacts and the behavior of the software under test by validation and verification. Software testing can also provide an objective, independent view of the software to allow the business to appreciate and understand the risks of software implementation\nwikipedia"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-can-we-test-our-code",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-can-we-test-our-code",
    "title": "Unit testing in R",
    "section": "How can we test our code?",
    "text": "How can we test our code?\n\n\nStatically\n\n\n(without executing the code)\nhappens constantly, as we are writing code\nvia code reviews\ncompilers/interpreters/linters statically analyse the code for syntax errors\n\n\n\n\n\nDynamically"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-can-we-test-our-code-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-can-we-test-our-code-1",
    "title": "Unit testing in R",
    "section": "How can we test our code?",
    "text": "How can we test our code?\n\n\nStatically\n\n(without executing the code)\nhappens constantly, as we are writing code\nvia code reviews\ncompilers/interpreters/linters statically analyse the code for syntax errors\n\n\n\n\nDynamically\n\n\n(by executing the code)\nsplit into functional and non-functional testing\ntesting can be manual, or automated\n\n\n\n\n\n\nnon-functional testing covers things like performance, security, and usability testing"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests",
    "title": "Unit testing in R",
    "section": "Different types of functional tests",
    "text": "Different types of functional tests\nUnit Testing checks each component (or unit) for accuracy independently of one another.\n\nIntegration Testing integrates units to ensure that the code works together.\n\n\nEnd-to-End Testing (e2e) makes sure that the entire system functions correctly.\n\n\nUser Acceptance Testing (UAT) ensures that the product meets the real user‚Äôs requirements."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests-1",
    "title": "Unit testing in R",
    "section": "Different types of functional tests",
    "text": "Different types of functional tests\nUnit Testing checks each component (or unit) for accuracy independently of one another.\nIntegration Testing integrates units to ensure that the code works together.\nEnd-to-End Testing (e2e) makes sure that the entire system functions correctly.\n\nUser Acceptance Testing (UAT) ensures that the product meets the real user‚Äôs requirements.\n\n\nUnit, Integration, and E2E testing are all things we can automate in code, whereas UAT testing is going to be manual"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests-2",
    "title": "Unit testing in R",
    "section": "Different types of functional tests",
    "text": "Different types of functional tests\nUnit Testing checks each component (or unit) for accuracy independently of one another.\n\nIntegration Testing integrates units to ensure that the code works together.\nEnd-to-End Testing (e2e) makes sure that the entire system functions correctly.\nUser Acceptance Testing (UAT) ensures that the product meets the real user‚Äôs requirements.\n\n\nOnly focussing on unit testing in this talk, but the techniques/packages could be extended to integration testing. Often other tools (potentially specific tools) are needed for E2E testing."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#example",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#example",
    "title": "Unit testing in R",
    "section": "Example",
    "text": "Example\nWe have a {shiny} app which grabs some data from a database, manipulates the data, and generates a plot.\n\n\nwe would write unit tests to check the data manipulation and plot functions work correctly (with pre-created sample/simple datasets)\nwe would write integration tests to check that the data manipulation function works with the plot function (with similar data to what we used for the unit tests)\nwe would write e2e tests to ensure that from start to finish the app grabs the data and produces a plot as required\n\n\n\nsimple (unit tests) to complex (e2e tests)"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-pyramid",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-pyramid",
    "title": "Unit testing in R",
    "section": "Testing Pyramid",
    "text": "Testing Pyramid\n\n\nImage source: The Testing Pyramid: Simplified for One and All headspin.io"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function",
    "title": "Unit testing in R",
    "section": "Let‚Äôs create a simple function‚Ä¶",
    "text": "Let‚Äôs create a simple function‚Ä¶\n\nmy_function &lt;- function(x, y) {\n  \n  stopifnot(\n    \"x must be numeric\" = is.numeric(x),\n    \"y must be numeric\" = is.numeric(y),\n    \"x must be same length as y\" = length(x) == length(y),\n    \"cannot divide by zero!\" = y != 0\n  )\n\n  x / y\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function-1",
    "title": "Unit testing in R",
    "section": "Let‚Äôs create a simple function‚Ä¶",
    "text": "Let‚Äôs create a simple function‚Ä¶\n\nmy_function &lt;- function(x, y) {\n  \n  stopifnot(\n    \"x must be numeric\" = is.numeric(x),\n    \"y must be numeric\" = is.numeric(y),\n    \"x must be same length as y\" = length(x) == length(y),\n    \"cannot divide by zero!\" = y != 0\n  )\n\n  x / y\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function-2",
    "title": "Unit testing in R",
    "section": "Let‚Äôs create a simple function‚Ä¶",
    "text": "Let‚Äôs create a simple function‚Ä¶\n\nmy_function &lt;- function(x, y) {\n  \n  stopifnot(\n    \"x must be numeric\" = is.numeric(x),\n    \"y must be numeric\" = is.numeric(y),\n    \"x must be same length as y\" = length(x) == length(y),\n    \"cannot divide by zero!\" = y != 0\n  )\n\n  x / y\n}\n\n\nThe Ten Rules of Defensive Programming in R"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test",
    "title": "Unit testing in R",
    "section": "‚Ä¶ and create our first test",
    "text": "‚Ä¶ and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-1",
    "title": "Unit testing in R",
    "section": "‚Ä¶ and create our first test",
    "text": "‚Ä¶ and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-2",
    "title": "Unit testing in R",
    "section": "‚Ä¶ and create our first test",
    "text": "‚Ä¶ and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-3",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-3",
    "title": "Unit testing in R",
    "section": "‚Ä¶ and create our first test",
    "text": "‚Ä¶ and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-4",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-4",
    "title": "Unit testing in R",
    "section": "‚Ä¶ and create our first test",
    "text": "‚Ä¶ and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-5",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-5",
    "title": "Unit testing in R",
    "section": "‚Ä¶ and create our first test",
    "text": "‚Ä¶ and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})\n\nTest passed ü•á"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#other-expect_-functions",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#other-expect_-functions",
    "title": "Unit testing in R",
    "section": "other expect_*() functions‚Ä¶",
    "text": "other expect_*() functions‚Ä¶\n\ntest_that(\"my_function correctly divides values\", {\n  expect_lt(\n    my_function(4, 2),\n    10\n  )\n  expect_gt(\n    my_function(1, 4),\n    0.2\n  )\n  expect_length(\n    my_function(c(4, 1), c(2, 4)),\n    2\n  )\n})\n\nTest passed üò∏\n\n\n\n{testthat} documentation"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert",
    "title": "Unit testing in R",
    "section": "Arrange, Act, Assert",
    "text": "Arrange, Act, Assert\n\n\n\n\n\ntest_that(\"my_function works\", {\n  # arrange\n  #  \n  #\n  #\n\n  # act\n  #\n\n  # assert\n  #\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-1",
    "title": "Unit testing in R",
    "section": "Arrange, Act, Assert",
    "text": "Arrange, Act, Assert\n\n\nwe arrange the environment, before running the function\n\n\nto create sample values\ncreate fake/temporary files\nset random seed\nset R options/environment variables\n\n\n\n\ntest_that(\"my_function works\", {\n  # arrange\n  x &lt;- 5\n  y &lt;- 7\n  expected &lt;- 0.714285\n\n  # act\n  #\n\n  # assert\n  #\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-2",
    "title": "Unit testing in R",
    "section": "Arrange, Act, Assert",
    "text": "Arrange, Act, Assert\n\n\nwe arrange the environment, before running the function\nwe act by calling the function\n\n\ntest_that(\"my_function works\", {\n  # arrange\n  x &lt;- 5\n  y &lt;- 7\n  expected &lt;- 0.714285\n\n  # act\n  actual &lt;- my_function(x, y)\n\n  # assert\n  #\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-3",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-3",
    "title": "Unit testing in R",
    "section": "Arrange, Act, Assert",
    "text": "Arrange, Act, Assert\n\n\nwe arrange the environment, before running the function\nwe act by calling the function\nwe assert that the actual results match our expected results\n\n\ntest_that(\"my_function works\", {\n  # arrange\n  x &lt;- 5\n  y &lt;- 7\n  expected &lt;- 0.714285\n\n  # act\n  actual &lt;- my_function(x, y)\n\n  # assert\n  expect_equal(actual, expected)\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#our-test-failed",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#our-test-failed",
    "title": "Unit testing in R",
    "section": "Our test failed!?! üò¢",
    "text": "Our test failed!?! üò¢\n\ntest_that(\"my_function works\", {\n  # arrange\n  x &lt;- 5\n  y &lt;- 7\n  expected &lt;- 0.714285\n\n  # act\n  actual &lt;- my_function(x, y)\n\n  # assert\n  expect_equal(actual, expected)\n})\n\n‚îÄ‚îÄ Failure: my_function works ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n`actual` not equal to `expected`.\n1/1 mismatches\n[1] 0.714 - 0.714 == 7.14e-07\n\n\nError:\n! Test failed"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#tolerance-to-the-rescue",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#tolerance-to-the-rescue",
    "title": "Unit testing in R",
    "section": "Tolerance to the rescue üôÇ",
    "text": "Tolerance to the rescue üôÇ\n\ntest_that(\"my_function works\", {\n  # arrange\n  x &lt;- 5\n  y &lt;- 7\n  expected &lt;- 0.714285\n\n  # act\n  actual &lt;- my_function(x, y)\n\n  # assert\n  expect_equal(actual, expected, tolerance = 1e-6)\n})\n\nTest passed üéä\n\n\n\n(this is a slightly artificial example, usually the default tolerance is good enough)"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-edge-cases",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-edge-cases",
    "title": "Unit testing in R",
    "section": "Testing edge cases",
    "text": "Testing edge cases\n\n\nRemember the validation steps we built into our function to handle edge cases?\n\nLet‚Äôs write tests for these edge cases:\nwe expect errors\n\n\ntest_that(\"my_function works\", {\n  expect_error(my_function(5, 0))\n  expect_error(my_function(\"a\", 3))\n  expect_error(my_function(3, \"a\"))\n  expect_error(my_function(1:2, 4))\n})\n\nTest passed üåà"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#another-simple-example",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#another-simple-example",
    "title": "Unit testing in R",
    "section": "Another (simple) example",
    "text": "Another (simple) example\n\n\n\nmy_new_function &lt;- function(x, y) {\n  if (x &gt; y) {\n    \"x\"\n  } else {\n    \"y\"\n  }\n}\n\n\nConsider this function - there is branched logic, so we need to carefully design tests to validate the logic works as intended."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#another-simple-example-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#another-simple-example-1",
    "title": "Unit testing in R",
    "section": "Another (simple) example",
    "text": "Another (simple) example\n\nmy_new_function &lt;- function(x, y) {\n  if (x &gt; y) {\n    \"x\"\n  } else {\n    \"y\"\n  }\n}\n\n\n\ntest_that(\"it returns 'x' if x is bigger than y\", {\n  expect_equal(my_new_function(4, 3), \"x\")\n})\n\nTest passed ü•á\n\ntest_that(\"it returns 'y' if y is bigger than x\", {\n  expect_equal(my_new_function(3, 4), \"y\")\n  expect_equal(my_new_function(3, 3), \"y\")\n})\n\nTest passed ü•á"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-to-design-good-tests",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-to-design-good-tests",
    "title": "Unit testing in R",
    "section": "How to design good tests",
    "text": "How to design good tests\na non-exhaustive list\n\nconsider all the functions arguments,\nwhat are the expected values for these arguments?\nwhat are unexpected values, and are they handled?\nare there edge cases that need to be handled?\nhave you covered all of the different paths in your code?\nhave you managed to create tests that check the range of results you expect?"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#but-why-create-tests",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#but-why-create-tests",
    "title": "Unit testing in R",
    "section": "But, why create tests?",
    "text": "But, why create tests?\nanother non-exhaustive list\n\ngood tests will help you uncover existing issues in your code\nwill defend you from future changes that break existing functionality\nwill alert you to changes in dependencies that may have changed the functionality of your code\ncan act as documentation for other developers"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-complex-functions",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-complex-functions",
    "title": "Unit testing in R",
    "section": "Testing complex functions",
    "text": "Testing complex functions\n\n\n\nmy_big_function &lt;- function(type) {\n  con &lt;- dbConnect(RSQLite::SQLite(), \"data.db\")\n  df &lt;- tbl(con, \"data_table\") |&gt;\n    collect() |&gt;\n    mutate(across(date, lubridate::ymd))\n\n  conditions &lt;- read_csv(\n    \"conditions.csv\", col_types = \"cc\"\n  ) |&gt;\n    filter(condition_type == type)\n\n  df |&gt;\n    semi_join(conditions, by = \"condition\") |&gt;\n    count(date) |&gt;\n    ggplot(aes(date, n)) +\n    geom_line() +\n    geom_point()\n}\n\n\nWhere do you even begin to start writing tests for something so complex?\n\n\nNote: to get the code on the left to fit on one page, I skipped including a few library calls\n\nlibrary(tidyverse)\nlibrary(DBI)"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions",
    "title": "Unit testing in R",
    "section": "Split the logic into smaller functions",
    "text": "Split the logic into smaller functions\nFunction to get the data from the database\n\nget_data_from_sql &lt;- function() {\n  con &lt;- dbConnect(RSQLite::SQLite(), \"data.db\")\n  tbl(con, \"data_table\") |&gt;\n    collect() |&gt;\n    mutate(across(date, lubridate::ymd))\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-1",
    "title": "Unit testing in R",
    "section": "Split the logic into smaller functions",
    "text": "Split the logic into smaller functions\nFunction to get the relevant conditions\n\nget_conditions &lt;- function(type) {\n  read_csv(\n    \"conditions.csv\", col_types = \"cc\"\n  ) |&gt;\n    filter(condition_type == type)\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-2",
    "title": "Unit testing in R",
    "section": "Split the logic into smaller functions",
    "text": "Split the logic into smaller functions\nFunction to combine the data and create a count by date\n\nsummarise_data &lt;- function(df, conditions) {\n  df |&gt;\n    semi_join(conditions, by = \"condition\") |&gt;\n    count(date)\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-3",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-3",
    "title": "Unit testing in R",
    "section": "Split the logic into smaller functions",
    "text": "Split the logic into smaller functions\nFunction to generate a plot from the summarised data\n\ncreate_plot &lt;- function(df) {\n  df |&gt;\n    ggplot(aes(date, n)) +\n    geom_line() +\n    geom_point()\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-4",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-4",
    "title": "Unit testing in R",
    "section": "Split the logic into smaller functions",
    "text": "Split the logic into smaller functions\nThe original function refactored to use the new functions\n\nmy_big_function &lt;- function(type) {\n  conditions &lt;- get_conditions(type)\n\n  get_data_from_sql() |&gt;\n    summarise_data(conditions) |&gt;\n    create_plot()\n}\n\n\nThis is going to be significantly easier to test, because we now can verify that the individual components work correctly, rather than having to consider all of the possibilities at once."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data",
    "title": "Unit testing in R",
    "section": "Let‚Äôs test summarise_data",
    "text": "Let‚Äôs test summarise_data\nsummarise_data &lt;- function(df, conditions) {\n  df |&gt;\n    semi_join(conditions, by = \"condition\") |&gt;\n    count(date)\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-1",
    "title": "Unit testing in R",
    "section": "Let‚Äôs test summarise_data",
    "text": "Let‚Äôs test summarise_data\ntest_that(\"it summarises the data\", {\n  # arrange\n  \n\n\n\n\n\n\n  \n\n  \n  # act\n  \n  # assert\n  \n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-2",
    "title": "Unit testing in R",
    "section": "Let‚Äôs test summarise_data",
    "text": "Let‚Äôs test summarise_data\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  \n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  \n\n\n\n\n  # act\n  \n  # assert\n  \n})\n\nGenerate some random data to build a reasonably sized data frame.\nYou could also create a table manually, but part of the trick of writing good tests for this function is to make it so the dates don‚Äôt all have the same count.\nThe reason for this is it‚Äôs harder to know for sure that the count worked if every row returns the same value.\nWe don‚Äôt need the values to be exactly like they are in the real data, just close enough. Instead of dates, we can use numbers, and instead of actual conditions, we can use letters."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-3",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-3",
    "title": "Unit testing in R",
    "section": "Let‚Äôs test summarise_data",
    "text": "Let‚Äôs test summarise_data\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  set.seed(123)\n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  \n\n\n\n\n  # act\n  \n  # assert\n  \n})\n\nTests need to be reproducible, and generating our table at random will give us unpredictable results.\nSo, we need to set the random seed; now every time this test runs we will generate the same data."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-4",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-4",
    "title": "Unit testing in R",
    "section": "Let‚Äôs test summarise_data",
    "text": "Let‚Äôs test summarise_data\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  set.seed(123)\n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  conditions &lt;- tibble(condition = c(\"a\", \"b\"))    \n  \n\n\n\n  # act\n  \n  # assert\n  \n})\n\nCreate the conditions table. We don‚Äôt need all of the columns that are present in the real csv, just the ones that will make our code work.\nWe also need to test that the filtering join (semi_join) is working, so we want to use a subset of the conditions that were used in df."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-5",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-5",
    "title": "Unit testing in R",
    "section": "Let‚Äôs test summarise_data",
    "text": "Let‚Äôs test summarise_data\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  set.seed(123)\n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  conditions &lt;- tibble(condition = c(\"a\", \"b\"))    \n  \n  \n\n  \n  # act\n  actual &lt;- summarise_data(df, conditions)\n  # assert\n  \n})\n\nBecause we are generating df randomly, to figure out what our ‚Äúexpected‚Äù results are, I simply ran the code inside of the test to generate the ‚Äúactual‚Äù results.\nGenerally, this isn‚Äôt a good idea. You are creating the results of your test from the code; ideally, you want to be thinking about what the results of your function should be.\nImagine your function doesn‚Äôt work as intended, there is some subtle bug that you are not yet aware of. By writing tests ‚Äúbackwards‚Äù you may write test cases that confirm the results, but not expose the bug. This is why it‚Äôs good to think about edge cases."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-6",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-6",
    "title": "Unit testing in R",
    "section": "Let‚Äôs test summarise_data",
    "text": "Let‚Äôs test summarise_data\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  set.seed(123)\n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  conditions &lt;- tibble(condition = c(\"a\", \"b\"))    \n  expected &lt;- tibble(\n    date = 1:10,\n    n = c(19, 18, 12, 14, 17, 18, 24, 18, 31, 21)\n  )  \n  # act\n  actual &lt;- summarise_data(df, conditions)\n  # assert\n  \n})\n\nThat said, in cases where we can be confident (say by static analysis of our code) that it is correct, building tests in this way will give us the confidence going forwards that future changes do not break existing functionality.\nIn this case, I have created the expected data frame using the results from running the function."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-7",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-7",
    "title": "Unit testing in R",
    "section": "Let‚Äôs test summarise_data",
    "text": "Let‚Äôs test summarise_data\n\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  set.seed(123)\n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  conditions &lt;- tibble(condition = c(\"a\", \"b\"))\n  expected &lt;- tibble(\n    date = 1:10,\n    n = c(19, 18, 12, 14, 17, 18, 24, 18, 31, 21)\n  )\n  # act\n  actual &lt;- summarise_data(df, conditions)\n  # assert\n  expect_equal(actual, expected)\n})\n\nTest passed üò∏\n\n\n\nThe test works!"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#next-steps",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#next-steps",
    "title": "Unit testing in R",
    "section": "Next steps",
    "text": "Next steps\n\nYou can add tests to any R project (to test functions),\nBut {testthat} works best with Packages\nThe R Packages book has 3 chapters on testing\nThere are two useful helper functions in {usethis}\n\nuse_testthat() will set up the folders for test scripts\nuse_test() will create a test file for the currently open script"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#next-steps-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#next-steps-1",
    "title": "Unit testing in R",
    "section": "Next steps",
    "text": "Next steps\n\nIf your test needs to temporarily create a file, or change some R-options, the {withr} package has a lot of useful functions that will automatically clean things up when the test finishes\nIf you are writing tests that involve calling out to a database, or you want to test my_big_function (from before) without calling the intermediate functions, then you should look at the {mockery} package"
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#which-is-easier-to-read",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#which-is-easier-to-read",
    "title": "Coffee and Coding",
    "section": "Which is easier to read?",
    "text": "Which is easier to read?\n\nae_attendances |&gt;\n  filter(org_code %in% c(\"RNA\", \"RL4\")) |&gt;\n  mutate(performance = 1 + breaches / attendances) |&gt;\n  filter(type == 1) |&gt;\n  mutate(met_target = performance &gt;= 0.95)\n\nor\n\nae_attendances |&gt;\n  filter(\n    org_code %in% c(\"RNA\", \"RL4\"),\n    type == 1\n  ) |&gt;\n  mutate(\n    performance = 1 + breaches / attendances,\n    met_target = performance &gt;= 0.95\n  )\n\n\n  spending a few seconds to neatly format your code can greatly improve the legibility to future readers, making the intent of the code far clearer, and will make finding bugs easier to spot.\n\n\n  (have you spotted the mistake in the snippets above?)"
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#tidyverse-style-guide",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#tidyverse-style-guide",
    "title": "Coffee and Coding",
    "section": "Tidyverse Style Guide",
    "text": "Tidyverse Style Guide\n\nGood coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread\n\n\nAll style guides are fundamentally opinionated. Some decisions genuinely do make code easier to use (especially matching indenting to programming structure), but many decisions are arbitrary. The most important thing about a style guide is that it provides consistency, making code easier to write because you need to make fewer decisions.\n\ntidyverse style guide"
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#lintr-styler-are-your-new-best-friends",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#lintr-styler-are-your-new-best-friends",
    "title": "Coffee and Coding",
    "section": "{lintr} + {styler} are your new best friends",
    "text": "{lintr} + {styler} are your new best friends\n\n\n{lintr}\n\n{lintr} is a static code analysis tool that inspects your code (without running it)\nit checks for certain classes of errors (e.g.¬†mismatched { and (‚Äôs)\nit warns about potential issues (e.g.¬†using variables that aren‚Äôt defined)\nit warns about places where you are not adhering to the code style\n\n\n{styler}\n\n{styler} is an RStudio add in that automatically reformats your code, tidying it up to match the style guide\n99.9% of the time it will give you equivalent code, but there is the potential that it may change the behaviour of your code\nit will overwrite the files that you ask it to run on however, so it is vital to be using version control\na good workflow here is to save your file, ‚Äústage‚Äù the changes to your file, then run {styler}. You can then revert back to the staged changed if needed."
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#what-does-lintr-look-like",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#what-does-lintr-look-like",
    "title": "Coffee and Coding",
    "section": "What does {lintr} look like?",
    "text": "What does {lintr} look like?\n\n\n\nsource: Good practice for writing R code and R packages\n\nrunning lintr can be done in the console, e.g.\n\nlintr::lintr_dir(\".\")\n\nor via the Addins menu"
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#using-styler",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#using-styler",
    "title": "Coffee and Coding",
    "section": "Using {styler}",
    "text": "Using {styler}\n\nsource: Good practice for writing R code and R packages"
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#further-thoughts-on-improving-code-legibility",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#further-thoughts-on-improving-code-legibility",
    "title": "Coffee and Coding",
    "section": "Further thoughts on improving code legibility",
    "text": "Further thoughts on improving code legibility\n\ndo not let files grow too big\nbreak up logic into separate files, then you can use source(\"filename.R) to run the code in that file\nidealy, break up your logic into separate functions, each function having it‚Äôs own file, and then call those functions within your analysis\ndo not repeat yourself - if you are copying and pasting your code then you should be thinking about how to write a single function to handle this repeated logic\n\n\n\nview slides at the-strategy-unit.github.io/data_science/presentations"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#welcome-to-coffee-and-coding",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#welcome-to-coffee-and-coding",
    "title": "Coffee and coding",
    "section": "Welcome to coffee and coding",
    "text": "Welcome to coffee and coding\n\nProject demos, showcasing work from a particular project\nMethod demos, showcasing how to use a particular method/tool/package\nSurgery and problem solving sessions\nDefining code standards and SOP"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#what-are-we-trying-to-achieve",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#what-are-we-trying-to-achieve",
    "title": "Coffee and coding",
    "section": "What are we trying to achieve?",
    "text": "What are we trying to achieve?\n\nLegibility\nReproducibility\nAccuracy\nLaziness"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#what-are-some-of-the-fundamental-principles",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#what-are-some-of-the-fundamental-principles",
    "title": "Coffee and coding",
    "section": "What are some of the fundamental principles?",
    "text": "What are some of the fundamental principles?\n\nPredictability, reducing mental load, and reducing truck factor\nMaking it easy to collaborate with yourself and others on different computers, in the cloud, in six months‚Äô time‚Ä¶\nDRY"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#what-is-rap",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#what-is-rap",
    "title": "Coffee and coding",
    "section": "What is RAP",
    "text": "What is RAP\n\na process in which code is used to minimise manual, undocumented steps, and a clear, properly documented process is produced in code which can reliably give the same result from the same dataset\nRAP should be:\n\n\nthe core working practice that must be supported by all platforms and teams; make this a core focus of NHS analyst training\n\nGoldacre review"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#the-road-to-rap",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#the-road-to-rap",
    "title": "Coffee and coding",
    "section": "The road to RAP",
    "text": "The road to RAP\n\nWe‚Äôre roughly using NHS Digital‚Äôs RAP stages\nThere is an incredibly large amount to learn!\nConfession time! (everything I do not know‚Ä¶)\nYou don‚Äôt need to do it all at once\nYou don‚Äôt need to do it all at all ever\nEach thing you learn will incrementally help you\nRemember- that‚Äôs why we learnt this stuff. Because it helped us. And it can help you too"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--baseline",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--baseline",
    "title": "Coffee and coding",
    "section": "Levels of RAP- Baseline",
    "text": "Levels of RAP- Baseline\n\nData produced by code in an open-source language (e.g., Python, R, SQL).\nCode is version controlled (see Git basics and using Git collaboratively guides).\nRepository includes a README.md file (or equivalent) that clearly details steps a user must follow to reproduce the code\nCode has been peer reviewed.\nCode is published in the open and linked to & from accompanying publication (if relevant).\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--silver",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--silver",
    "title": "Coffee and coding",
    "section": "Levels of RAP- Silver",
    "text": "Levels of RAP- Silver\n\nCode is well-documented‚Ä¶\nCode is well-organised following standard directory format\nReusable functions and/or classes are used where appropriate\nPipeline includes a testing framework\nRepository includes dependency information (e.g.¬†requirements.txt, PipFile, environment.yml\nData is handled and output in a Tidy data format\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--gold",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--gold",
    "title": "Coffee and coding",
    "section": "Levels of RAP- Gold",
    "text": "Levels of RAP- Gold\n\nCode is fully packaged\nRepository automatically runs tests etc. via CI/CD or a different integration/deployment tool e.g.¬†GitHub Actions\nProcess runs based on event-based triggers (e.g., new data in database) or on a schedule\nChanges to the RAP are clearly signposted. E.g. a changelog in the package, releases etc. (See gov.uk info on Semantic Versioning)\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#a-learning-journey-to-get-us-there",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#a-learning-journey-to-get-us-there",
    "title": "Coffee and coding",
    "section": "A learning journey to get us there",
    "text": "A learning journey to get us there\n\nCode style, organising your files\nFunctions and iteration\nGit and GitHub\nPackaging your code\nTesting\nPackage management and versioning"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#how-we-can-help-each-other-get-there",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#how-we-can-help-each-other-get-there",
    "title": "Coffee and coding",
    "section": "How we can help each other get there",
    "text": "How we can help each other get there\n\nWork as a team!\nCoffee and coding!\nAsk for help!\nDo pair coding!\nGet your code reviewed!\nJoin the NHS-R/ NHSPycom communities\n\n\n\nview slides at the-strategy-unit.github.io/data_science/presentations"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-is-rap",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-is-rap",
    "title": "RAP",
    "section": "What is RAP",
    "text": "What is RAP\n\na process in which code is used to minimise manual, undocumented steps, and a clear, properly documented process is produced in code which can reliably give the same result from the same dataset\nRAP should be:\n\n\nthe core working practice that must be supported by all platforms and teams; make this a core focus of NHS analyst training\n\nGoldacre review"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-are-we-trying-to-achieve",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-are-we-trying-to-achieve",
    "title": "RAP",
    "section": "What are we trying to achieve?",
    "text": "What are we trying to achieve?\n\nLegibility\nReproducibility\nAccuracy\nLaziness"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-are-some-of-the-fundamental-principles",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-are-some-of-the-fundamental-principles",
    "title": "RAP",
    "section": "What are some of the fundamental principles?",
    "text": "What are some of the fundamental principles?\n\nPredictability, reducing mental load, and reducing truck factor\nMaking it easy to collaborate with yourself and others on different computers, in the cloud, in six months‚Äô time‚Ä¶\nDRY"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#the-road-to-rap",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#the-road-to-rap",
    "title": "RAP",
    "section": "The road to RAP",
    "text": "The road to RAP\n\nWe‚Äôre roughly using NHS Digital‚Äôs RAP stages\nThere is an incredibly large amount to learn!\nConfession time! (everything I do not know‚Ä¶)\nYou don‚Äôt need to do it all at once\nYou don‚Äôt need to do it all at all ever\nEach thing you learn will incrementally help you\nRemember- that‚Äôs why we learnt this stuff. Because it helped us. And it can help you too"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--baseline",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--baseline",
    "title": "RAP",
    "section": "Levels of RAP- Baseline",
    "text": "Levels of RAP- Baseline\n\nData produced by code in an open-source language (e.g., Python, R, SQL).\nCode is version controlled (see Git basics and using Git collaboratively guides).\nRepository includes a README.md file (or equivalent) that clearly details steps a user must follow to reproduce the code\nCode has been peer reviewed.\nCode is published in the open and linked to & from accompanying publication (if relevant).\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--silver",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--silver",
    "title": "RAP",
    "section": "Levels of RAP- Silver",
    "text": "Levels of RAP- Silver\n\nCode is well-documented‚Ä¶\nCode is well-organised following standard directory format\nReusable functions and/or classes are used where appropriate\nPipeline includes a testing framework\nRepository includes dependency information (e.g.¬†requirements.txt, PipFile, environment.yml\nData is handled and output in a Tidy data format\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--gold",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--gold",
    "title": "RAP",
    "section": "Levels of RAP- Gold",
    "text": "Levels of RAP- Gold\n\nCode is fully packaged\nRepository automatically runs tests etc. via CI/CD or a different integration/deployment tool e.g.¬†GitHub Actions\nProcess runs based on event-based triggers (e.g., new data in database) or on a schedule\nChanges to the RAP are clearly signposted. E.g. a changelog in the package, releases etc. (See gov.uk info on Semantic Versioning)\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#a-learning-journey-to-get-you-there",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#a-learning-journey-to-get-you-there",
    "title": "RAP",
    "section": "A learning journey to get you there",
    "text": "A learning journey to get you there\n\nCode style, organising your files\nFunctions and iteration\nGit and GitHub\nPackaging your code\nTesting\nPackage management and versioning"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#how-we-can-help-each-other-get-there",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#how-we-can-help-each-other-get-there",
    "title": "RAP",
    "section": "How we can help each other get there",
    "text": "How we can help each other get there\n\nWork as a team!\nCoffee and coding!\nAsk for help!\nDo pair coding!\nGet your code reviewed!\nJoin the NHS-R/ NHSPycom communities"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#haca",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#haca",
    "title": "RAP",
    "section": "HACA",
    "text": "HACA\n\nThe first national analytics conference for health and care\nInsight to action!\nJuly 11th and 12th, University of Birmingham\nAccepting abstracts for short and long talks and posters\nAbstract deadline 27th March\nHelp is available (with abstract, poster, preparing presentation‚Ä¶)!\n\n\n\nview slides at the-strategy-unit.github.io/data_science/presentations"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-is-data-science",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-is-data-science",
    "title": "Travels with R and Python",
    "section": "What is data science?",
    "text": "What is data science?\n\n‚ÄúA data scientist knows more about computer science than the average statistician, and more about statistics than the average computer scientist‚Äù\n\n(Josh Wills, a former head of data engineering at Slack)"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#drew-conways-famous-venn-diagram",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#drew-conways-famous-venn-diagram",
    "title": "Travels with R and Python",
    "section": "Drew Conway‚Äôs famous Venn diagram",
    "text": "Drew Conway‚Äôs famous Venn diagram\n\nSource"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-are-the-skills-of-data-science",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-are-the-skills-of-data-science",
    "title": "Travels with R and Python",
    "section": "What are the skills of data science?",
    "text": "What are the skills of data science?\n\nAnalysis\n\nML\nStats\nData viz\n\nSoftware engineering\n\nProgramming\nSQL/ data\nDevOps\nRAP"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-are-the-skills-of-data-science-1",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-are-the-skills-of-data-science-1",
    "title": "Travels with R and Python",
    "section": "What are the skills of data science?",
    "text": "What are the skills of data science?\n\nDomain knowledge\n\nCommunication\nProblem formulation\nDashboards and reports"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#stats-and-data-viz",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#stats-and-data-viz",
    "title": "Travels with R and Python",
    "section": "Stats and data viz",
    "text": "Stats and data viz\n\nML leans a bit more towards atheoretical prediction\nStats leans a bit more towards inference (but they both do both)\nData scientists may use different visualisations\n\nInteractive web based tools\nDashboard based visualisers e.g.¬†{stminsights}"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#software-engineering",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#software-engineering",
    "title": "Travels with R and Python",
    "section": "Software engineering",
    "text": "Software engineering\n\nProgramming\n\nNo/ low code data science?\n\nSQL/ data\n\nTend to use reproducible automated processes\n\nDevOps\n\nPlan, code, build, test, release, deploy, operate, monitor\n\nRAP\n\nI will come back to this"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#domain-knowledge",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#domain-knowledge",
    "title": "Travels with R and Python",
    "section": "Domain knowledge",
    "text": "Domain knowledge\n\nDo stuff that matters\n\nThe best minds of my generation are thinking about how to make people click ads. That sucks. Jeffrey Hammerbacher\n\nConvince other people that it matters\nThis is the hardest part of data science"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#rap",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#rap",
    "title": "Travels with R and Python",
    "section": "RAP",
    "text": "RAP\n\nData science isn‚Äôt RAP\nRAP isn‚Äôt data science\nThey are firm friends"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#reproducibility",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#reproducibility",
    "title": "Travels with R and Python",
    "section": "Reproducibility",
    "text": "Reproducibility\n\nReproducibility in science\nThe $6B spreadsheet error\nGeorge Osbourne‚Äôs austerity was based on a spreadsheet error\nFor us, reproducibility also means we can do the same analysis 50 times in one minute\n\nWhich is why I started down the road of data science"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-is-rap",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-is-rap",
    "title": "Travels with R and Python",
    "section": "What is RAP",
    "text": "What is RAP\n\na process in which code is used to minimise manual, undocumented steps, and a clear, properly documented process is produced in code which can reliably give the same result from the same dataset\nRAP should be:\n\n\nthe core working practice that must be supported by all platforms and teams; make this a core focus of NHS analyst training\n\n\nGoldacre review"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--baseline",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--baseline",
    "title": "Travels with R and Python",
    "section": "Levels of RAP- Baseline",
    "text": "Levels of RAP- Baseline\n\nData produced by code in an open-source language (e.g., Python, R, SQL)\nCode is version controlled\nRepository includes a README.md file that clearly details steps a user must follow to reproduce the code\nCode has been peer reviewed\nCode is published in the open and linked to & from accompanying publication (if relevant)\n\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--silver",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--silver",
    "title": "Travels with R and Python",
    "section": "Levels of RAP- Silver",
    "text": "Levels of RAP- Silver\n\nCode is well-documented‚Ä¶\nCode is well-organised following standard directory format\nReusable functions and/or classes are used where appropriate\nPipeline includes a testing framework\nRepository includes dependency information (e.g.¬†requirements.txt, PipFile, environment.yml)\nData is handled and output in a Tidy data format\n\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--gold",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--gold",
    "title": "Travels with R and Python",
    "section": "Levels of RAP- Gold",
    "text": "Levels of RAP- Gold\n\nCode is fully packaged\nRepository automatically runs tests etc. via CI/CD or a different integration/deployment tool e.g.¬†GitHub Actions\nProcess runs based on event-based triggers (e.g., new data in database) or on a schedule\nChanges to the RAP are clearly signposted. E.g. a changelog in the package, releases etc. (See gov.uk info on Semantic Versioning)\n\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#data-science-in-healthcare",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#data-science-in-healthcare",
    "title": "Travels with R and Python",
    "section": "Data science in healthcare",
    "text": "Data science in healthcare\n\nForecasting\n\nStats versus ML\n\nText mining\n\nR versus Python\n\nDemand modelling\n\nDevOps as a way of life"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#get-involved",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#get-involved",
    "title": "Travels with R and Python",
    "section": "Get involved!",
    "text": "Get involved!\n\nNHS-R community\n\nWebinars, training, conference, Slack\n\nNHS Pycom\n\nditto‚Ä¶\n\nMLCSU GitHub?\nBuild links with the other CSUs"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#contact",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#contact",
    "title": "Travels with R and Python",
    "section": "Contact",
    "text": "Contact\n\n\n\n\n strategy.unit@nhs.net\n The-Strategy-Unit\n\n\n\n\n\n chris.beeley1@nhs.net\n chrisbeeley\n\n\n\n\n\n\nview slides at the-strategy-unit.github.io/data_science/presentations"
  },
  {
    "objectID": "style/style_guide.html",
    "href": "style/style_guide.html",
    "title": "Style Guide",
    "section": "",
    "text": "In general, follow the conventions of the tidyverse style guide.\nPrefer packages to be explicitly namespaced with a double colon in production code, like dplyr::mutate(), though this is not essential in exploratory data analysis.\nFavour the base R pipe (|&gt;) over the {magrittr} pipe (%&gt;%).\nAvoid library(tidyverse) in production code because it attaches a lot of packages that might not be used, though you may use it in exploratory data analysis.\nUse {styler} and {lintr} (or Python equivalents such as black) to tidy your code.\nInsert linebreaks in your code at or before column 80.\nWhen using {dplyr}, favour one mutate over many. For example, between the two examples below, example B is preferred:\n\nEXAMPLE A:\nlibrary(dplyr)\n\nstarwars |&gt;\n  mutate(height_cm = height) |&gt;\n  mutate(name_copy = name)\nEXAMPLE B:\nstarwars |&gt;\n  mutate(\n    height_cm = height,\n    name_copy = name\n  )\n\n\n\n\nFavour Quarto (.qmd files) over R Markdown (.Rmd) for document production.\nUse Git for all projects and GitHub as the remote home for of all of the project code.\nUse the Reproducible Analytical Pipelines (RAP) approach wherever possible.\nLine breaks in Markdown (.md) files should be at 120 characters or at sentence breaks.\nWhen writing about code, use curly braces to identify a {package} name and use backticks around `functions()` as these render nicely and highlight the words clearly.\nIf you‚Äôre not sure about something try the NHS-R Way, the UK Government accessibility guidelines, or the Turing Way. If you‚Äôre still not sure, just ask the team."
  },
  {
    "objectID": "style/style_guide.html#code-style",
    "href": "style/style_guide.html#code-style",
    "title": "Style Guide",
    "section": "",
    "text": "In general, follow the conventions of the tidyverse style guide.\nPrefer packages to be explicitly namespaced with a double colon in production code, like dplyr::mutate(), though this is not essential in exploratory data analysis.\nFavour the base R pipe (|&gt;) over the {magrittr} pipe (%&gt;%).\nAvoid library(tidyverse) in production code because it attaches a lot of packages that might not be used, though you may use it in exploratory data analysis.\nUse {styler} and {lintr} (or Python equivalents such as black) to tidy your code.\nInsert linebreaks in your code at or before column 80.\nWhen using {dplyr}, favour one mutate over many. For example, between the two examples below, example B is preferred:\n\nEXAMPLE A:\nlibrary(dplyr)\n\nstarwars |&gt;\n  mutate(height_cm = height) |&gt;\n  mutate(name_copy = name)\nEXAMPLE B:\nstarwars |&gt;\n  mutate(\n    height_cm = height,\n    name_copy = name\n  )"
  },
  {
    "objectID": "style/style_guide.html#additional-assorted-notes-on-style",
    "href": "style/style_guide.html#additional-assorted-notes-on-style",
    "title": "Style Guide",
    "section": "",
    "text": "Favour Quarto (.qmd files) over R Markdown (.Rmd) for document production.\nUse Git for all projects and GitHub as the remote home for of all of the project code.\nUse the Reproducible Analytical Pipelines (RAP) approach wherever possible.\nLine breaks in Markdown (.md) files should be at 120 characters or at sentence breaks.\nWhen writing about code, use curly braces to identify a {package} name and use backticks around `functions()` as these render nicely and highlight the words clearly.\nIf you‚Äôre not sure about something try the NHS-R Way, the UK Government accessibility guidelines, or the Turing Way. If you‚Äôre still not sure, just ask the team."
  },
  {
    "objectID": "style/project_structure.html",
    "href": "style/project_structure.html",
    "title": "Project Structure",
    "section": "",
    "text": "Analytical projects should be self-contained and portable. This means that all the materials required for an analysis should be organised into a single folder that can be shared in its entirety and be re-run by other people, ideally via GitHub.\nWe recommend RStudio Projects as a system for creating standardised project structures that meet these goals. The {usethis} package contains a number of helper functions to help get you started, including usethis::create_project().\n\n\nOne of the most common issues you‚Äôll face when using a project someone else has created, or you created previously, is maintaining the required packages to run the project. Knowing what packages are needed to run a particular project isn‚Äôt always obvious, and over time packages can change, rendering code that once worked unusable.\nThe {renv} R package helps solve this problem by:\n\nKeeping track of the packages that are required for a particular project.\nLogging the installed version of all of the packages.\nMaintaining a per-project library of packages, so projects don‚Äôt interfere with one another.\n\n\n\n\nIt‚Äôs helpful to split discrete analytical tasks into separate script files, which can make it easier to handle the codebase in context and provide an obvious order of operations. For example, 01_read.R, 02_wrangle.R, 03_model.R, etc.\nYou could still forget to re-run one of the numbered files, however, or it may take a long time to re-run all the steps again if you only make one small change to the code. This is where a workflow manager is useful.\nWe recommend the {targets} R package as a workflow manager. You write a series of steps and {targets} automatically recognises all the relationships between functions and objects as a graph. This means {targets} knows the order that things should be run and knows which bits of code need to be re-run if there are upstream changes. It‚Äôs a well-documented and supported package.\n\n\n\nIt‚Äôs beneficial to convert code into discrete functions where possible. This makes it easier to:\n\nreduce the chance of errors, because you‚Äôll avoid repetitive and mistake-prone copy-pasting of code\nunderstand your scripts, because code can be condensed into a simpler calls that are easier to read\nreuse your code, because functions allow you to consistently call the same code more than once and can be copied into other projects\ndebug, because the source of an error can be more easily traced and your code can be tested more easily\n\nConsider the DRY (Don‚Äôt Repeat Yourself) principle when deciding whether or not to convert some code into a function. It may be better to write a function if you‚Äôve used the same piece of code more than once in an analysis, especially if it contains many lines.\nFunction names should be short but descriptive and should contain a verb that describes what the function does. For example, get_geospatial_data() may be better than the generic get_data(), which is certainly better than the uninformative data().\nIn a project, it‚Äôs conventional to put your functions in a folder called R in the project‚Äôs root directory. You can group functions into separate R scripts with meaningful names to make it easier to organise them (read-data.R, model.R, etc). You can then source() these function scripts into your analytical scripts as required.\n\n\n\n\nIt may be beneficial to gather your functions into a discrete package so that you and others can install and reuse them for other projects.\nThe {usethis} package has a number of shortcuts to help you set up a package. You can begin with usethis::create_package() to generate the basic structure and then usethis::use_r and usethis::use_test() to add scripts and {testthat} tests into the correct folder structure.\nWe recommend you include a number of extra files in your package to make its purpose clear and to encourage collaboration. This includes:\n\na README file to describe the purpose of your package and provide some simple examples, which you can set up with usethis::use_readme_md() or usethis::use_readme_rmd() if it contains R code that you want to execute\na NEWS file with usethis::use_news_md(), which is used to communicate the latest changes to your package\na CODE_OF_CONDUCT file with usethis::use_code_of_conduct to explain to collaborators how they should engage with your project\nvignettes with usethis::use_vignette(), which are short documents that let you mix code with prose to describe how to use the functions in your package\n\nWe recommend semantic versioning as you develop your package. In this system, the version number is composed of three digits (like ‚Äò1.2.3‚Äô) that are each incremented as you make major breaking changes, minor changes and patches or bug fixes. The usethis::use_version() function can help you to do this and to automatically update the DESCRIPTION and NEWS file.\nUse {pkgdown} to autogenerate a website from your package‚Äôs documentation. This lets people see your documentation rendered nicely on the internet, without the need to install the package. You can serve this site on the web and update it automatically using GitHub Pages and GitHub Actions."
  },
  {
    "objectID": "style/project_structure.html#rstudio-projects",
    "href": "style/project_structure.html#rstudio-projects",
    "title": "Project Structure",
    "section": "",
    "text": "Analytical projects should be self-contained and portable. This means that all the materials required for an analysis should be organised into a single folder that can be shared in its entirety and be re-run by other people, ideally via GitHub.\nWe recommend RStudio Projects as a system for creating standardised project structures that meet these goals. The {usethis} package contains a number of helper functions to help get you started, including usethis::create_project().\n\n\nOne of the most common issues you‚Äôll face when using a project someone else has created, or you created previously, is maintaining the required packages to run the project. Knowing what packages are needed to run a particular project isn‚Äôt always obvious, and over time packages can change, rendering code that once worked unusable.\nThe {renv} R package helps solve this problem by:\n\nKeeping track of the packages that are required for a particular project.\nLogging the installed version of all of the packages.\nMaintaining a per-project library of packages, so projects don‚Äôt interfere with one another.\n\n\n\n\nIt‚Äôs helpful to split discrete analytical tasks into separate script files, which can make it easier to handle the codebase in context and provide an obvious order of operations. For example, 01_read.R, 02_wrangle.R, 03_model.R, etc.\nYou could still forget to re-run one of the numbered files, however, or it may take a long time to re-run all the steps again if you only make one small change to the code. This is where a workflow manager is useful.\nWe recommend the {targets} R package as a workflow manager. You write a series of steps and {targets} automatically recognises all the relationships between functions and objects as a graph. This means {targets} knows the order that things should be run and knows which bits of code need to be re-run if there are upstream changes. It‚Äôs a well-documented and supported package.\n\n\n\nIt‚Äôs beneficial to convert code into discrete functions where possible. This makes it easier to:\n\nreduce the chance of errors, because you‚Äôll avoid repetitive and mistake-prone copy-pasting of code\nunderstand your scripts, because code can be condensed into a simpler calls that are easier to read\nreuse your code, because functions allow you to consistently call the same code more than once and can be copied into other projects\ndebug, because the source of an error can be more easily traced and your code can be tested more easily\n\nConsider the DRY (Don‚Äôt Repeat Yourself) principle when deciding whether or not to convert some code into a function. It may be better to write a function if you‚Äôve used the same piece of code more than once in an analysis, especially if it contains many lines.\nFunction names should be short but descriptive and should contain a verb that describes what the function does. For example, get_geospatial_data() may be better than the generic get_data(), which is certainly better than the uninformative data().\nIn a project, it‚Äôs conventional to put your functions in a folder called R in the project‚Äôs root directory. You can group functions into separate R scripts with meaningful names to make it easier to organise them (read-data.R, model.R, etc). You can then source() these function scripts into your analytical scripts as required."
  },
  {
    "objectID": "style/project_structure.html#packages",
    "href": "style/project_structure.html#packages",
    "title": "Project Structure",
    "section": "",
    "text": "It may be beneficial to gather your functions into a discrete package so that you and others can install and reuse them for other projects.\nThe {usethis} package has a number of shortcuts to help you set up a package. You can begin with usethis::create_package() to generate the basic structure and then usethis::use_r and usethis::use_test() to add scripts and {testthat} tests into the correct folder structure.\nWe recommend you include a number of extra files in your package to make its purpose clear and to encourage collaboration. This includes:\n\na README file to describe the purpose of your package and provide some simple examples, which you can set up with usethis::use_readme_md() or usethis::use_readme_rmd() if it contains R code that you want to execute\na NEWS file with usethis::use_news_md(), which is used to communicate the latest changes to your package\na CODE_OF_CONDUCT file with usethis::use_code_of_conduct to explain to collaborators how they should engage with your project\nvignettes with usethis::use_vignette(), which are short documents that let you mix code with prose to describe how to use the functions in your package\n\nWe recommend semantic versioning as you develop your package. In this system, the version number is composed of three digits (like ‚Äò1.2.3‚Äô) that are each incremented as you make major breaking changes, minor changes and patches or bug fixes. The usethis::use_version() function can help you to do this and to automatically update the DESCRIPTION and NEWS file.\nUse {pkgdown} to autogenerate a website from your package‚Äôs documentation. This lets people see your documentation rendered nicely on the internet, without the need to install the package. You can serve this site on the web and update it automatically using GitHub Pages and GitHub Actions."
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "Data Science Blog",
    "section": "",
    "text": "Nearest neighbour imputation\n\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nJacqueline Grout\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code and Test Driven Development\n\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2024\n\n\nYiWen Hon\n\n\n\n\n\n\n  \n\n\n\n\nReinstalling R Packages\n\n\n\n\n\n\n\ngit\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2023\n\n\nTom Jemmett\n\n\n\n\n\n\n  \n\n\n\n\nAlternative remote repositories\n\n\n\n\n\n\n\ngit\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2023\n\n\nTom Jemmett\n\n\n\n\n\n\n  \n\n\n\n\nCreating a hotfix with git\n\n\n\n\n\n\n\ngit\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nMar 24, 2023\n\n\nTom Jemmett\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs/posts/2024-01-17_nearest_neighbour.html",
    "href": "blogs/posts/2024-01-17_nearest_neighbour.html",
    "title": "Nearest neighbour imputation",
    "section": "",
    "text": "Recently I have been gathering data by GP practice, from a variety of different sources. The ultimate purpose of my project is to be able to report at an ICB/sub-ICB level1. The various datasets cover different timescales and consequently changes in GP practices over time have left me with mismatching datasets.1¬†An ICB (Integrated Care Board) is a statutory NHS organisation responsible for planning health services for their local populations\nMy approach has been to take as the basis of my project a recent GP List. Later in my project I want to perform calculations at a GP practice level based on an underlying health need and the data for this need is a CHD prevalence value from a dataset that is around 8 years old, and for which there is no update or alternative. From my recent list of 6454 practices, when I match to the need dataset, I am left with 151 practices without a value for need. If I remove these practices from the analysis then this could impact the analysis by sub-ICB since often a group of practices in the same area could be subject to changes, mergers and reorganisation.\nHere‚Äôs the packages and some demo objects to work with to create an example for two practices:\n\n\nCode\n# Packages\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidygeocoder)\nlibrary(leaflet)\nlibrary(viridisLite)\nlibrary(gt)\n\n# Create some data with two practices with no need data \n# and a selection of practices locally with need data\npractices &lt;- tribble(\n  ~practice_code, ~postcode, ~has_orig_need, ~value,\n  \"P1\",\"CV1 4FS\", 0, NA,\n  \"P2\",\"CV1 3GB\", 1, 7.3,\n  \"P3\",\"CV11 5TW\", 1, 6.9,\n  \"P4\",\"CV6 3HZ\", 1, 7.1,\n  \"P5\",\"CV6 1HS\", 1, 7.7,\n  \"P6\",\"CV6 5DF\", 1, 8.2,\n  \"P7\",\"CV6 3FA\", 1, 7.9,\n  \"P8\",\"CV1 2DL\", 1, 7.5,\n  \"P9\",\"CV1 4JH\", 1, 7.7,\n  \"P10\",\"CV10 0GQ\", 1, 7.5,\n  \"P11\",\"CV10 0JH\", 1, 7.8,\n  \"P12\",\"CV11 5QT\", 0, NA,\n  \"P13\",\"CV11 6AB\", 1, 7.6,\n  \"P14\",\"CV6 4DD\", 1,7.9\n) \n\n# get domain of numeric data\n(domain &lt;- range(practices$has_orig_need))\n\n# make a colour palette\npal &lt;- colorNumeric(palette = viridis(2), domain = domain)\n\n\nTo provide a suitable estimate of need for the newer practices without values, all the practices in the dataset were geocoded2 using the geocode function from the {tidygeocoder} package.2¬†Geocoding is the process of converting addresses (often the postcode) into geographic coordinates (such as latitude and longitude) that can be plotted on a map.\n\npractices &lt;- practices |&gt;\n  mutate(id = row_number()) |&gt;\n  geocode(postalcode = postcode) |&gt;\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326)\n\n\n\nCode\npractices |&gt;\n  gt()\n\n\n\n\n\n\n  \n    \n    \n      practice_code\n      postcode\n      has_orig_need\n      value\n      id\n      geometry\n    \n  \n  \n    P1\nCV1 4FS\n0\nNA\n1\nc(-1.50686326666667, 52.4141089666667)\n    P2\nCV1 3GB\n1\n7.3\n2\nc(-1.51888, 52.4034199)\n    P3\nCV11 5TW\n1\n6.9\n3\nc(-1.46746, 52.519)\n    P4\nCV6 3HZ\n1\n7.1\n4\nc(-1.52231, 52.42367)\n    P5\nCV6 1HS\n1\n7.7\n5\nc(-1.52542, 52.41989)\n    P6\nCV6 5DF\n1\n8.2\n6\nc(-1.498344825, 52.4250186)\n    P7\nCV6 3FA\n1\n7.9\n7\nc(-1.51787, 52.43135)\n    P8\nCV1 2DL\n1\n7.5\n8\nc(-1.49105, 52.40582)\n    P9\nCV1 4JH\n1\n7.7\n9\nc(-1.50653, 52.41953)\n    P10\nCV10 0GQ\n1\n7.5\n10\nc(-1.52197, 52.54074)\n    P11\nCV10 0JH\n1\n7.8\n11\nc(-1.5163199, 52.53723)\n    P12\nCV11 5QT\n0\nNA\n12\nc(-1.46927, 52.51899)\n    P13\nCV11 6AB\n1\n7.6\n13\nc(-1.45822, 52.52682)\n    P14\nCV6 4DD\n1\n7.9\n14\nc(-1.50832, 52.44104)\n  \n  \n  \n\n\n\n\nThis map shows the practices, purple are the practices with no need data and yellow are practices with need data available.\n\n\nCode\n# make map to display practices\nleaflet(practices) |&gt; \n  addTiles() |&gt;\n  addCircleMarkers(color = ~pal(has_orig_need)) \n\n\n\n\n\n\nThe data was split into those with, and without, a value for need. Using st_join from the {sf} package to join those without, and those with, a value for need, using the geometry to find all those within 1500m (1.5km).\n\nno_need &lt;- practices |&gt;\n  filter(has_orig_need == 0)\n\nwith_need &lt;- practices |&gt;\n  filter(has_orig_need == 1)\n\n\nneighbours &lt;- no_need |&gt;\n  select(no_need_postcode = postcode,no_need_prac_code=practice_code) |&gt;\n  st_join(with_need, st_is_within_distance, 1500) |&gt;\n  st_drop_geometry() |&gt;\n  select(id, no_need_postcode,no_need_prac_code) |&gt;\n  inner_join(x = with_need, by = join_by(\"id\")) \n\n\n\nCode\nleaflet(neighbours) |&gt; \n  addTiles() |&gt;\n  addCircleMarkers(color = \"purple\") |&gt;\n  addMarkers(  -1.50686326666667, 52.4141089666667, popup = \"Practice with no data\"\n) |&gt;\n  addCircles(-1.50686326666667, 52.4141089666667,radius=1500) |&gt;\n  addMarkers(-1.46927, 52.51899, popup = \"Practice with no data\"\n) |&gt;\naddCircles(-1.46927, 52.51899,radius=1500)\n\n\n\n\n\n\nThe data for the ‚Äúneighbours‚Äù was grouped by the practice code of those without need data and a mean value was calculated for each practice to generate an estimated value.\n\nneighbours_estimate &lt;- neighbours |&gt;\n  group_by(no_need_prac_code) |&gt;\n    summarise(need_est=mean(value)) |&gt;\n    st_drop_geometry(select(no_need_prac_code,need_est)) \n\nThe original data was joined back to the ‚Äúneighbours‚Äù.\n\n  practices_with_neighbours_estimate &lt;- practices |&gt;\n    left_join(neighbours_estimate, join_by(practice_code==no_need_prac_code))  |&gt;\n    st_drop_geometry(select(practice_code,need_est))\n\n\n\nCode\n  practices_with_neighbours_estimate |&gt;\n    select(-has_orig_need,-id) |&gt;\n    gt()\n\n\n\n\n\n\n  \n    \n    \n      practice_code\n      postcode\n      value\n      need_est\n    \n  \n  \n    P1\nCV1 4FS\nNA\n7.583333\n    P2\nCV1 3GB\n7.3\nNA\n    P3\nCV11 5TW\n6.9\nNA\n    P4\nCV6 3HZ\n7.1\nNA\n    P5\nCV6 1HS\n7.7\nNA\n    P6\nCV6 5DF\n8.2\nNA\n    P7\nCV6 3FA\n7.9\nNA\n    P8\nCV1 2DL\n7.5\nNA\n    P9\nCV1 4JH\n7.7\nNA\n    P10\nCV10 0GQ\n7.5\nNA\n    P11\nCV10 0JH\n7.8\nNA\n    P12\nCV11 5QT\nNA\n7.250000\n    P13\nCV11 6AB\n7.6\nNA\n    P14\nCV6 4DD\n7.9\nNA\n  \n  \n  \n\n\n\n\nFinally, an updated data frame was created of the need data using the actual need for the practice where available, otherwise using estimated need.\n\npractices_with_neighbours_estimate &lt;- practices_with_neighbours_estimate |&gt;\n    mutate(need_to_use = case_when(value&gt;=0 ~ value,\n                                       .default = need_est)) |&gt;\n    select(practice_code,need_to_use) \n\n\n\n\n\n\n\n  \n    \n    \n      practice_code\n      need_to_use\n    \n  \n  \n    P1\n7.583333\n    P2\n7.300000\n    P3\n6.900000\n    P4\n7.100000\n    P5\n7.700000\n    P6\n8.200000\n    P7\n7.900000\n    P8\n7.500000\n    P9\n7.700000\n    P10\n7.500000\n    P11\n7.800000\n    P12\n7.250000\n    P13\n7.600000\n    P14\n7.900000\n  \n  \n  \n\n\n\n\nFor my project, this method has successfully generated a prevalence for 125 of the 151 practices without a need value, leaving just 26 practices without a need. This is using a 1.5 km radius. In each use case there will be a decision to make regarding a more accurate estimate (smaller radius) and therefore fewer matches versus a less accurate estimate (using a larger radius) and therefore more matches.\nThis approach could be replicated for other similar uses/purposes. A topical example from an SU project is the need to assign population prevalence for hypertension and compare it to current QOF3 data. Again, the prevalence data is a few years old so we have to move the historical data to fit with current practices and this leaves missing data that can be estimated using this method.3¬†QOF (Quality and Outcomes Framework) is a voluntary annual reward and incentive programme for all GP practices in England, detailing practice achievement results."
  },
  {
    "objectID": "blogs/posts/2024-01-10-advent-of-code-and-test-driven-development.html",
    "href": "blogs/posts/2024-01-10-advent-of-code-and-test-driven-development.html",
    "title": "Advent of Code and Test Driven Development",
    "section": "",
    "text": "Advent of Code is an annual event, where daily coding puzzles are released from 1st ‚Äì 24th December. We ran one of our fortnightly Coffee & Coding sessions introducing Advent of Code to people who code in the Strategy Unit, as well as the concept of test-driven development as a potential way of approaching the puzzles.\nTest-driven development (TDD) is an approach to coding which involves writing the test for a function BEFORE we write the function. This might seem quite counterintuitive, but it makes it easier to identify bugs üêõ when they are introduced to our code, and ensures that our functions meet all necessary criteria. From my experience, this takes quite a long time to implement and can be quite tedious, but it is definitely worth it overall, especially as your project develops. Testing is also recommended in the NHS Reproducible Analytical Pipeline (RAP) guidelines.\nAn interesting thing to note about TDD is that we‚Äôre always expecting our first test to fail, and indeed failing tests are useful and important! If we wrote tests that just passed all the time, this would not be useful at all for our code.\nThe way that Advent of Code is structured, with test data for each puzzle and an expected test result, makes it very amenable to a test-driven approach. In order to support this, Matt and I created template repositories for a test-driven approach to Advent of Code, in Python and in R.\nOur goal when setting this up was to introduce others in the Strategy Unit to both TDD and Advent of Code. Advent of code can be challenging and I personally struggle to get past the first week, but it encourages creative (and maybe even fun?!) approaches to coding problems. I‚Äôm glad that we had the chance to explore some of the puzzles together in Coffee & Coding ‚Äì it was interesting to see so many different approaches to the same problem, and hopefully it also gave us all the chance to practice writing tests."
  }
]