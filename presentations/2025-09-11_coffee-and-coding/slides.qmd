---
title: "Efficient Coding"
subtitle: "Principles and Practices for Performant Code"
author: "Eirini & Rhian, DS @ SU"
format:
  revealjs:
    theme: dark
    code-fold: true
    code-overflow: wrap
    font-size: 0.7em
jupyter: python3
---

## Agenda

- **Measuring Performance**: Time and profile your code
- **Common Performance Tweaks**: Easy wins for faster code
- **Loops vs. Vectorisation vs. Functional**: Choose the right approach
- **Optimising Loops**: When you should use them
- **Beyond Basics**: Tools for further optimisation

## Measuring Performance

## Measuring Performance: Timing

```{python}
#| echo: true
#| code-fold: show
from timeit import timeit  # Python's precise timing module

size = 1_000_000

def sum_of_squares(): # Function to measure
    return sum(i**2 for i in range(size))

# Time the function execution
# number=100: run multiple times for statistical significance
# globals=globals(): access functions defined in current scope
execution_time = timeit('sum_of_squares()',
                       globals=globals(),
                       number=100)

# Calculate and display average execution time per call
print(f"Average execution time: {execution_time/1000:.6f}s")
```

## Measuring Performance: Profiling (1)

```{python}
#| echo: true
import cProfile, pstats
from io import StringIO

def sum_of_squares(n):
    return sum(i * i for i in range(n))

# Create a StringIO object to capture output
pr = cProfile.Profile()
pr.enable()
sum_of_squares(1_000_000)
pr.disable()

# Display results
s = StringIO()
ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
ps.print_stats(5)
print(s.getvalue())
```

## Measuring Performance: Profiling (2)

**Understanding the output:**

- `ncalls`: No. of times the function was called directly
- `tottime`: sum_of_squares consumed 0.000 seconds in its own code
- `cumtime`: Total time including nested calls
- `percall`: The bottleneck(s) i.e. `{built-in method builtins.sum}` and the generator expression (`i * i for i in range(n)`)

## Performance Tweaks

## Performance Tweak: String Concatenation

```{python}
#| echo: true
#| code-fold: show
from timeit import timeit  # Import timing functionality

size = 100_000

def join_method(): # Fast - O(n) complexity with clear intent
    return "".join("Hello World! " for _ in range(size))  # Single operation

def concat_method(): # Slow - O(nÂ²) complexity
    result = ""
    for _ in range(size):
        result += "Hello World! "  # Creates new string each time
    return result

t1 = timeit(join_method, number=100)  # Time the fast method
t2 = timeit(concat_method, number=100)  # Time the slow method
print(f"join: {t1:.6f}s\n+=: {t2:.6f}s")
print(f"Speedup: {t2/t1:.1f}x faster")
```

## Performance Tweak: Appropriate Data Structures

```{python}
#| echo: true
#| code-fold: show
from timeit import timeit  # For timing the operations
import random  # To select a random lookup value

size = 100_000

# Setup test data
data = list(range(size))  # A list with 100,000 items
lookup_val = random.choice(data)  # Random value to find
lookup_set = set(data)  # Same data as a set for O(1) lookup

# Time list lookup (O(n) - must check each element)
t1 = timeit(lambda: lookup_val in data, number=100)

# Time set lookup (O(1) - constant time hash table)
t2 = timeit(lambda: lookup_val in lookup_set, number=100)

print(f"List lookup: {t1:.6f}s\nSet lookup: {t2:.6f}s")
print(f"Speedup: {t1/t2:.1f}x faster")
```

## Performance Tweak: Pre-allocating Arrays

```{python}
#| echo: true
#| code-fold: show
from timeit import timeit  # For timing operations
import numpy as np  # For pre-allocated arrays

size = 100_000  # Number of elements to process

def growing_list(): # Growing a list dynamically (expensive)
    result = []  # Empty list that will grow
    for i in range(size):
        result.append(i**2)  # Triggers resize
    return result

def preallocated_array(): # Pre-allocated array
    result = np.zeros(size, dtype=int)  # Preallocate array
    for i in range(size):
        result[i] = i**2  # No resizing needed
    return result

t1 = timeit(growing_list, number=100)
t2 = timeit(preallocated_array, number=100)
print(f"Growing list: {t1:.6f}s\nPre-allocated: {t2:.6f}s")
print(f"Speedup: {t2/t1:.1f}x faster")
```

## Loops vs. Vectorisation vs. Functional

| Approach | Best For | Example Use Case |
|---|---|---|
| **Loops** | Complex logic, small data | Custom algorithms |
| **Vectorisation** | Numerical operations | Data science, NumPy |
| **Functional** | Data transformations | Pipelines, map/reduce/filter |

## Loops

```{python}
#| echo: true
#| code-fold: show
from timeit import timeit  # For timing code execution

size = 100_000

def standard_loop():  # Slower - bytecode overhead + repeated .append() calls
    result = []
    for i in range(size):
        result.append(i**2)
    return result

def list_comprehension():  # Faster, concise
    return [i**2 for i in range(size)]

t1 = timeit(standard_loop, number=100)
t2 = timeit(list_comprehension, number=100)

print(f"Standard loop: {t1:.6f}s\nList comprehension: {t2:.6f}s")
print(f"Speedup: {t1/t2:.1f}x faster")
```

## Vectorisation with NumPy

```{python}
#| echo: true
#| code-fold: show
from timeit import timeit  # For timing code execution
import numpy as np  # NumPy for vectorised operations

size = 100_000

def python_way(): # Pure Python approach (loops through each element)
    return [i**2 for i in range(size)]

def numpy_way(): # NumPy vectorised approach (operates on entire array at once)
    return np.arange(size)**2 # Uses C implementation

# Compare execution times
t1 = timeit(python_way, number=100)
t2 = timeit(numpy_way, number=100)

print(f"Python: {t1:.6f}s\nNumPy: {t2:.6f}s")
print(f"Speedup: {t1/t2:.1f}x faster")
```

## Vectorisation with Pandas

```{python}
#| echo: true
#| code-fold: show
import pandas as pd  # For DataFrame operations
import numpy as np   # For random data generation
from timeit import timeit  # For timing operations

# Create sample dataframe with 10,000 random values (reduced for demo)
df = pd.DataFrame({"value": np.random.rand(10_000)})

def apply_method(): # Slow: Using apply (runs Python function on each row)
    return df["value"].apply(lambda x: x**2)

def vector_method(): # Fast: Vectorised operations (C implementation)
    return df["value"]**2

# Compare execution times
t1 = timeit(apply_method, number=100)
t2 = timeit(vector_method, number=100)

print(f"apply: {t1:.6f}s\nvectorised: {t2:.6f}s")
print(f"Speedup: {t1/t2:.1f}x faster")
```

## Functional Programming
[Functional programming in Python? by David Vujic](https://www.youtube.com/watch?v=hz4OPyBYA98)

```{python}
#| echo: true
#| code-fold: show
from timeit import timeit

size = 100_000

# Compare map vs list comprehension
t1 = timeit(lambda: list(map(lambda x: x**2, range(size))), number=100)
t2 = timeit(lambda: [x**2 for x in range(size)], number=100)

print(f"map: {t1:.6f}s\ncomprehension: {t2:.6f}s")
print(f"Speedup: {t1/t2:.1f}x faster")
```

## Generators

```{python}
#| echo: true
#| code-fold: show
def count_up_to(limit):
    count = 0
    while count < limit:
        yield count
        count += 1

# Usage
for number in count_up_to(5):
    print(number)
```

## When to Use Each Approach

- **Vectorisation**: Large numerical datasets (NumPy/Pandas)
- **List Comprehensions**: Simple transformations on sequences
- **Functional**: Complex pipelines, data transformations
- **Loops**: Complex logic, small datasets, or when readability matters
- **Generators**: Process large datasets, that shouldn't be loaded entirely into memory

## Loop Optimisation Techniques

```{python}
#| echo: true
#| code-fold: show
from timeit import timeit  # For timing execution
import math  # For sqrt, sin, cos functions
from random import random

size = 1_000_000

data = [random()**2 for _ in range(size)] # Create test data

def regular_loop(): # len() and function lookups each iteration
    result = 0
    for i in range(len(data)):  # len() recalculated in every iteration
        x = data[i]
        result += math.sqrt(x) + math.sin(x) + math.cos(x)  # Function lookups each time
    return result

def optimised_loop(): # pre-computed length and local function bindings
    result = 0
    n = len(data)  # Pre-compute length once
    sqrt, sin, cos = math.sqrt, math.sin, math.cos  # Bind functions locally
    for i in range(n):
        x = data[i]
        result += sqrt(x) + sin(x) + cos(x)
    return result

t1 = timeit(regular_loop, number=100)
t2 = timeit(optimised_loop, number=100)
print(f"Regular: {t1:.6f}s\nOptimised: {t2:.6f}s")
print(f"Speedup: {t1/t2:.1f}x faster")
```

## Best Practices Summary

1. **Measure first** - profile before optimising
2. **Use appropriate data structures** for the task
3. **Vectorise numerical operations** when possible
4. **Avoid premature optimisation** - readable code first
5. **Know when to use loops, comprehensions, or functional styles**

[Effective Python, The Book: Third Edition, Brett Slatkin](https://effectivepython.com/)

# Appendix: Beyond Basics

## Appendix: Just-in-Time Compilation (1)

```{python}
#| echo: true
#| code-fold: show
from numba import jit
import numpy as np
from timeit import timeit

def slow_func(x):
    total = 0
    for i in range(len(x)):
        total += np.sin(x[i]) * np.cos(x[i])
    return total

@jit(nopython=True)
def fast_func(x):
    total = 0
    for i in range(len(x)):
        total += np.sin(x[i]) * np.cos(x[i])
    return total

x = np.random.random(10_000)
t1 = timeit(lambda: slow_func(x), number=100)
t2 = timeit(lambda: fast_func(x), number=100)
print(f"Python: {t1:.6f}s\nNumba: {t2:.6f}s")
print(f"Speedup: {t1/t2:.1f}x faster")
```

## Appendix: Just-In-Time Compilation (2)

**What is JIT?**

> [JIT (Just-In-Time) compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation) translates code into machine code at runtime to improve execution speed. This approach can improve performance by optimising the execution of frequently run code segments.

**Key Benefits:**
- Can provide 10-100x speed-ups for numerical code
- Works especially well with NumPy operations
- Requires minimal code changes (just add decorators)

## Appendix: Cython (Basics)

**Pure Python version (slow.py):**
```python
def calculate_sum(n):
    """Sum the squares from 0 to n-1"""
    total = 0
    for i in range(n):
        total += i * i
    return total
```

**Cython version (fast.pyx):**
```python
def calculate_sum_cy(int n):
    """Same function with static typing"""
    cdef int i, total = 0  # Static type declarations
    for i in range(n):
        total += i * i
    return total
```

**Result**: Typically 20-100x faster performance

## Appendix: Cython (Best Practices)

**Key techniques for maximum performance:**

```python
# 1. Declare types for all variables
cdef:
    int i, n = 10_000  # Integer variables
    double x = 0.5   # Floating point
    int* ptr         # C pointer

# 2. Use typed memoryviews for arrays (faster than NumPy)
def process(double[:] arr):  # Works with any array-like object
    cdef int i
    for i in range(arr.shape[0]):
        arr[i] = arr[i] * 2  # Direct memory access

# 3. Move Python operations outside loops
cdef double total = 0
py_func = some_python_function  # Store reference outside loop
for i in range(n):
    total += c_only_operations(i)

# 4. Use nogil for parallel execution with OpenMP
cpdef process_parallel(double[:] data) nogil:  # No Python GIL
    # Can now use OpenMP for parallelism
```

## Appendix: Cython (Compiling)

**Option 1: Using setuptools (recommended for projects)**
```python
# Create setup.py in your project directory:
from setuptools import setup, Extension
from Cython.Build import cythonize

setup(
    ext_modules = cythonize([
        Extension("fast", ["fast.pyx"]),
    ])
)

# Then compile: python setup.py build_ext --inplace
```

**Option 2: Quick development with pyximport**
```python
import pyximport
pyximport.install()  # Automatically compiles .pyx files
import fast  # Will compile fast.pyx on first import
```

**Option 3: Direct compilation**
```bash
cython -a fast.pyx  # Generates fast.c and HTML report
gcc -shared -fPIC -o fast.so fast.c \
    $(python3-config --includes) $(python3-config --ldflags)
```
