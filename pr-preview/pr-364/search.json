[
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#why",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#why",
    "title": "Repeating Yourself with Functions",
    "section": "Why?",
    "text": "Why?\n\nForecasting project, need to do the same thing with data for 6 centres.\nCopy-paste runs risk of not doing the same thing each time (and boring/time-consuming/frustrating).\nRepetition –&gt; function."
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#what",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#what",
    "title": "Repeating Yourself with Functions",
    "section": "What?",
    "text": "What?\n\n\nDemo with plots, equally applicable to ‘doing stuff’ with data.\n\n\n# preview data\nhead(new_rtt)\n\n  provider_code count rtt_yrmon rtt_mon\n1           RJE    83  Nov 2015      11\n2           RJE    75  Dec 2015      12\n3           RJE    82  Jan 2016       1\n4           RJE    74  Feb 2016       2\n5           RJE    62  Mar 2016       3\n6           RJE    76  Apr 2016       4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember, this is about writing functions, not creating stunning visualisations!\n\n\n\nRepeat this for each of the 6 centres"
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#how",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#how",
    "title": "Repeating Yourself with Functions",
    "section": "How?",
    "text": "How?\nDo it ‘normally’ for one centre. What are the parameters to change?\n\n\np1 &lt;- new_rtt |&gt; \n  filter(provider_code == \"RJE\") |&gt; \n  ggplot(aes(x = rtt_yrmon, y = count)) +\n  geom_line() +\n  su_theme() +\n    theme(legend.position = \"none\") +\n  labs(title = \"RJE\",\n       subtitle = \"time trend of new referrals\")\n\np2 &lt;- new_rtt |&gt; \n  filter(provider_code == \"RJE\") |&gt; \n  ggplot(aes(x = month(rtt_yrmon), y = count)) +\n  geom_col() +\n  su_theme() +\n    theme(legend.position = \"none\") +\n  labs(\n       subtitle = \"monthly pattern of new referrals\")\n\nplots &lt;- ggarrange(p1, p2, nrow = 2)\n\nplots\n\n\n\n\n\n\n\n\n\n\nThis becomes the argument for the function.\nChoose a name for the argument (!= variable_name)\nIn this example we will use prov in place of \"RJE\"\n\n\n\nPlease remember, this is about writing functions, not creating stunning visualisations!"
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#anatomy-of-a-function",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#anatomy-of-a-function",
    "title": "Repeating Yourself with Functions",
    "section": "Anatomy of a Function",
    "text": "Anatomy of a Function\n\nfn_name &lt;- function(arguments){\n  \n  # do stuff\n  \n}\n\nRun the function with fn_name(parameter as argument)"
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#turning-our-code-into-a-function",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#turning-our-code-into-a-function",
    "title": "Repeating Yourself with Functions",
    "section": "Turning our code into a function",
    "text": "Turning our code into a function\n\n\n\np1 &lt;- new_rtt |&gt; \n  filter(provider_code == \"RJE\") |&gt; \n  ggplot(aes(x = rtt_yrmon, y = count)) +\n  geom_line() +\n  su_theme() +\n    theme(legend.position = \"none\") +\n  labs(title = \"RJE\",\n       subtitle = \"time trend of new referrals\")\n\np2 &lt;- new_rtt |&gt; \n  filter(provider_code == \"RJE\") |&gt; \n  ggplot(aes(x = month(rtt_yrmon), y = count)) +\n  geom_col() +\n  su_theme() +\n    theme(legend.position = \"none\") +\n  labs(\n       subtitle = \"monthly pattern of new referrals\")\n\nplots &lt;- ggarrange(p1, p2, nrow = 2)\n\nplots\n\n\n\nfn_plots &lt;- function(prov){\n  \n    p1 &lt;- new_rtt |&gt; \n      filter(provider_code == prov) |&gt; \n      ggplot(aes(x = rtt_yrmon, y = count)) +\n      geom_line() +\n      su_theme() +\n        theme(legend.position = \"none\") +\n      labs(title = prov,\n           subtitle = \"time trend of new referrals\")\n    \n    p2 &lt;- new_rtt |&gt; \n      filter(provider_code == prov) |&gt; \n      ggplot(aes(x = month(rtt_yrmon), y = count)) +\n      geom_col() +\n      su_theme() +\n        theme(legend.position = \"none\") +\n      labs(\n           subtitle = \"monthly pattern of new referrals\")\n    \n    plots &lt;- ggarrange(p1, p2, nrow = 2)\n    \n    plots\n    \n}"
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#running-our-function",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#running-our-function",
    "title": "Repeating Yourself with Functions",
    "section": "Running our function",
    "text": "Running our function\n\n\n\nfn_plots &lt;- function(prov){\n  \n    p1 &lt;- new_rtt |&gt; \n      filter(provider_code == prov) |&gt; \n      ggplot(aes(x = rtt_yrmon, y = count)) +\n      geom_line() +\n      su_theme() +\n        theme(legend.position = \"none\") +\n      labs(title = prov,\n           subtitle = \"time trend of new referrals\")\n    \n    p2 &lt;- new_rtt |&gt; \n      filter(provider_code == prov) |&gt; \n      ggplot(aes(x = month(rtt_yrmon), y = count)) +\n      geom_col() +\n      su_theme() +\n        theme(legend.position = \"none\") +\n      labs(\n           subtitle = \"monthly pattern of new referrals\")\n    \n    plots &lt;- ggarrange(p1, p2, nrow = 2)\n    \n    plots\n    \n}\n\n\n\nfn_plots(\"RKB\")"
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#what-if-we-want-more-than-one-argument",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#what-if-we-want-more-than-one-argument",
    "title": "Repeating Yourself with Functions",
    "section": "What if we want more than one argument?",
    "text": "What if we want more than one argument?\nEasy! Just add them to the arguments when you define the function.\nIf I wanted to run this function on multiple dataframes I would change the function to:\n\nfn_plots &lt;- function(df, prov){\n  \n    p1 &lt;- df |&gt; \n      filter(provider_code == prov) \n    # and the rest as before\n}\n\nand run it with fn_plots(new_rtt, \"RKB\").\nNote that the order of entering the parameters is important. If I tried to run fn_plots(\"RKB\", new_rtt) it would look for a dataframe called \"RKB\" and a provider called new_rtt."
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#working-through-a-list-of-parameters",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#working-through-a-list-of-parameters",
    "title": "Repeating Yourself with Functions",
    "section": "Working through a list of parameters",
    "text": "Working through a list of parameters\nAvoid manually running fn_plots() for each provider.\nUse purrr::map to iterate over a list\n\n\n# create a vector of all the providers\nprov_labels &lt;- c(\"RJE\", \"RKB\", \"RL4\", \"RRK\", \"RWE\", \"RX1\")\n\nmap(prov_labels, ~ fn_plots(.x))\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]"
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#troubleshooting---does-the-function-work",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#troubleshooting---does-the-function-work",
    "title": "Repeating Yourself with Functions",
    "section": "Troubleshooting - does the function work?",
    "text": "Troubleshooting - does the function work?\nCrawl before you can walk - make sure fn_plot() works for one parameter.\nInsert browser() into the function while testing - steps into the function (don’t forget to remove it when it works!)\n\n\nThis is a new function that will save each time-trend plot\n\nfn_save_plot &lt;- function(prov){\n  \n    p &lt;- new_rtt |&gt; \n      filter(provider_code == prov) |&gt; \n      ggplot(aes(x = month(rtt_yrmon), y = count)) +\n      geom_col() +\n      su_theme() +\n        theme(legend.position = \"none\") +\n      labs(\n           subtitle = paste0(prov, \" - monthly pattern of new referrals\"))\n    \n    ggsave(paste0(prov, \"_plot.png\"), \n           plot = p)\n  \n}\n\n\n \n\n\n\nCheck out Shannon Pileggi’s slides for more options"
  },
  {
    "objectID": "presentations/2023-09-07_coffee_and_coding_functions/index.html#troubleshooting---does-it-walk-the-walk",
    "href": "presentations/2023-09-07_coffee_and_coding_functions/index.html#troubleshooting---does-it-walk-the-walk",
    "title": "Repeating Yourself with Functions",
    "section": "Troubleshooting - does it walk the walk?",
    "text": "Troubleshooting - does it walk the walk?\nWhen learning to walk, use safely() or possibly() in your walk function - it will indicate if any parameters have failed, rather than just fall down.\n\n\n\n# wrap fn_plots in safely\nsafe_pl &lt;- safely(.f = fn_save_plot)\n\nmap(prov_labels, ~ safe_pl(.x))\n\n\n# wrap fn_plots in possibly\nposs_pl &lt;- possibly(.f = fn_save_plot)\n\nmap(prov_labels, ~ poss_pl(.x))\n\n\nConsole output of wrapping function in possibly"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#ai-could-help-identify-high-risk-heart-patients",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#ai-could-help-identify-high-risk-heart-patients",
    "title": "Identifying patients at risk",
    "section": "AI could help identify high-risk heart patients1",
    "text": "AI could help identify high-risk heart patients1\nThe University of Leeds has helped train an AI system called Optimise, that looked at health records of more than two million people.\n…\nOf those two million records that were scanned, more than 400,000 people were identified as being high risk for the likes of heart failure, stroke and diabetes.\nhttps://www.bbc.co.uk/news/articles/cj620yl96kzo"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#how-it-works",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#how-it-works",
    "title": "Identifying patients at risk",
    "section": "How it works",
    "text": "How it works\n\nThe input: Health records\nHealth records can be structured or unstructured\n\nStructured: can be stored in a table\nUnstructured: can’t be stored in a table, different shapes/sizes (e.g. text, audio, images)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#example-of-structured-data-in-health-records",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#example-of-structured-data-in-health-records",
    "title": "Identifying patients at risk",
    "section": "Example of structured data in health records",
    "text": "Example of structured data in health records\n\n\n\nID\nBMI\nAge\nIMD Decile\nSmoker\nBlood Pressure\n\n\n\n\n1\n17\n49\n3\n1\n110/70\n\n\n2\n25\n67\n1\n1\n129/70\n\n\n3\n20\n39\n8\n0\n140/90\n\n\n4\n28\n81\n6\n0\n130/85\n\n\n5\n29\n41\n4\n0\n120/80\n\n\n\nData is consistent within each column in the table."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#example-of-unstructured-data-in-health-records",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#example-of-unstructured-data-in-health-records",
    "title": "Identifying patients at risk",
    "section": "Example of unstructured data in health records",
    "text": "Example of unstructured data in health records\n\n\n\n\n\n\n\nID\nNotes\n\n\n\n\n1\nShortness of breath\n\n\n2\nPatient attended clinic following one week of fever, vomiting, and abdominal pain.\n\n\n\nThe length of each sentence is different - data not consistent."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#a-simple-approach-to-classifying-data-knn",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#a-simple-approach-to-classifying-data-knn",
    "title": "Identifying patients at risk",
    "section": "A simple approach to classifying data: KNN",
    "text": "A simple approach to classifying data: KNN\n\n\nClustering algorithms like K Nearest Neighbours (KNN) are on the more basic end of the scale, requiring very little computational power.\n\n1\n\nAntti Ajanki AnAj, CC BY-SA 3.0 http://creativecommons.org/licenses/by-sa/3.0/, via Wikimedia Commons"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#a-simple-approach-to-classifying-data-decision-tree",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#a-simple-approach-to-classifying-data-decision-tree",
    "title": "Identifying patients at risk",
    "section": "A simple approach to classifying data: Decision Tree",
    "text": "A simple approach to classifying data: Decision Tree\n1\nhttps://www.researchgate.net/publication/26635430_Using_Machine_Learning_Algorithms_in_Cardiovascular_Disease_Risk_Evaluation"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#there-are-many-different-models-out-there",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#there-are-many-different-models-out-there",
    "title": "Identifying patients at risk",
    "section": "There are many different models out there! 🥴",
    "text": "There are many different models out there! 🥴\n1\nhttps://scikit-learn.org/1.3/tutorial/machine_learning_map/"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#what-makes-a-model-simple-or-complex",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#what-makes-a-model-simple-or-complex",
    "title": "Identifying patients at risk",
    "section": "What makes a model simple or complex?",
    "text": "What makes a model simple or complex?\n\nThere are dozens of different algorithms out there\nEach algorithm has different strengths and weaknesses\nWhat makes a model simple or complex is the amount of computational power required and how much the model needs to “learn” - how many parameters there are"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#is-the-input-or-the-computation-complex",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#is-the-input-or-the-computation-complex",
    "title": "Identifying patients at risk",
    "section": "Is the input or the computation complex?",
    "text": "Is the input or the computation complex?\n“We used UK primary care EHR data from 2,081,139 individuals aged ≥ 30 years…\nWe trained a random forest classifier using age, sex, ethnicity and comorbidities (OPTIMISE).”1\nNadarajah, Ramesh, et al. “Machine learning to identify community-dwelling individuals at higher risk of incident cardio-renal-metabolic diseases and death.” Future Healthcare Journal 11 (2024): 100109. https://www.sciencedirect.com/science/article/pii/S2514664524002212"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#pros-and-cons-of-simple-a.i.-approaches",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#pros-and-cons-of-simple-a.i.-approaches",
    "title": "Identifying patients at risk",
    "section": "Pros and cons of simple “A.I.” approaches",
    "text": "Pros and cons of simple “A.I.” approaches\n\n\nPros:\n\nSimple models are more easily explained\nCan sometimes find new patterns in the data\n\n\nCons:\n\nThe quality of the data determines the quality of the model\nNot able to handle very complex tasks"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-yiwen/index.html#issues-to-look-out-for",
    "href": "presentations/2024-10-10_what-is-ai-yiwen/index.html#issues-to-look-out-for",
    "title": "Identifying patients at risk",
    "section": "🚩 Issues to look out for 🚩",
    "text": "🚩 Issues to look out for 🚩\n\nHow complex is the input, or the computational approach?\nHow is the model’s performance measured?\nDoes the model get updated?\nWhere did the data come from?\nHave issues of bias or ethics been considered?\n\n\n\nContinuously learning, or learning from mistakes vs. snapshot in time"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-is-rap",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-is-rap",
    "title": "RAP",
    "section": "What is RAP",
    "text": "What is RAP\n\na process in which code is used to minimise manual, undocumented steps, and a clear, properly documented process is produced in code which can reliably give the same result from the same dataset\nRAP should be:\n\n\nthe core working practice that must be supported by all platforms and teams; make this a core focus of NHS analyst training\n\nGoldacre review"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-are-we-trying-to-achieve",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-are-we-trying-to-achieve",
    "title": "RAP",
    "section": "What are we trying to achieve?",
    "text": "What are we trying to achieve?\n\nLegibility\nReproducibility\nAccuracy\nLaziness"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-are-some-of-the-fundamental-principles",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#what-are-some-of-the-fundamental-principles",
    "title": "RAP",
    "section": "What are some of the fundamental principles?",
    "text": "What are some of the fundamental principles?\n\nPredictability, reducing mental load, and reducing truck factor\nMaking it easy to collaborate with yourself and others on different computers, in the cloud, in six months’ time…\nDRY"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#the-road-to-rap",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#the-road-to-rap",
    "title": "RAP",
    "section": "The road to RAP",
    "text": "The road to RAP\n\nWe’re roughly using NHS Digital’s RAP stages\nThere is an incredibly large amount to learn!\nConfession time! (everything I do not know…)\nYou don’t need to do it all at once\nYou don’t need to do it all at all ever\nEach thing you learn will incrementally help you\nRemember- that’s why we learnt this stuff. Because it helped us. And it can help you too"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--baseline",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--baseline",
    "title": "RAP",
    "section": "Levels of RAP- Baseline",
    "text": "Levels of RAP- Baseline\n\nData produced by code in an open-source language (e.g., Python, R, SQL).\nCode is version controlled (see Git basics and using Git collaboratively guides).\nRepository includes a README.md file (or equivalent) that clearly details steps a user must follow to reproduce the code\nCode has been peer reviewed.\nCode is published in the open and linked to & from accompanying publication (if relevant).\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--silver",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--silver",
    "title": "RAP",
    "section": "Levels of RAP- Silver",
    "text": "Levels of RAP- Silver\n\nCode is well-documented…\nCode is well-organised following standard directory format\nReusable functions and/or classes are used where appropriate\nPipeline includes a testing framework\nRepository includes dependency information (e.g. requirements.txt, PipFile, environment.yml\nData is handled and output in a Tidy data format\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--gold",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#levels-of-rap--gold",
    "title": "RAP",
    "section": "Levels of RAP- Gold",
    "text": "Levels of RAP- Gold\n\nCode is fully packaged\nRepository automatically runs tests etc. via CI/CD or a different integration/deployment tool e.g. GitHub Actions\nProcess runs based on event-based triggers (e.g., new data in database) or on a schedule\nChanges to the RAP are clearly signposted. E.g. a changelog in the package, releases etc. (See gov.uk info on Semantic Versioning)\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#a-learning-journey-to-get-you-there",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#a-learning-journey-to-get-you-there",
    "title": "RAP",
    "section": "A learning journey to get you there",
    "text": "A learning journey to get you there\n\nCode style, organising your files\nFunctions and iteration\nGit and GitHub\nPackaging your code\nTesting\nPackage management and versioning"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#how-we-can-help-each-other-get-there",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#how-we-can-help-each-other-get-there",
    "title": "RAP",
    "section": "How we can help each other get there",
    "text": "How we can help each other get there\n\nWork as a team!\nCoffee and coding!\nAsk for help!\nDo pair coding!\nGet your code reviewed!\nJoin the NHS-R/ NHSPycom communities"
  },
  {
    "objectID": "presentations/2023-03-09_midlands-analyst-rap/index.html#haca",
    "href": "presentations/2023-03-09_midlands-analyst-rap/index.html#haca",
    "title": "RAP",
    "section": "HACA",
    "text": "HACA\n\nThe first national analytics conference for health and care\nInsight to action!\nJuly 11th and 12th, University of Birmingham\nAccepting abstracts for short and long talks and posters\nAbstract deadline 27th March\nHelp is available (with abstract, poster, preparing presentation…)!"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#intro",
    "href": "presentations/2024-11-21_agile_project_management/index.html#intro",
    "title": "Forged in fire",
    "section": "Intro",
    "text": "Intro\n\nI got a new job two years ago\nLearning ++\nI want to tell the story\nI’ve drawn from three wells:\n\nAgile/ Scrum\nMLOps\nThe skills and experience of those around me\n\nThis talk will not properly define scrum or MLOps"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#the-new-hospital-programme",
    "href": "presentations/2024-11-21_agile_project_management/index.html#the-new-hospital-programme",
    "title": "Forged in fire",
    "section": "The New Hospital Programme",
    "text": "The New Hospital Programme\n\nJanuary 2023: Development phase\nApril-ish: Thinking about deployment\nOctober-ish: Model is on its way to full production and much remains to do"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#operational-mode",
    "href": "presentations/2024-11-21_agile_project_management/index.html#operational-mode",
    "title": "Forged in fire",
    "section": "Operational mode",
    "text": "Operational mode\n\nThe team was growing in November\nThere were two priorities\n\nIncrease bus factor for Tom (which I’ll come back to)\nDeploy the first production ready version"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#the-first-deployed-version",
    "href": "presentations/2024-11-21_agile_project_management/index.html#the-first-deployed-version",
    "title": "Forged in fire",
    "section": "The first deployed version",
    "text": "The first deployed version\n\nDecember-ish: Many needs that were not anticipated\nThis first release kicked off lots of other work\nThe second release kicked off lots of other work\nIt was very hard to do any long term planning\nBy March we were confused and tired (and it was all my fault)"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#scrum",
    "href": "presentations/2024-11-21_agile_project_management/index.html#scrum",
    "title": "Forged in fire",
    "section": "Scrum",
    "text": "Scrum\n\nWe are not doing “proper” scrum\nProduct owner, scrum master, everyone else\nFive week sprints with a one week recovery run between each one\nSprint planning, sprint catchup, sprint retro"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#sprint-retro",
    "href": "presentations/2024-11-21_agile_project_management/index.html#sprint-retro",
    "title": "Forged in fire",
    "section": "Sprint retro",
    "text": "Sprint retro\n\nWhat went well, what could have gone better, and what to improve next time\nLooking at process, not blaming individuals\nRequires maturity and trust to bring up issues, and to respond to them in a constructive way\nShould agree at the end on one process improvement which goes in the next sprint\nWe’ve had some really, really good retros and I think it’s a really important process for a team"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#what-did-scrum-give-the-team",
    "href": "presentations/2024-11-21_agile_project_management/index.html#what-did-scrum-give-the-team",
    "title": "Forged in fire",
    "section": "What did scrum give the team?",
    "text": "What did scrum give the team?\n\nSimultaneous releases of linked repos\nThe team works autonomously in the sprint\nBetter conversations about “no”\nThe planning and retro process improves the team’s processes, not just the code"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#product-owner",
    "href": "presentations/2024-11-21_agile_project_management/index.html#product-owner",
    "title": "Forged in fire",
    "section": "Product owner",
    "text": "Product owner\n\nMy lessson- get out the way\nA better connection between high level and low level planning\nClear release dates and responsibilities\nClear what I should be doing"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#agility",
    "href": "presentations/2024-11-21_agile_project_management/index.html#agility",
    "title": "Forged in fire",
    "section": "Agility",
    "text": "Agility\n\nThis project was agile whether we liked it or not!\nMy 2022 agile definition:\n\nCustomers can’t make up their minds\nIt’s hard to design software all at once\nContinuous delivery keeps customers happy"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#some-highlights-from-the-agile-manifesto",
    "href": "presentations/2024-11-21_agile_project_management/index.html#some-highlights-from-the-agile-manifesto",
    "title": "Forged in fire",
    "section": "Some highlights from The agile manifesto",
    "text": "Some highlights from The agile manifesto\n\n“Welcome changing requirements, even late in development”\n“At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly”\n“Continuous attention to technical excellence and good design enhances agility”\n“Simplicity- the art of maximizing the amount of work not done- is essential”"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#how-scrum-helped",
    "href": "presentations/2024-11-21_agile_project_management/index.html#how-scrum-helped",
    "title": "Forged in fire",
    "section": "How scrum helped",
    "text": "How scrum helped\n\nThose shaping the project wanted to be able to make quick changes- and see the long term plan\nAgility is a mindset, a mode of practice\nIf anything we were actually too agile\nBeing agile is all about being able to review and make decisions frequently\nBut it isn’t about changing what you’re doing all the time\nGood code and good teams are ready to change direction- whether they change or not"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#mlops",
    "href": "presentations/2024-11-21_agile_project_management/index.html#mlops",
    "title": "Forged in fire",
    "section": "MLOps",
    "text": "MLOps\n\nLike DevOps, but for ML 🙂\nDeploying, updating, and monitoring models in production\nThis isn’t a very ML-y project but much of it still applies (not all though)\nWe have a set of customers using a model and we want to push updates to that model monthly\nWe also want to be able to rerun old models (for audit purposes)"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#levels",
    "href": "presentations/2024-11-21_agile_project_management/index.html#levels",
    "title": "Forged in fire",
    "section": "Levels",
    "text": "Levels\n\nLevels of MLOps- 0, 1, 2\nContinuous integration and continuous delivery\nThere are various finicky differences in definition\nI’ll just talk about the bottom, top, and where we are"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#level-0-mlops",
    "href": "presentations/2024-11-21_agile_project_management/index.html#level-0-mlops",
    "title": "Forged in fire",
    "section": "Level 0 MLOps",
    "text": "Level 0 MLOps\n\nYou process your data in a Jupyter notebook\nYou make a model on a Jupyter notebook\nYou give your code to the engineers who attempt to recreate your data and environment"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#level-2-mlops",
    "href": "presentations/2024-11-21_agile_project_management/index.html#level-2-mlops",
    "title": "Forged in fire",
    "section": "Level 2 MLOps",
    "text": "Level 2 MLOps\n\nData is processed in a pipeline\nCode is continously integrated and deployed to production\nDevelopment, staging, and production environments are configured identically"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#where-are-we",
    "href": "presentations/2024-11-21_agile_project_management/index.html#where-are-we",
    "title": "Forged in fire",
    "section": "Where are we?",
    "text": "Where are we?\n\nWe’re sort of level 1-ish\nData pipelines are definitely RAP! (and recently quicker)\nWe have a dev environment\nMerge to main deploys to dev, GitHub release deploys to prod"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#qa-qa-qa",
    "href": "presentations/2024-11-21_agile_project_management/index.html#qa-qa-qa",
    "title": "Forged in fire",
    "section": "QA, QA, QA",
    "text": "QA, QA, QA\n\nQA is super, super, super important\nQA of data in particular- it’s also time consuming (trying to RAP!)\nQA is done at regular staging points\nQA is done continuously"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#other-stuff-i-learned-along-the-way",
    "href": "presentations/2024-11-21_agile_project_management/index.html#other-stuff-i-learned-along-the-way",
    "title": "Forged in fire",
    "section": "Other stuff I learned along the way",
    "text": "Other stuff I learned along the way\n\nIt doesn’t matter who’s right, it matters how people feel\nTransparency isn’t just good for robust code and analysis- it builds relationships\nBus factor, bus factor, bus factor\nDocumentation, documentation, documentation"
  },
  {
    "objectID": "presentations/2024-11-21_agile_project_management/index.html#the-future",
    "href": "presentations/2024-11-21_agile_project_management/index.html#the-future",
    "title": "Forged in fire",
    "section": "The future",
    "text": "The future\n\nOpen source\nMo’ team members, mo’ conscious work on “teaming” and collaboration"
  },
  {
    "objectID": "presentations/2024-10-09-rap-cautionary-tales/index.html#many-moons-ago",
    "href": "presentations/2024-10-09-rap-cautionary-tales/index.html#many-moons-ago",
    "title": "RAP",
    "section": "Many moons ago…",
    "text": "Many moons ago…\n\nI received an FOI request\nIt was clearly sent to multiple trusts\nIt was the second year this request was sent through\nThey provided the figures we sent the year before\nI hadn’t saved my work from the year before\nI wrote the query to answer the request\nChecked to see I got broadly similar results to the previous year"
  },
  {
    "objectID": "presentations/2024-10-09-rap-cautionary-tales/index.html#my-figures-were-way-off",
    "href": "presentations/2024-10-09-rap-cautionary-tales/index.html#my-figures-were-way-off",
    "title": "RAP",
    "section": "My figures were way off",
    "text": "My figures were way off"
  },
  {
    "objectID": "presentations/2024-10-09-rap-cautionary-tales/index.html#what-could-i-have-done-differently",
    "href": "presentations/2024-10-09-rap-cautionary-tales/index.html#what-could-i-have-done-differently",
    "title": "RAP",
    "section": "What could I have done differently?",
    "text": "What could I have done differently?\n\nSaved my Sql query?\nSaved the Excel file along with the query?\nSaved a full set of instructions alongside?\n\n\nIt’s easy not to do these when it’s (seemingly) a super simple request.\n\n\nIf you haven’t got a good structure in your team for where to save these kinds of requests, then does it even help to save them?"
  },
  {
    "objectID": "presentations/2024-10-09-rap-cautionary-tales/index.html#what-would-i-do-differently-now",
    "href": "presentations/2024-10-09-rap-cautionary-tales/index.html#what-would-i-do-differently-now",
    "title": "RAP",
    "section": "What would I do differently now?",
    "text": "What would I do differently now?\n\nIdeally, use something like Quarto (.Qmd) [all one script]\n\nquery the data from the database\ndo any data transformations needed\nsave the results to the required format\n\nsave it in version control with a sensible name (Year/FOI #?)\ndocument what it is doing\nMake this part of our teams strategy for dealing with FOI’s"
  },
  {
    "objectID": "presentations/2024-10-09-rap-cautionary-tales/index.html#another-cautionary-example",
    "href": "presentations/2024-10-09-rap-cautionary-tales/index.html#another-cautionary-example",
    "title": "RAP",
    "section": "Another cautionary example",
    "text": "Another cautionary example\n\nA colleague was the point of contact for one specialty\nThey always produced their reports\nOne month they were off\n☎️ “We need the report for our meeting, now”\nWhat report?"
  },
  {
    "objectID": "presentations/2024-10-09-rap-cautionary-tales/index.html#the-report",
    "href": "presentations/2024-10-09-rap-cautionary-tales/index.html#the-report",
    "title": "RAP",
    "section": "The report…",
    "text": "The report…\n\nAfter a bit of searching I found a couple of different Excel files\nWhich one was the most recent?\nGuessed based on time stamps\nInside the file were three sheets: Sql, Data, Table\nCould I just run the Sql, dump it into Data, and refresh the Table?\nThere was no notes or documentation"
  },
  {
    "objectID": "presentations/2024-10-09-rap-cautionary-tales/index.html#did-i-provide-the-correct-report-for-the-user",
    "href": "presentations/2024-10-09-rap-cautionary-tales/index.html#did-i-provide-the-correct-report-for-the-user",
    "title": "RAP",
    "section": "Did I provide the correct report for the user?",
    "text": "Did I provide the correct report for the user?\n\n🤷‍♂️"
  },
  {
    "objectID": "presentations/2024-10-09-rap-cautionary-tales/index.html#rap-is-cool-and-all-but",
    "href": "presentations/2024-10-09-rap-cautionary-tales/index.html#rap-is-cool-and-all-but",
    "title": "RAP",
    "section": "🥉/🥈/🥇 RAP is cool and all… but",
    "text": "🥉/🥈/🥇 RAP is cool and all… but\n\nThere are so many easy wins that get you most of the way\nConsistently save files\nStore any query and steps to reproduce alongside results\nUse PowerQuery or equivalent to automate data-&gt;Excel\nUse .Rmd/.Qmd documents to do everything in one file\nUtilise git/version control\nblah blah blah, docker, CI/CD…"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#a-note-on-richard-stallman",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#a-note-on-richard-stallman",
    "title": "Open source licensing",
    "section": "A note on Richard Stallman",
    "text": "A note on Richard Stallman\n\nRichard Stallman has been heavily criticised for some of this views\nHe is hard to ignore when talking about open source so I am going to talk about him\nNothing in this talk should be read as endorsing any of his comments outside (or inside) the world of open source"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#the-origin-of-open-source",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#the-origin-of-open-source",
    "title": "Open source licensing",
    "section": "The origin of open source",
    "text": "The origin of open source\n\nIn the 50s and 60s source code was routinely shared with hardware and users were often expected to modify to run on their hardware\nBy the late 1960s the production cost of software was rising relative to hardware and proprietary licences became more prevalent\nIn 1980 Richard Stallman’s department at MIT took delivery of a printer they were not able to modify the source code for\nRichard Stallman launched the GNU project in 1983 to fight for software freedoms\nMIT licence was launched in the late 1980s\nCathedral and the bazaar was released in 1997 (more on which later)"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#what-is-open-source",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#what-is-open-source",
    "title": "Open source licensing",
    "section": "What is open source?",
    "text": "What is open source?\n\nThink free as in free speech, not free beer (Stallman)\n\n\nOpen source does not mean free of charge! Software freedom implies the ability to sell code\nFree of charge does not mean open source! Many free to download pieces of software are not open source (Zoom, for example)\n\n\nBy Chao-Kuei et al. - https://www.gnu.org/philosophy/categories.en.html, GPL, Link"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#the-four-freedoms",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#the-four-freedoms",
    "title": "Open source licensing",
    "section": "The four freedoms",
    "text": "The four freedoms\n\nFreedom 0: The freedom to use the program for any purpose.\nFreedom 1: The freedom to study how the program works, and change it to make it do what you wish.\nFreedom 2: The freedom to redistribute and make copies so you can help your neighbor.\nFreedom 3: The freedom to improve the program, and release your improvements (and modified versions in general) to the public, so that the whole community benefits."
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#cathedral-and-the-bazaar",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#cathedral-and-the-bazaar",
    "title": "Open source licensing",
    "section": "Cathedral and the bazaar",
    "text": "Cathedral and the bazaar\n\nEvery good work of software starts by scratching a developer’s personal itch.\nGood programmers know what to write. Great ones know what to rewrite (and reuse).\nPlan to throw one [version] away; you will, anyhow (copied from Frederick Brooks’s The Mythical Man-Month).\nIf you have the right attitude, interesting problems will find you.\nWhen you lose interest in a program, your last duty to it is to hand it off to a competent successor.\nTreating your users as co-developers is your least-hassle route to rapid code improvement and effective debugging.\nRelease early. Release often. And listen to your customers.\nGiven a large enough beta-tester and co-developer base, almost every problem will be characterized quickly and the fix obvious to someone.\nSmart data structures and dumb code works a lot better than the other way around.\nIf you treat your beta-testers as if they’re your most valuable resource, they will respond by becoming your most valuable resource."
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#cathedral-and-the-bazaar-cont.",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#cathedral-and-the-bazaar-cont.",
    "title": "Open source licensing",
    "section": "Cathedral and the bazaar (cont.)",
    "text": "Cathedral and the bazaar (cont.)\n\nThe next best thing to having good ideas is recognizing good ideas from your users. Sometimes the latter is better.\nOften, the most striking and innovative solutions come from realizing that your concept of the problem was wrong.\nPerfection (in design) is achieved not when there is nothing more to add, but rather when there is nothing more to take away. (Attributed to Antoine de Saint-Exupéry)\nAny tool should be useful in the expected way, but a truly great tool lends itself to uses you never expected.\nWhen writing gateway software of any kind, take pains to disturb the data stream as little as possible—and never throw away information unless the recipient forces you to!\nWhen your language is nowhere near Turing-complete, syntactic sugar can be your friend.\nA security system is only as secure as its secret. Beware of pseudo-secrets.\nTo solve an interesting problem, start by finding a problem that is interesting to you.\nProvided the development coordinator has a communications medium at least as good as the Internet, and knows how to lead without coercion, many heads are inevitably better than one."
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#the-disciplines-of-open-source-are-the-disciplines-of-good-data-science",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#the-disciplines-of-open-source-are-the-disciplines-of-good-data-science",
    "title": "Open source licensing",
    "section": "The disciplines of open source are the disciplines of good data science",
    "text": "The disciplines of open source are the disciplines of good data science\n\nMeaningful README\nMeaningful commit messages\nModularity\nSeparating data code from analytic code from interactive code\nAssigning issues and pull requests for action/ review\nDon’t forget one of the most lazy and incompetent developers you will ever work with is yourself, six months later"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#what-licences-exist",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#what-licences-exist",
    "title": "Open source licensing",
    "section": "What licences exist?",
    "text": "What licences exist?\n\nPermissive\n\nSuch as MIT but there are others. Recommended by NHSX draft guidelines on open source\nApache is a notable permissive licence- includes a patent licence\nIn our work the OGL is also relevant- civil servant publish stuff under OGL (and MIT- it isn’t particularly recommended for code)\n\nCopyleft\n\nGPL2, GPL3, AGPL (“the GPL of the web”)\nNote that the provisions of the GPL only apply when you distribute the code\nAt a certain point it all gets too complicated and you need a lawyer\nMPL is a notable copyleft licence- can combine with proprietary code as long as kept separate\n\nArguments for permissive/ copyleft- getting your code used versus preserving software freedoms for other people\nNote that most of the licences are impossible to read! There is a website to explain tl;dr"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#what-is-copyright-and-why-does-it-matter",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#what-is-copyright-and-why-does-it-matter",
    "title": "Open source licensing",
    "section": "What is copyright and why does it matter",
    "text": "What is copyright and why does it matter\n\nCopyright is assigned at the moment of creation\nIf you made it in your own time, it’s yours (usually!)\nIf you made it at work, it belongs to your employer\nIf someone paid you to make it (“work for hire”) it belongs to them\nCrucially, the copyright holder can relicence software\n\nIf it’s jointly authored it depends if it’s a “collective” or “joint” work\nHonestly it’s pretty complicated. Just vest copyright in an organisation or group of individuals you trust\nGoldacre review suggests using Crown copyright for copyright in the NHS because it’s a “shoal, not a big fish” (with apologies to Ben whom I am misquoting)"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#iceweasel",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#iceweasel",
    "title": "Open source licensing",
    "section": "Iceweasel",
    "text": "Iceweasel\n\nIceweasel is a story of trademark rather than copyright\nDebian (a Linux flavour) had the permission to use the source code of Firefox, but not the logo\nSo they took the source code and made their own version\nThis sounds very obscure and unimportant but it could become important in future projects of ours, like…"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#what-we-have-learned-in-recent-projects",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#what-we-have-learned-in-recent-projects",
    "title": "Open source licensing",
    "section": "What we have learned in recent projects",
    "text": "What we have learned in recent projects\n\nThe huge benefits of being open\n\nTransparency\nWorking with customers\nGoodwill\n\nNonfree mitigators\nDifferent licences for different repos"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#software-freedom-means-allowing-people-to-do-stuff-you-dont-like",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#software-freedom-means-allowing-people-to-do-stuff-you-dont-like",
    "title": "Open source licensing",
    "section": "Software freedom means allowing people to do stuff you don’t like",
    "text": "Software freedom means allowing people to do stuff you don’t like\n\nFreedom 0: The freedom to use the program for any purpose.\nFreedom 3: The freedom to improve the program, and release your improvements (and modified versions in general) to the public, so that the whole community benefits.\nThe code isn’t the only thing with worth in the project\nThis is why there are whole businesses founded on “here’s the Linux source code”\nSo when we’re sharing code we are letting people do stupid things with it but we’re not recommending that they do stupid things with it\nPeople do stupid things with Excel and Microsoft don’t accept liability for that, and neither should we\nThis issue of sharing analytic code and merchantability for a particular purpose is poorly understood and I think everyone needs to be clearer on it (us, and our customers)\nIn my view a world where consultants are selling our code is better than a world where they’re selling their spreadsheets"
  },
  {
    "objectID": "presentations/2024-05-30_open-source-licensing/index.html#open-source-as-in-piano",
    "href": "presentations/2024-05-30_open-source-licensing/index.html#open-source-as-in-piano",
    "title": "Open source licensing",
    "section": "“Open source as in piano”",
    "text": "“Open source as in piano”\n\nThe patient experience QDC project\nOur current project\nOpen source code is not necessarily to be run, but understood and learned from\nBuilding a group of people who can use and contribute to your code is arguably as important as writing it"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#introduction",
    "href": "presentations/2023-03-23_collaborative-working/index.html#introduction",
    "title": "Collaborative working",
    "section": "Introduction",
    "text": "Introduction\n\nThis is definitely an art and not a science\nI do not claim to have all, or even most of, the answers\nHow you use these tools is way more important than the tools themselves\nThis is a culture and not a technique"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#costs",
    "href": "presentations/2023-03-23_collaborative-working/index.html#costs",
    "title": "Collaborative working",
    "section": "Costs",
    "text": "Costs\n\nDelay and time\nStress and disagreement\nCommittee thinking\nLearning and effort"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#benefits",
    "href": "presentations/2023-03-23_collaborative-working/index.html#benefits",
    "title": "Collaborative working",
    "section": "Benefits",
    "text": "Benefits\n\n“From each according to their ability”\nLearning\nReproducibility and reduced truck factor\nFun!"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#github-as-an-organising-principle-behind-work",
    "href": "presentations/2023-03-23_collaborative-working/index.html#github-as-an-organising-principle-behind-work",
    "title": "Collaborative working",
    "section": "GitHub as an organising principle behind work",
    "text": "GitHub as an organising principle behind work\n\nA project is just a set of milestones\nA milestone is just a set of issues\nAn issue is just a set of commits\nA commit is just text added and removed"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#the-repo-owner",
    "href": "presentations/2023-03-23_collaborative-working/index.html#the-repo-owner",
    "title": "Collaborative working",
    "section": "The repo owner",
    "text": "The repo owner\n\nReview milestones\nReview issues\n\nDiscuss the issue on the issue- NOT on email!\n\nReview pull requests and get your pull requests reviewed!"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#asynchronous-communication",
    "href": "presentations/2023-03-23_collaborative-working/index.html#asynchronous-communication",
    "title": "Collaborative working",
    "section": "Asynchronous communication",
    "text": "Asynchronous communication\n\nInvolve others before you pull request\nInvolve others when you pull request\nRead issues!\nComment on issues!\nFile issues- suggestions/ bug reports/ questions\n\nNOT in emails"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#asynchronous-work",
    "href": "presentations/2023-03-23_collaborative-working/index.html#asynchronous-work",
    "title": "Collaborative working",
    "section": "Asynchronous work",
    "text": "Asynchronous work\n\nEvery piece of work has an issues associated with it\nEvery piece of work associated with an issue lives on its own branch\nEvery branch is incorporated to the main repo by a pull request\nEvery pull request is reviewed"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#iteration-and-documentation",
    "href": "presentations/2023-03-23_collaborative-working/index.html#iteration-and-documentation",
    "title": "Collaborative working",
    "section": "Iteration and documentation",
    "text": "Iteration and documentation\n\nAnalyse early, analyse often (using RAPs!)\nWrite down what you did\nWrite down what you did but then changed your mind about\nFavour Quarto/ RMarkdown\n\nClean sessions\nDocumentation and graphics"
  },
  {
    "objectID": "presentations/2023-03-23_collaborative-working/index.html#data-and-.gitignore",
    "href": "presentations/2023-03-23_collaborative-working/index.html#data-and-.gitignore",
    "title": "Collaborative working",
    "section": "Data and .gitignore",
    "text": "Data and .gitignore\n\nYour repo needs to be reproducible but also needs to be safe\nThe main branch should be reproducible by anyone at any time\n\nDocument package dependencies (using renv)\nDocument data loads if the data isn’t in the repo"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#why",
    "href": "presentations/2024-05-16_store-data-safely/index.html#why",
    "title": "Store Data Safely",
    "section": "Why?",
    "text": "Why?\nBecause:\n\ndata may be sensitive\nGitHub was designed for source control of code\nGitHub has repository file-size limits\nit makes data independent from code\nit prevents repetition"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#other-approaches",
    "href": "presentations/2024-05-16_store-data-safely/index.html#other-approaches",
    "title": "Store Data Safely",
    "section": "Other approaches",
    "text": "Other approaches\nTo prevent data commits:\n\nuse a .gitignore file (*.csv, etc)\nuse Git hooks\navoid ‘add all’ (git add .) when staging\nensure thorough reviews of (small) pull-requests"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#what-if-i-committed-data",
    "href": "presentations/2024-05-16_store-data-safely/index.html#what-if-i-committed-data",
    "title": "Store Data Safely",
    "section": "What if I committed data?",
    "text": "What if I committed data?\n‘It depends’, but if it’s sensitive:\n\n‘undo’ the commit with git reset\nuse a tool like BFG to expunge the file from Git history\ndelete the repo and restart 🔥\n\nA data security breach may have to be reported."
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#data-hosting-solutions",
    "href": "presentations/2024-05-16_store-data-safely/index.html#data-hosting-solutions",
    "title": "Store Data Safely",
    "section": "Data-hosting solutions",
    "text": "Data-hosting solutions\nWe’ll talk about two main options for The Strategy Unit:\n\nPosit Connect and the {pins} package\nAzure Data Storage\n\nWhich to use? It depends."
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#a-platform-by-posit",
    "href": "presentations/2024-05-16_store-data-safely/index.html#a-platform-by-posit",
    "title": "Store Data Safely",
    "section": "A platform by Posit",
    "text": "A platform by Posit\n\n\nhttps://connect.strategyunitwm.nhs.uk/"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#a-package-by-posit",
    "href": "presentations/2024-05-16_store-data-safely/index.html#a-package-by-posit",
    "title": "Store Data Safely",
    "section": "A package by Posit",
    "text": "A package by Posit\n\n\nhttps://pins.rstudio.com/"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#basic-approach",
    "href": "presentations/2024-05-16_store-data-safely/index.html#basic-approach",
    "title": "Store Data Safely",
    "section": "Basic approach",
    "text": "Basic approach\ninstall.packages(\"pins\")\nlibrary(pins)\n\nboard_connect()\npin_write(board, data, \"pin_name\")\npin_read(board, \"user_name/pin_name\")"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#live-demo",
    "href": "presentations/2024-05-16_store-data-safely/index.html#live-demo",
    "title": "Store Data Safely",
    "section": "Live demo",
    "text": "Live demo\n\nLink RStudio to Posit Connect (authenticate)\nConnect to the board\nWrite a new pin\nCheck pin status and details\nPin versions\nUse pinned data\nUnpin your pin"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#should-i-use-it",
    "href": "presentations/2024-05-16_store-data-safely/index.html#should-i-use-it",
    "title": "Store Data Safely",
    "section": "Should I use it?",
    "text": "Should I use it?\n\n\n⚠️ {pins} is not great because:\n\nyou should not upload sensitive data!\nthere’s a file-size upload limit\npin organisation is a bit awkward (no subfolders)\n\n\n{pins} is helpful because:\n\nauthentication is straightforward\ndata can be versioned\nyou can control permissions\nthere are R and Python versions of the package"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#what-is-azure-data-storage",
    "href": "presentations/2024-05-16_store-data-safely/index.html#what-is-azure-data-storage",
    "title": "Store Data Safely",
    "section": "What is Azure Data Storage?",
    "text": "What is Azure Data Storage?\nMicrosoft cloud storage for unstructured data or ‘blobs’ (Binary Large Objects): data objects in binary form that do not necessarily conform to any file format.\nHow is it different?\n\nNo hierarchy – although you can make pseudo-‘folders’ with the blobnames.\nAuthenticates with your Microsoft account."
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#authenticating-to-azure-data-storage",
    "href": "presentations/2024-05-16_store-data-safely/index.html#authenticating-to-azure-data-storage",
    "title": "Store Data Safely",
    "section": "Authenticating to Azure Data Storage",
    "text": "Authenticating to Azure Data Storage\n\nYou are all part of the “strategy-unit-analysts” group; this gives you read/write access to specific Azure storage containers.\nYou can store sensitive information like the container ID in a local .Renviron or .env file that should be ignored by git.\nUsing {AzureAuth}, {AzureStor} and your credentials, you can connect to the Azure storage container, upload files and download them, or read the files directly from storage!"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#step-1-load-your-environment-variables",
    "href": "presentations/2024-05-16_store-data-safely/index.html#step-1-load-your-environment-variables",
    "title": "Store Data Safely",
    "section": "Step 1: load your environment variables",
    "text": "Step 1: load your environment variables\nStore sensitive info in an .Renviron file that’s kept out of your Git history! The info can then be loaded in your script.\n.Renviron:\nAZ_STORAGE_EP=https://STORAGEACCOUNT.blob.core.windows.net/\nScript:\nep_uri &lt;- Sys.getenv(\"AZ_STORAGE_EP\")\nTip: reload .Renviron with readRenviron(\".Renviron\")"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#step-1-load-your-environment-variables-1",
    "href": "presentations/2024-05-16_store-data-safely/index.html#step-1-load-your-environment-variables-1",
    "title": "Store Data Safely",
    "section": "Step 1: load your environment variables",
    "text": "Step 1: load your environment variables\nIn the demo script we are providing, you will need these environment variables:\nep_uri &lt;- Sys.getenv(\"AZ_STORAGE_EP\")\napp_id &lt;- Sys.getenv(\"AZ_APP_ID\")\ncontainer_name &lt;- Sys.getenv(\"AZ_STORAGE_CONTAINER\")\ntenant &lt;- Sys.getenv(\"AZ_TENANT_ID\")"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#step-2-authenticate-with-azure",
    "href": "presentations/2024-05-16_store-data-safely/index.html#step-2-authenticate-with-azure",
    "title": "Store Data Safely",
    "section": "Step 2: Authenticate with Azure",
    "text": "Step 2: Authenticate with Azure\n\n\ntoken &lt;- AzureAuth::get_azure_token(\n  \"https://storage.azure.com\",\n  tenant = tenant,\n  app = app_id,\n  auth_type = \"device_code\",\n)\nThe first time you do this, you will have link to authenticate in your browser and a code in your terminal to enter. Use the browser that works best with your @mlcsu.nhs.uk account!"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#step-3-connect-to-container",
    "href": "presentations/2024-05-16_store-data-safely/index.html#step-3-connect-to-container",
    "title": "Store Data Safely",
    "section": "Step 3: Connect to container",
    "text": "Step 3: Connect to container\nendpoint &lt;- AzureStor::blob_endpoint(ep_uri, token = token)\ncontainer &lt;- AzureStor::storage_container(endpoint, container_name)\n\n# List files in container\nblob_list &lt;- AzureStor::list_blobs(container)\nIf you get 403 error, delete your token and re-authenticate, try a different browser/incognito, etc.\nTo clear Azure tokens: AzureAuth::clean_token_directory()"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#interact-with-the-container",
    "href": "presentations/2024-05-16_store-data-safely/index.html#interact-with-the-container",
    "title": "Store Data Safely",
    "section": "Interact with the container",
    "text": "Interact with the container\nIt’s possible to interact with the container via your browser!\nYou can upload and download files using the Graphical User Interface (GUI), login with your @mlcsu.nhs.uk account: https://portal.azure.com/#home\nAlthough it’s also cooler to interact via code… 😎"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#interact-with-the-container-1",
    "href": "presentations/2024-05-16_store-data-safely/index.html#interact-with-the-container-1",
    "title": "Store Data Safely",
    "section": "Interact with the container",
    "text": "Interact with the container\n# Upload contents of a local directory to container\nAzureStor::storage_multiupload(\n  container,\n  \"LOCAL_FOLDERNAME/*\",\n  \"FOLDERNAME_ON_AZURE\"\n)\n\n# Upload specific file to container\nAzureStor::storage_upload(\n  container,\n  \"data/ronald.jpeg\",\n  \"newdir/ronald.jpeg\"\n)"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#load-csv-files-directly-from-azure-container",
    "href": "presentations/2024-05-16_store-data-safely/index.html#load-csv-files-directly-from-azure-container",
    "title": "Store Data Safely",
    "section": "Load csv files directly from Azure container",
    "text": "Load csv files directly from Azure container\ndf_from_azure &lt;- AzureStor::storage_read_csv(\n  container,\n  \"newdir/cats.csv\",\n  show_col_types = FALSE\n)\n\n# Load file directly from Azure container (by storing it in memory)\n\nparquet_in_memory &lt;- AzureStor::storage_download(\n  container, src = \"newdir/cats.parquet\", dest = NULL\n)\n\nparq_df &lt;- arrow::read_parquet(parquet_in_memory)"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#interact-with-the-container-2",
    "href": "presentations/2024-05-16_store-data-safely/index.html#interact-with-the-container-2",
    "title": "Store Data Safely",
    "section": "Interact with the container",
    "text": "Interact with the container\n# Delete from Azure container (!!!)\nAzureStor::delete_storage_file(container, BLOB_NAME)"
  },
  {
    "objectID": "presentations/2024-05-16_store-data-safely/index.html#what-does-this-achieve",
    "href": "presentations/2024-05-16_store-data-safely/index.html#what-does-this-achieve",
    "title": "Store Data Safely",
    "section": "What does this achieve?",
    "text": "What does this achieve?\n\nData is not in the repository, it is instead stored in a secure location\nCode can be open – sensitive information like Azure container name stored as environment variables\nLarge filesizes possible, other people can also access the same container.\nNaming conventions can help to keep blobs organised (these create pseudo-folders)"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#targets-for-analysts",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#targets-for-analysts",
    "title": "Coffee and Coding",
    "section": "{targets} for analysts",
    "text": "{targets} for analysts\n\n\n\nTom previously presented about {targets} at a coffee and coding last March and you can revisit his presentation and learn about the reasons why you should use the package to manage your pipeline and see a simple demonstration of how to use the package.\nMatt has presented previously about {targets} and making your workflows (pipelines) reproducible.\nSo….. if you aren’t really even sure why your pipeline needs managing as an analyst or whether you actually have one (you do) then links to their presentations are at the end"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#aims",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#aims",
    "title": "Coffee and Coding",
    "section": "Aims",
    "text": "Aims\n\nIn this presentation we aim to demonstrate the real-world use of {targets} in an analysis project, but first a brief explanation\n\n\nWithout {targets} we\n\n\nWrite a script\nExecute script\nMake changes\nGo to step 2\n\n\nWith {targets} we will\n\n\nlearn how the various stages of our analysis fit together\nsave time by only running necessary stages as we cycle through the process\nhelp future you and colleagues re-visiting the analysis - Matt says “its like a time-capsule”\nmake Reproducible Analytical Pipelines\n\n\nsource: The {targets} R package user manual"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#explain-the-live-project",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#explain-the-live-project",
    "title": "Coffee and Coding",
    "section": "Explain the live project",
    "text": "Explain the live project\n\noriginal project had 30+ metrics\nmultiple inter-related processing steps\neach time a metric changed or a process was altered it impacted across the project\nthere was potential for mistakes, duplication, lots of wasted time\nusing targets provides a structure that handles these inter-relationships"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#how-targets-can-help",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#how-targets-can-help",
    "title": "Coffee and Coding",
    "section": "How {targets} can help",
    "text": "How {targets} can help\n\ngets you thinking about your analysis and its building blocks\ntargets forces you into a functions approach to workflow\nentire pipeline is reproducible\nvisualise on one page\nsaves time\n(maybe we need an advanced function writing session in another C&C?)"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#demonstration-in-a-live-project",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#demonstration-in-a-live-project",
    "title": "Coffee and Coding",
    "section": "Demonstration in a live project",
    "text": "Demonstration in a live project\nLet’s look at a real life example in a live project…"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#visualising",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#visualising",
    "title": "Coffee and Coding",
    "section": "Visualising",
    "text": "Visualising\nCurrent project in {targets} and visualised with tar_visnetwork()"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#code",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#code",
    "title": "Coffee and Coding",
    "section": "Code",
    "text": "Code\n\nit’s like a recipe of steps\nit’s easier to read\nyou have built functions which you can transfer and reuse\nit’s efficient, good practice\ndebugging is easier because if/when it fails you know exactly which target it has failed on\nit creates intermediate cached objects you can fetch at any time"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#how-can-i-start-using-it",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#how-can-i-start-using-it",
    "title": "Coffee and Coding",
    "section": "How can I start using it?",
    "text": "How can I start using it?\n\nYou could “retro-fit” it to your project, but … ideally you should start your project off using {targets}\nThere are at least three of us in SU who have used it in our projects.\nWe are offering to hand hold you to get started with your next project.\nMatt, Tom, Jacqueline"
  },
  {
    "objectID": "presentations/2024-01-25_coffee-and-coding/index.html#useful-targets-links",
    "href": "presentations/2024-01-25_coffee-and-coding/index.html#useful-targets-links",
    "title": "Coffee and Coding",
    "section": "Useful {targets} links",
    "text": "Useful {targets} links\n\nTom’s previous coffee and coding presentation\nMatt’s previous presentations\nThe {targets} documentation is detailed and easy to follow.\nA demo repository demonstrated in last weeks NHSE C&C\nSoftware Carpentry are developing a course here Pre-alpha targets course\nLive project demonstrated in this presentation using {targets}"
  },
  {
    "objectID": "presentations/2024-09-05_earl-nhp/index.html#the-new-hospital-programme-nhp",
    "href": "presentations/2024-09-05_earl-nhp/index.html#the-new-hospital-programme-nhp",
    "title": "Using R and Python to model future hospital activity",
    "section": "The New Hospital Programme (NHP)",
    "text": "The New Hospital Programme (NHP)\n\n\n\nA manifesto commitment\nFuture activity must be modelled\nNeed consistency across schemes\n\n\n\n\n\n\nBuilding new hospitals - replacing crumbling infrastructure in some cases, completely new builds in others.\nIt’s important to size the hospitals according to the type and quantity of activity there will be in the future.\nThere are many proprietary black box models in use for estimating healthcare activity in the future - no consistency, difficult to compare results\nStrategy Unit was asked to develop a model to be used across all of the builds: a model owned and operated by the NHS, for the NHS.\n\n\n::::"
  },
  {
    "objectID": "presentations/2024-09-05_earl-nhp/index.html#model-process",
    "href": "presentations/2024-09-05_earl-nhp/index.html#model-process",
    "title": "Using R and Python to model future hospital activity",
    "section": "",
    "text": "A probabilistic Monte Carlo simulation that:\n\nTakes hospital activity from a baseline year, using NHS England’s Hospital Episode Statistics (HES) data\nApplies variables that:\n\nare outside of our control (e.g. population changes, using ONS projections)\ncan reduce hospital activity (mitigators, e.g. virtual wards or teleappointments)\n\nForecasts future demand based on these variables, outputting probabilistic predictive intervals"
  },
  {
    "objectID": "presentations/2024-09-05_earl-nhp/index.html#our-challenges",
    "href": "presentations/2024-09-05_earl-nhp/index.html#our-challenges",
    "title": "Using R and Python to model future hospital activity",
    "section": "Our challenges",
    "text": "Our challenges\n\n28 hospitals currently using the model\nModel is being developed whilst in production\nModel is very complex - technically, and for end users\n\n\n\nHospitals are actively using the model while it is still in development, which can be tricky\nDataset is massive for each hospital - hundreds and thousands of rows - all activity for a hospital trust in one year\nModel can accommodate hundreds of different variables, understanding and setting these can be challenging for end users\nWe have comprehensive, openly available documentation and also a team of Model Relationship Managers to help address this"
  },
  {
    "objectID": "presentations/2024-09-05_earl-nhp/index.html#tools-and-platforms",
    "href": "presentations/2024-09-05_earl-nhp/index.html#tools-and-platforms",
    "title": "Using R and Python to model future hospital activity",
    "section": "Tools and platforms",
    "text": "Tools and platforms\n\nData pipelines: {targets} , SQL \nModel: Python , Docker \nApps: {shiny} and {golem} , Posit Connect \nInfrastructure and storage: Azure \nDocumentation: Quarto \nVersion control and collaboration: Git , GitHub \n\n\n\nSo how did we solve the problem?\nHere’s a rundown of the tools and platforms that we use.\nThe data pipeline is orchestrated by {targets} for its recipe-like format and so we re-run only what needs re-running.\nThe model is built in Python and involves a lot of pandas DataFrame manipulations.\nWe use Azure for storage of model input data and JSON files of results.\nUsers input model paramters in one Shiny app and view results in another. This uses modules and {golem} for its package focus, as well as {bs4Dash}. We have development and productino environments.\nWe have a deployed Quarto website that contains the documentation for the whole project.\nIn general, we’re following the principles of Reproducible Analytical Pipelines (RAP) in everything we do.\nAll originally written by Tom.\nAs the team has grown we have shared responsibilities: YiWen in Python, Matt with Shiny, Tom as technical lead."
  },
  {
    "objectID": "presentations/2024-09-05_earl-nhp/index.html#structure",
    "href": "presentations/2024-09-05_earl-nhp/index.html#structure",
    "title": "Using R and Python to model future hospital activity",
    "section": "",
    "text": "This is a simplified overview of the structure and flow of information through the system.\nThe full structure is quite complex, reflecting the complexity of user needs and the scale of the task.\nData from our database is processed and stored in Azure Storage Containers via a targets pipeline. Additional data, like ONS population projections, are also stored.\nThe users interact with a Shiny app to set their input parameters. The app provides some contextual information derived from the data held in Azure. Users click a button to run the model.\nThe model is deployed as a Docker container in Azure Continer Instances, triggered by an API call.\nThe model results are stored as JSON in an Azure container, ready for collection and presentation in an outputs app.\nUsers can view charts and tables and download files for further analysis.\nSo there’s clear front- and backends and we have\nFurther complexity is added by the need to process and present information despite changes to the model over time.\nWe use development and production environments for our apps to help reduce errors."
  },
  {
    "objectID": "presentations/2024-09-05_earl-nhp/index.html#outputs-app",
    "href": "presentations/2024-09-05_earl-nhp/index.html#outputs-app",
    "title": "Using R and Python to model future hospital activity",
    "section": "",
    "text": "Here’s a preview of the outputs app.\nIn the navbar you can see that users can aggregate by hospital sites; view charts and tables; and download results files for further processing.\nThere are also context-specific drodown menus to focus in on certain data. For example, to see results by activity type: inpatients, outpatients or A&E.\nIn this particular tab we can see a beeswarm plot showing each simulation as an individual point. This kind of presentation is important to remind users that the model outputs a distribution; that there are range of possibilities.\nThe data provided here to users is used to drive decisions about the size of hospital that will be developed."
  },
  {
    "objectID": "presentations/2024-09-05_earl-nhp/index.html#next",
    "href": "presentations/2024-09-05_earl-nhp/index.html#next",
    "title": "Using R and Python to model future hospital activity",
    "section": "Next",
    "text": "Next\n\nForecast regionally and nationally\nMove data and pipelines into Databricks\nOpen-source model code\n\n\n\nWe’re currently working with hospitals and trusts, but we’re also expanding the geographical scale to produce results at the regional and national scale. This will require some thinking around processing, modelling and generating outputs.\nWe’re currently transferring data processing into Databricks, partly to bring all the steps into one platform but also as an opportunity to speed up the processing by using Spark.\nFinally, we already have some aspects in the open, like the project information site, but we’d also like to open-source the model code itself so that others can use and develop it."
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#patient-experience",
    "href": "presentations/2023-05-15_text-mining/index.html#patient-experience",
    "title": "Text mining of patient experience data",
    "section": "Patient experience",
    "text": "Patient experience\n\nThe NHS collects a lot of patient experience data\nRate the service 1-5 (Very poor… Excellent) but also give written feedback\n\n“Parking was difficult”\n“Doctor was rude”\n“You saved my life”\n\nMany organisations lack the staffing to read all of the feedback in a systematic way"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#text-mining",
    "href": "presentations/2023-05-15_text-mining/index.html#text-mining",
    "title": "Text mining of patient experience data",
    "section": "Text mining",
    "text": "Text mining\n\nWe have built an algorithm to read it\n\nTheme\n“Criticality”\n\nFits alongside other work happening within NHSE\n\nA framework for understanding patient experience"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#patient-experience-101",
    "href": "presentations/2023-05-15_text-mining/index.html#patient-experience-101",
    "title": "Text mining of patient experience data",
    "section": "Patient experience 101",
    "text": "Patient experience 101\n\nTick box scoring is not useful (or accurate)\nText based data is complex and built on human experience\nWe’re not making word clouds!\nWe’re not classifying movie reviews or Reddit posts\nThe tool should enhance, not replace, human understanding\n“A recommendation engine for feedback data”"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#everything-open-all-the-time",
    "href": "presentations/2023-05-15_text-mining/index.html#everything-open-all-the-time",
    "title": "Text mining of patient experience data",
    "section": "Everything open, all the time",
    "text": "Everything open, all the time\n\nThis project was coded in the open and is MIT licensed\nEngage with the organisations as we find them\n\nDo they want code or a docker image?\nDo they want to fetch their own themes from an API?\nDo they want to use our dashboard?"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#phase-1",
    "href": "presentations/2023-05-15_text-mining/index.html#phase-1",
    "title": "Text mining of patient experience data",
    "section": "Phase 1",
    "text": "Phase 1\n\n10 categories and moderate performance on criticality analysis\nscikit-learn\nShiny\nReticulate\nR package of Python code"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#golem-all-the-things",
    "href": "presentations/2023-05-15_text-mining/index.html#golem-all-the-things",
    "title": "Text mining of patient experience data",
    "section": "Golem all the things!",
    "text": "Golem all the things!\n\nOpinionated way of building Shiny\nAllows flexibility in deployed versions using YAML\nAgnostic to deployment\nEmphasises dependency management and testing\nSeparate “reactive” and “business” logic (see the accompanying book)"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#phase-2",
    "href": "presentations/2023-05-15_text-mining/index.html#phase-2",
    "title": "Text mining of patient experience data",
    "section": "Phase 2",
    "text": "Phase 2\n\n30-50 categories and excellent criticality performance\nscikit-learn/ BERT\nMore Shiny\nSeparate the code bases\nFastAPI\nInspired by the Royal College of Paediatrics and Child Health API\nDocumentation, documentation, documentation"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#making-it-useful",
    "href": "presentations/2023-05-15_text-mining/index.html#making-it-useful",
    "title": "Text mining of patient experience data",
    "section": "Making it useful",
    "text": "Making it useful\n\nAccurately rating low frequency categories\nPer category precision and recall\nSpeed versus accuracy\nRepresenting the thematic structure"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#the-future",
    "href": "presentations/2023-05-15_text-mining/index.html#the-future",
    "title": "Text mining of patient experience data",
    "section": "The future",
    "text": "The future\n\nOff the shelf, proprietary data collection systems dominate\nThey often offer bundled analytic products of low quality\nThe DS time can’t and doesn’t want to offer a complete data system\nHow can we best contribute to improving patient experience for patients in the NHS?\n\nIf the patient experience data won’t come to the mountain…"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#open-source-ftw",
    "href": "presentations/2023-05-15_text-mining/index.html#open-source-ftw",
    "title": "Text mining of patient experience data",
    "section": "Open source FTW!",
    "text": "Open source FTW!\n\nOften individuals in the NHS don’t want private companies to “benefit” from open code\nBut if they make their products better with open code the patients win\nBest practice as code"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#the-projects",
    "href": "presentations/2023-05-15_text-mining/index.html#the-projects",
    "title": "Text mining of patient experience data",
    "section": "The projects",
    "text": "The projects\n\nhttps://github.com/CDU-data-science-team/pxtextmining\nhttps://github.com/CDU-data-science-team/experiencesdashboard\nhttps://github.com/CDU-data-science-team/PatientExperience-QDC"
  },
  {
    "objectID": "presentations/2023-05-15_text-mining/index.html#the-team",
    "href": "presentations/2023-05-15_text-mining/index.html#the-team",
    "title": "Text mining of patient experience data",
    "section": "The team",
    "text": "The team\n\nYiWen Hon (Python & Machine learning)\nOluwasegun Apejoye (Shiny)\n\nContact:\n\nchris.beeley1@nhs.net\nhttps://fosstodon.org/@chrisbeeley"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#the-new-hospital-programme-nhp-demand-model",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#the-new-hospital-programme-nhp-demand-model",
    "title": "Data science in the UK NHS",
    "section": "The New Hospital Programme (NHP) Demand Model",
    "text": "The New Hospital Programme (NHP) Demand Model\n\nDiagram of model process"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#section-1",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#section-1",
    "title": "Data science in the UK NHS",
    "section": "",
    "text": "Here’s a simplified, zoomed-in version.\nHES data from the database is used to make the inputs data, which is pulled in to the inputs app.\nThat sends information to the model, which creates results that are sent back to storage, and from which the ourputs app does its thing.\nBy why did we choose this setup? It seems complicated and busy."
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#principles",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#principles",
    "title": "Data science in the UK NHS",
    "section": "Principles",
    "text": "Principles\n\nDeploy alongside develop\nReproducible analytical pipelines\nTransparent\nOpen (FOSS where possible)\nTeam skills and work management"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#tools-and-platforms",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#tools-and-platforms",
    "title": "Data science in the UK NHS",
    "section": "Tools and platforms",
    "text": "Tools and platforms\n\nData pipelines: , parquet, CSV\nModel: Python , Docker \nApps: {shiny} and {golem} , Posit Connect \nInfrastructure and storage: Azure \nDocumentation: Quarto \nVersion control and collaboration: Git , GitHub"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#data-for-nhp",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#data-for-nhp",
    "title": "Data science in the UK NHS",
    "section": "Data for NHP",
    "text": "Data for NHP\n\n\n\nHES data stored on Azure\nDatabricks and PySpark\nCSVs and TSVs for reference data\nparameters derived and stored in JSON"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#model-running",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#model-running",
    "title": "Data science in the UK NHS",
    "section": "Model running",
    "text": "Model running\nParameters JSON passed to model via API\n\nDocker image stored on Azure Container Registry\nRuns in Azure Container Instance\nBuilt-in paralellisation features of Python language"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#interfaces",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#interfaces",
    "title": "Data science in the UK NHS",
    "section": "Interfaces",
    "text": "Interfaces\n\n\nCoded in R - Shiny Open repos: nhp_inputs and nhp_outputs\nModular design\n\nEach function is in its own .R\nEach module is separate\nUIs and servers are separate\nPackaged using {golem} to use R CMD check(), devtools::document() etc"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#outputs",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#outputs",
    "title": "Data science in the UK NHS",
    "section": "Outputs",
    "text": "Outputs\nMany formats:\n\nCSV\nJSON\nMS Word (using {officeR})\nOutputs app\nProject information Quarto website\n\nWe use a _brand.yml to help easily maintain style format of outputs (Quarto and Shiny).\n\n\nThe main way information flows out of the NHP model is via the outputs app, which also provides the functionality to download results into a spreadsheet software of your choice through CSV files.\nThe JSON files we’ve discussed are also produced and stored and users can ask us for them if useful\nWe produce templated reports in Microsoft Word for schemes, allowing them the comfort and accessibility of familiar software without greatly sacrificing the programmatic and RAP-compliant nature of our work.\nAnd our Quarto website holding the project information is a key tool for dissemination of info. Its repo is open, so you can go and look at all the code used to create it.\nIn essence we will aim to provide outputs that best meet the needs of our model users, to maximise the impact of the modelling and communicate the uncertainty around estimates as clearly as we can.\nWe also make use of ‘brand.yml’ to be consistent in the appearance of our NHP outputs. You can see this yaml file on our github, with an informative README that contains useful links to guidance around branding and usage."
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#reproducible-analytical-pipeline-rap-principles",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#reproducible-analytical-pipeline-rap-principles",
    "title": "Data science in the UK NHS",
    "section": "Reproducible Analytical Pipeline (RAP) principles",
    "text": "Reproducible Analytical Pipeline (RAP) principles\nData versioning\nData and model follow the same semantic versioning1.\n\n\n\nNow we’re moving into ever-mode nerdy territory. You’ll all be familiar with RAP, what it means and why it matters.\nWe as a team are fully on board with those principles of transparency, quality, sharing and continuous improvement.\nOne way we do this is through version control - not only of the code, but of the data too.\nWe keep a public log of all changes to the model, viewable at the link below on our project information site, which is a Quarto website whose deployment is handled by GitHub Actions.\nThe code is versioned semantically, like usual software is, following the major-minor-patch naming convention. We are currently on version 3.4 of the model\n\n\n\n\nData Log\n\n\nOn another page of the Quarto website, we have a data log. As you can see, the data are named the same as the model versions, and we have history here going right back to the begining of the model lifespan.\nRelease notes are linked to and held on the nhp_data repository, which is also open source. Lets follow one of these release notes links to see the kinds of information stored.\n\n\n\n\nThe Strategy Unit GitHub\n\n\nHere in GitHub you can see the release notes for the current version of the model, v3.4.0. These notes are partly auto-generated by GitHub through collating the pull request comments on the elements merged to create this release, but we also might overwrite or simplify some things.\nFor instance we can see that Tom merged PR number 89 that replace the AEC mitigators with the new SDEC ones. I can click on this link to go to that merged PR and see all the details, timeline and conversations around it.\nThe full changelog also contains all the commits relevant to this new model version.\n\n\nModel Updates"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#rap",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#rap",
    "title": "Data science in the UK NHS",
    "section": "RAP",
    "text": "RAP\n\nModularised code\nStyling and Linting\nOPEN source code\nVersion control\nEnvironment management:\nDocker\n{renv}\nconda"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#rap-continued",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#rap-continued",
    "title": "Data science in the UK NHS",
    "section": "RAP continued",
    "text": "RAP continued\n\nDocumentation\n\nProject Information\nREADMEs on GitHub\nDocstrings, packaged apps (golem)\n\nJSON schema {✔❌} for validating the parameters passed to the model"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#deployment-cicd",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#deployment-cicd",
    "title": "Data science in the UK NHS",
    "section": "Deployment (CI/CD)",
    "text": "Deployment (CI/CD)\nHow do we maintain clean, safe, working code, centrally, when we have open repositories and up to 10 people collaborating on maintaining that code alongside its active deployment?\n\n\nContinuous Integration\nAutomated checks, tests when merging code into main\n\nOn pull request submission\nOn merge to main\n\n\n\nContinuous Deployment\nAutomated checks and tests when deploying (to dev or to prod)\n\nOn merge to main\nOn release"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#github-actions",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#github-actions",
    "title": "Data science in the UK NHS",
    "section": "GitHub Actions",
    "text": "GitHub Actions\nActions like:\n\nFormatting ({Air}) to pick up stylistic inconsistencies\nLinting - for logical, syntactic and stylistic issues\nRendering the README.Rmd\nPackage checks (benefits of creating a Shiny app as a package)\nDeploys to a ‘preview’ site\nAssess code coverage ({codecov})"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#how-we-work",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#how-we-work",
    "title": "Data science in the UK NHS",
    "section": "How we work",
    "text": "How we work\nWe are AGILE, and use Scrum (light)\n\n3 week sprints with 1 week fallow\nWeekly sprint catch-up meetings\nKickoff and retro\nPromote T-shape expertise while reducing ‘bus-factor’\nTransparent prioritisation processes\nDistinct team roles"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#section-8",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#section-8",
    "title": "Data science in the UK NHS",
    "section": "",
    "text": "Scrum master : keeping the sprint on track\nProduct owner : steer work towards the goals\nProject director : overall responsibility for delivery\nDevelopment board : define the goals and priorities\nQA : oversee quality\n\n\n\n\n\n\n\nTip\n\n\nRoles have enough specificity to provide clarity, but are also shared. Flat management structure."
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#agile-and-scrum-in-github",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#agile-and-scrum-in-github",
    "title": "Data science in the UK NHS",
    "section": "Agile and Scrum in GitHub",
    "text": "Agile and Scrum in GitHub\nLeverage a LOT of GitHub’s excellent tooling\n\n\n\nProjects\nIssues with bespoke labelling\nBranch protection rules\nCODEOWNERS\nClear and consistent collaboration guidance\nChecklists"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#codeowners",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#codeowners",
    "title": "Data science in the UK NHS",
    "section": "CODEOWNERS",
    "text": "CODEOWNERS\nA simple but powerful idea!"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#product-team",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#product-team",
    "title": "Data science in the UK NHS",
    "section": "Product Team",
    "text": "Product Team\n‘The model’ is a product - it has current and potential use cases and user groups.\nWe need a team responsible for understanding the software business  as well as the software product 🚀.\n“What should we build next and why?”"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#nhs-r",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#nhs-r",
    "title": "Data science in the UK NHS",
    "section": "NHS-R",
    "text": "NHS-R\n\nI learned R in 2009 in the bad old days\nI swore that I’d make sure that others didn’t suffer like I did\nNHS-R is the fulfilment of this promise, created in 2018"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#core-values-of-open-source",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#core-values-of-open-source",
    "title": "Data science in the UK NHS",
    "section": "Core values of open source",
    "text": "Core values of open source\n\nTransparency\nCollaboration\nRelease early and often\nInclusive meritocracy\nCommunity\nWork across organisational boundaries (obviously)"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#core-values",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#core-values",
    "title": "Data science in the UK NHS",
    "section": "Core values",
    "text": "Core values\n\nFlat hierarchy\nSharing\nCooperate across organisational boundaries\nWe cooperate across international boundaries\nWe love beginners\nWe make mistakes and learn together"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#what-is-nhs-r",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#what-is-nhs-r",
    "title": "Data science in the UK NHS",
    "section": "What is NHS-R?",
    "text": "What is NHS-R?\n\nCulture &gt; Strategy\nDoing &gt; Talking\nNHS-R is your permission to work your way\nNobody ever asks us to do our best work\n\nNHSRplotthedots\n\nWe know what to build, we know what to learn\n“Computer (department) says no”"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#open-source",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#open-source",
    "title": "Data science in the UK NHS",
    "section": "Open source",
    "text": "Open source\n\nWe believe in open source\nAll NHS R solutions are open source\nWe teach git and GitHub and encourage organisations to share their code\nWe build stuff together because we believe in the value of the community"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#no-such-thing-as-a-free-lunch",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#no-such-thing-as-a-free-lunch",
    "title": "Data science in the UK NHS",
    "section": "No such thing as a free lunch",
    "text": "No such thing as a free lunch\n\nWe believe in R, we believe in the NHS-R community, and we believe in each other\nNHS-R is not a LinkedIn certification\nIt cannot be bought, sold, or exchanged\nYou can’t buy a community\nBut you can and must buy the glue that binds them together"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#force-multiplier",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#force-multiplier",
    "title": "Data science in the UK NHS",
    "section": "Force multiplier",
    "text": "Force multiplier\n\nCode is a force multiplier\nWickham, 2014 https://bit.ly/3jQ5SuJ\nSo is a community\nNHS-R is absurdly cheap and its ROI is absurdly high\nNHS-R is making people happy and productive\nNHS-R is changing the lives of its members and improving healthcare for everyone in the UK"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#whats-the-connection-between-the-nhp-model-and-nhs-r",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#whats-the-connection-between-the-nhp-model-and-nhs-r",
    "title": "Data science in the UK NHS",
    "section": "What’s the connection between the NHP model and NHS-R?",
    "text": "What’s the connection between the NHP model and NHS-R?\n\nWe’re not perfect but we’ve done the right thing\n\nOpen code\nOpen technologies\nStandard datasets\nDocumentation\nModularity"
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#free-as-in-piano",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#free-as-in-piano",
    "title": "Data science in the UK NHS",
    "section": "“Free as in piano”",
    "text": "“Free as in piano”\n\nThere are huge obstacles for other teams in using and contributing to the code\nSome are inherent- e.g. access to data\nBut across the system people lack access to:\n\nSkills\nSoftware installs (just Python!)\n“Kit” - cloud compute, Posit Connect, etc."
  },
  {
    "objectID": "presentations/2025-06-06_nhsr_nhp/index.html#how-can-nhs-r-help",
    "href": "presentations/2025-06-06_nhsr_nhp/index.html#how-can-nhs-r-help",
    "title": "Data science in the UK NHS",
    "section": "How can NHS-R help?",
    "text": "How can NHS-R help?\n\nNHS-R showcases the benefits of RAP\nNHS-R demystifies the “risks” to IT departments who refuse to install R/ Python\nNHS-R gives people the training and the community to learn things together\nNHS-R loves beginners but NHS-R also shows off the best data science going on in the NHS"
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#tldr",
    "href": "presentations/2024-05-23_github-team-sport/index.html#tldr",
    "title": "GitHub as a team sport",
    "section": "tl;dr",
    "text": "tl;dr\n\n\n\n‘Quality’ isn’t just good code\nTeamwork makes the dream work\nGitHub is a communication tool\n\n\n\n\n\n\n‘Too long; didn’t read’.\nGitHub isn’t just a dumping ground for code and Git history.\nIt’s a platform for working with teammates to get things done.\nQuality is improved by good communication, organisation and reduction of something called the ‘bus factor’ that I’ll get to in a minute."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#the-strategy-unit-su",
    "href": "presentations/2024-05-23_github-team-sport/index.html#the-strategy-unit-su",
    "title": "GitHub as a team sport",
    "section": "The Strategy Unit (SU)",
    "text": "The Strategy Unit (SU)\n\n\n\nAn ‘internal consultancy’\nHosted by NHS Midlands and Lancashire\nGrowing in size and reputation\n\n\n\n\n\n\nInitially a ‘start-up’ style operation that has expanded to 70+ staff.\n‘We produce high-quality, multi-disciplinary analytical work – and we help people apply the results.’\nA lot of our work is on the important New Hospital Programme (NHP).\n‘Our proposition is simple: better evidence, better decisions, better outcomes.’\nExpansion is tricky; how can we maintain quality?"
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#the-data-science-team",
    "href": "presentations/2024-05-23_github-team-sport/index.html#the-data-science-team",
    "title": "GitHub as a team sport",
    "section": "The Data Science Team",
    "text": "The Data Science Team\n     \n\nExpanded to 6, all remote\nModelling, Quarto, Shiny\nNew Hospital Programme (NHP)\n\n\n\nA new team, expanding rapidly from 2 to 6 in about a year.\nRemote across England.\nExperience from across the NHS and consultancy. I spent a decade in five central government departments before this.\nWe’re helping to model and design apps for the NHP to help build hospitals.\nSo: growing team, different experiences, important work, but few standardised processes. What to do?"
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#github-at-the-su",
    "href": "presentations/2024-05-23_github-team-sport/index.html#github-at-the-su",
    "title": "GitHub as a team sport",
    "section": "GitHub at the SU",
    "text": "GitHub at the SU\n\n\nWe should be exemplars\nAiming for open by default\nGitHub is on the homepage and there’s a Data Science site\n\n\n\nIt’s not just the DS team.\nWe have many other analysts eager to learn and contribute.\nHow can we set good standards and encourage use across the organisation?\nWe’re running Coffee & Coding sessions, teaching and encouraging talks and blogs on our site.\nWe want to drive up quality by making code open too.\nIt’s a statement of intent that the SU homepage links to our GitHub organisation."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#what-this-is",
    "href": "presentations/2024-05-23_github-team-sport/index.html#what-this-is",
    "title": "GitHub as a team sport",
    "section": "What this is",
    "text": "What this is\n\nLow-tech, no code\nTips and etiquette, not directives\nWhat’s been working for us\n\n\n\nBut this is not a technical talk about how to use Git for version control.\nMostly it’s about planning, workflows, standards and communication.\nIt’s things that our team have been doing and the ideas are evolving.\nI’ve worked mostly alone on GitHub projects in my career and never worked in a data science team of even this size. So at worst these slides are a way for me to write down what I’m learning."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#the-bus-factor",
    "href": "presentations/2024-05-23_github-team-sport/index.html#the-bus-factor",
    "title": "GitHub as a team sport",
    "section": "The ‘bus factor’ 🚍",
    "text": "The ‘bus factor’ 🚍\n\nWe should maintain quality\nWe need redundancy\nStandardised processes can help\n\n\n\nWhy do we care about discussing and ‘formalising’ these ideas?\nWe should encourage standard practices in case someone is ill or away.\nThis also makes it easier when new team members join.\nThis helps us maintain quality."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#rules",
    "href": "presentations/2024-05-23_github-team-sport/index.html#rules",
    "title": "GitHub as a team sport",
    "section": "‘Rules’",
    "text": "‘Rules’\n\nIt’s the spirit that counts\nDo as I say, not as I do\nKnow why you’re breaking the rules\n\n\n\nTo be clear though, nothing here is etched into stone.\nThere will be times where rules can be broken.\nBut we shouldn’t be complacent."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#github-flow",
    "href": "presentations/2024-05-23_github-team-sport/index.html#github-flow",
    "title": "GitHub as a team sport",
    "section": "GitHub flow",
    "text": "GitHub flow\n\nCreate a repository\nWrite issues\nPlan\nCreate a branch\nMake a pull request\nReview\nRelease\n\n\n\nThis is a fairly generic GitHub flow.\nI’ll talk through a few things in each of these categories."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#repositories",
    "href": "presentations/2024-05-23_github-team-sport/index.html#repositories",
    "title": "GitHub as a team sport",
    "section": "Repositories",
    "text": "Repositories\n\nAssign ‘owner’ and ‘deputy’ roles\nAdd README and .gitignore\nStore data elsewhere\n\n\n\nEasy starter: tell people what the purpose of the repo is and how to use it. This is what a README is for. This is an absolute must to lower the bus factor.\nWe should be prevent accidental file upload immediately. Use a .gitignore to exclude likely data files (as well as other unnecessary files). We’re thinking about common templates/cookiecutters.\nCommunicative files (README, .gitignores) are good, but so is vigilance (code review).\nOwners/deputies are in charge of ‘GitHub gardening’ (keeping issues in order, labelling, milestones, etc).\nDeputies help with bus factor.\nThe owner can be auto-selected as the reviewer. We’re experimenting with this for repos with external contributors, especially.\nData is stored elsewhere, on Azure or Posit Connect, due to sensitivity and size. This should be planned before you begin and recorded in the README."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#issues",
    "href": "presentations/2024-05-23_github-team-sport/index.html#issues",
    "title": "GitHub as a team sport",
    "section": "Issues",
    "text": "Issues\n\n\n\nAren’t just ‘problems’\nUse labels, including MoSCoW\nExplain the need, be informative\n\n\n\n\n\n\nIssues can be reminders or questions for further discussion, not just features to build.\nTickets should get two labels. We use a topic like ‘enhancement’, ‘bug’, ‘documentation’, ‘techdebt’, etc, plus MoSCoW (must, should, could, won’t) to help prioritisation.\nIssue templates can ensure certain info is provided, which is especially good for external contributors.\nRefer to other related commits by number (e.g. #1), which stops you repeating the same information.\nPrefer to reopen an issue if it doesn’t actually work.\nIssues can track separate sub-issues.\nYou can add checklists with markdown checkbox: - [ ] (these appear in the issue preview).\nYou can ‘hide’ comments if they’re out of date, etc."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#plan",
    "href": "presentations/2024-05-23_github-team-sport/index.html#plan",
    "title": "GitHub as a team sport",
    "section": "Plan",
    "text": "Plan\n\n\nTalk, review and reflect\nUse labels to prioritise\nSort into milestones\n\n\n\nWe have a repo and issues, what do we do now? Where to start?\nWe’ve begun working in sprints of about 4 weeks. We have sprint planning meetings to plan things out.\nConsider what needs to be done in the sprint period, what other issues support those goals?\nIs there time for other tasks, like clearing techdebt?\nAll issues should be assigned to a milestone.\nIssues in milestones should be sorted in priority order/order of expected completion (MoSCoW labels will help with this).\nThis helps focus the goals of the sprint and keep us on track."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#branches",
    "href": "presentations/2024-05-23_github-team-sport/index.html#branches",
    "title": "GitHub as a team sport",
    "section": "Branches",
    "text": "Branches\n\n\nOne issue, one branch, one assigned person\nName them sensibly\nBurn them\n\n\n\nOnly one person works on a branch at a time. This person is the one assigned to the relevant issue.\nBranch names should be numbered to match their issue, e.g. ‘123-add-filter’. This makes it obvious what issue is being fixed by that branch and should help identify if more than one person has a branch open for the same issue.\nIf commits from someone else are required, then all parties must communicate about the current state of the branch to ensure they pull changes and avoid merge conflicts.\nBranches are ephemeral and die when the PR is merged. They should be deleted (this can be done automatically).\nThe only branches to exist at all times should be main and a deployment branch, if necessary. All others should be active branches so it’s clear what’s being worked on."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#commits",
    "href": "presentations/2024-05-23_github-team-sport/index.html#commits",
    "title": "GitHub as a team sport",
    "section": "Commits",
    "text": "Commits\n\n\nDon’t commit to main!\n‘Small, early and often’\nMake messages meaningful\n\n\n\nThere’s not a lot of earth-shattering advice to give here; this stuff is fairly standard.\nDo not commit directly to main. Your work must be independently checked first to limit the chance of mistakes.\nMake your commits small in terms of code and files touched, if possible. This makes the Git history easier to read and makes reviews easier too.\nCommit and push early and often into your branch. This can help others see progress and helps reduce the bus factor.\nDon’t dump your work into a commit because it’s the end of the day.\nMake your commit messages meaningful. What does the commit do? Start with a verb in present tense (‘adds’, not ‘added’). Or maybe use ‘conventional’ commits."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#pull-requests-prs",
    "href": "presentations/2024-05-23_github-team-sport/index.html#pull-requests-prs",
    "title": "GitHub as a team sport",
    "section": "Pull requests (PRs)",
    "text": "Pull requests (PRs)\n\n\n\nSmall and closes an issue\nSelect the assignee and reviewer\nThe assignee merges\n\n\n\n\n\n\nPRs should solve the issue they’re related to. Occasionally one fix may solve another.\nThey should be named to explain what they do. The issue might be ‘the red button doesn’t work’; the PR might be ‘fix the red button’.\nThey should be small in terms of lines of code and files touched. This will make it easier and faster to understand and assess the changes.\nThe submitter should mark themself as the ‘assignee’ and choose a reviewer. You may want to chat with the reviewer to let them know if they have time.\nFor context, link to the issue(s) being closed with the magic words (‘closes’, ‘fixes’, etc), which will also close those issues as completed.\nInclude a short explanation or bullet-points of what the PR does. Provide any extra information to make the reviewer’s life easier (areas of focus, maybe) or to ask a question about some aspect of what you’ve written.\nThe PR submitter is the one who clicks the merge button. This is in case the submitter realises there’s something they need to add or change before the merge."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#reviewing-prs",
    "href": "presentations/2024-05-23_github-team-sport/index.html#reviewing-prs",
    "title": "GitHub as a team sport",
    "section": "Reviewing PRs",
    "text": "Reviewing PRs\n\n\n\nBe helpful, be kind\nUse GitHub suggestions\nDiscuss if unclear\n\n\n\n\n\n\nThe reviewer should typically check that the changes result in the issue being fixed. This may require pulling the branch and then testing it, but may not be necessary for small changes.\nThe reviewer should seek clarification and add comments where something isn’t clear.\nUse ‘suggestions’ as a reviewer rather than committing to someone else’s branch.\nWhen working at pace (when aren’t we?), we should err towards approval if the issue is completed rather than an endless cycle of asking for small changes. The submitter and reviewer should decide whether smaller things like code style or change in approach should be added as a new issue with a ‘techdebt’ label."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#releases",
    "href": "presentations/2024-05-23_github-team-sport/index.html#releases",
    "title": "GitHub as a team sport",
    "section": "Releases",
    "text": "Releases\n\nUse semantic versioning (1.2.3)\nAutofill notes with PR names\nDon’t release on a Friday 🙃\n\n\n\nTag the history and release on GitHub concurrently to keep them in sync (this is done automatically if the release is done from the GitHub interface).\nSemantic (x.y.z where x is breaking, y is new features and z is patches for bugs).\nWe typically just autofill the release description with the constituent PR titles. Which means it’s important to give them meaningful names.\nWe align releases with sprints, though patches may occur more frequently.\nWe link releases to deployment in many cases. Don’t release to prod on a Friday, lol."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#github-is-a-team-member",
    "href": "presentations/2024-05-23_github-team-sport/index.html#github-is-a-team-member",
    "title": "GitHub as a team sport",
    "section": "GitHub is a team member",
    "text": "GitHub is a team member\n\n\nAutomate with Actions\nProvide issue and repo templates\nAn all-in-one planner?\n\n\n\nI lied: we have 6 human team members. GitHub itself has features that can automate away some boring things and help prevent accidents or forgetfulness.\nGitHub Actions for continuous integration. R-CMD check at least for R projects. Start with r-lib examples as a basis.\nWe’re looking towards things like templates at the issue and repo levels; again to remove drudgery.\nWe use Trello to plan things and have to link to GitHub repos and issues in Trello cards. Can we use GitHub as our planner across multiple repos instead? Seems possible."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#are-we-curling",
    "href": "presentations/2024-05-23_github-team-sport/index.html#are-we-curling",
    "title": "GitHub as a team sport",
    "section": "Are we curling? 🥌",
    "text": "Are we curling? 🥌\n\n\nWe:\n\nare a small team\nassume specialist roles\nwork in sync\n\n\n\n\n\n\nYou have been wondering: if this is a ‘team sport’, what sport is it?\nThis is a terrible metaphor. But think about it."
  },
  {
    "objectID": "presentations/2024-05-23_github-team-sport/index.html#the-bottom-line-actually",
    "href": "presentations/2024-05-23_github-team-sport/index.html#the-bottom-line-actually",
    "title": "GitHub as a team sport",
    "section": "The bottom line, actually",
    "text": "The bottom line, actually\n\n\n\n\n\nCommunicate\nHelp each other\nBe kind\n\n\n\n\nThe ideas in this talk are things that have helped us, and could help you, to drive up and maintain quality. Some were obvious, some were specific features you might not have known about.\nBut none of these are replacements for being good team members.\nGitHub just provides some affordances to help you.\nI am the guy falling over, the stones are tasks, my team mates are picking me up and dusting me off.\nDid you learn at least one thing? What has your team been doing? What works for you?"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#intro",
    "href": "presentations/2024-11-27_text-mining/index.html#intro",
    "title": "Text mining",
    "section": "Intro",
    "text": "Intro\n\nRange of possible applications\nThere are a range of caveats depending on the use case\nSome of them are more relevant than others\nSome of them are “free”, and some aren’t"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#what-is-text-mining",
    "href": "presentations/2024-11-27_text-mining/index.html#what-is-text-mining",
    "title": "Text mining",
    "section": "What is text mining?",
    "text": "What is text mining?\n\nA variety of supervised and unsupervised methods\nThe uses we’ll discuss today have no “understanding” of text (no “intelligence”)\nThey all have the potential, therefore, to be very inaccurate (e.g. negation)\nBut they can be a low cost way of gathering insight, properly applied"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#i-just-want-a-bit-of-help-sorting",
    "href": "presentations/2024-11-27_text-mining/index.html#i-just-want-a-bit-of-help-sorting",
    "title": "Text mining",
    "section": "“I just want a bit of help sorting”",
    "text": "“I just want a bit of help sorting”\n\nThis is one of the safer things to do\nThere are a range of methods\nSome will give you some say of the “theme” of the piles, and some don’t\nGreat for: sifting, looking at relative size of piles, novel suggestions for themes for text\nBad for: accuracy, control over what’s in the piles"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#i-just-want-to-find-useful-examples",
    "href": "presentations/2024-11-27_text-mining/index.html#i-just-want-to-find-useful-examples",
    "title": "Text mining",
    "section": "“I just want to find useful examples”",
    "text": "“I just want to find useful examples”\n\nAlso quite a safe application, and one we implemented for patient experience\nThe algorithm is just helping you to find things with a particular theme or sentiment\nYou bring the understanding\nMany ways of achieving this, from easy to difficult\n\nUnsupervised and supervised\nSearching for strings vs semantic search"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#generating-summary-statistics",
    "href": "presentations/2024-11-27_text-mining/index.html#generating-summary-statistics",
    "title": "Text mining",
    "section": "Generating summary statistics",
    "text": "Generating summary statistics\n\nThis needs to be done with care\nYou can potentially lose a lot of nuance and meaning\nEven the best model is probably only around 80% accurate\nUseful for monitoring and making rough estimates about the size of things\nNot suitable for anything that needs accuracy (e.g. safeguarding)"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#free",
    "href": "presentations/2024-11-27_text-mining/index.html#free",
    "title": "Text mining",
    "section": "“Free”",
    "text": "“Free”\n\nUnsupervised learning is “free”- no labelling necessary\nArguably you may as well use it for everything, speculatively\nHowever there are lots of models and parameters to set\nSo “free” is not really “free”"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#whats-going-on",
    "href": "presentations/2024-11-27_text-mining/index.html#whats-going-on",
    "title": "Text mining",
    "section": "What’s going on?",
    "text": "What’s going on?\n\nText models are only as sensible as their inputs\nWe call the algorithm that turns text to numbers a “vectoriser”"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#bag-of-words-vs-tf-idf",
    "href": "presentations/2024-11-27_text-mining/index.html#bag-of-words-vs-tf-idf",
    "title": "Text mining",
    "section": "Bag of words vs TF-IDF",
    "text": "Bag of words vs TF-IDF\n\nBag of words just counts the number of times each word appears\nCrude but effective\nTF-IDF works similarly but makes rare words more important, which can help with topic modelling and classification"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#word-embeddings",
    "href": "presentations/2024-11-27_text-mining/index.html#word-embeddings",
    "title": "Text mining",
    "section": "Word embeddings",
    "text": "Word embeddings\n\nFrom Sutor et al., reproduced under fair use"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#word-embeddings-cont.",
    "href": "presentations/2024-11-27_text-mining/index.html#word-embeddings-cont.",
    "title": "Text mining",
    "section": "Word embeddings cont.",
    "text": "Word embeddings cont.\n\nThere are some smallish ones (like Glove), and some huge ones (like BERT)\nVectors are the only way to encode meaning and context"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#the-future",
    "href": "presentations/2024-11-27_text-mining/index.html#the-future",
    "title": "Text mining",
    "section": "The future",
    "text": "The future\n\nA number of different things suggest themselves\nUse of topic models to explore and search\nTraining of a supervised model for a particular project"
  },
  {
    "objectID": "presentations/2024-11-27_text-mining/index.html#the-dream",
    "href": "presentations/2024-11-27_text-mining/index.html#the-dream",
    "title": "Text mining",
    "section": "The dream",
    "text": "The dream\n\nZero shot model with human in the loop learning"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#intro",
    "href": "presentations/2024-12-04_what-the-heck/index.html#intro",
    "title": "What the heck do I do all day?",
    "section": "Intro",
    "text": "Intro\n\nFor analysts, there is a culture of “I want to progress without being a manager”\nMany analysts don’t want to and wouldn’t enjoy being a manager- and they should progress without being one\nBut many would, and I want to speak today to those who are thinking about being a manager, or already are"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#my-story",
    "href": "presentations/2024-12-04_what-the-heck/index.html#my-story",
    "title": "What the heck do I do all day?",
    "section": "My story",
    "text": "My story\n\nThe best lesson I ever learned\nSpent 15 years being a nerd and letting others lead\nI have never in my life had an analyst for a manager\nI fell into being a manager because I wanted to effect change at my previous employer"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#the-downside",
    "href": "presentations/2024-12-04_what-the-heck/index.html#the-downside",
    "title": "What the heck do I do all day?",
    "section": "The Downside",
    "text": "The Downside\n\nI don’t always understand everything that’s happening\n\nStill don’t know what databricks is 😂\n\nSome of the things I do are honestly kind of boring\nSome days I just ping between meetings all day and never catch myself up"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#what-the-heck-do-i-do-all-day",
    "href": "presentations/2024-12-04_what-the-heck/index.html#what-the-heck-do-i-do-all-day",
    "title": "What the heck do I do all day?",
    "section": "What the heck do I do all day",
    "text": "What the heck do I do all day\n\nProduct owner\nTeam leader\nLine manager\nMember of the SU leadership group\nHead of data science\nNumber one learning- be different people at different times"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#product-owner",
    "href": "presentations/2024-12-04_what-the-heck/index.html#product-owner",
    "title": "What the heck do I do all day?",
    "section": "Product owner",
    "text": "Product owner\n\nWe do kind of scrum\nI decide what’s in and what’s out\nNumber one learning- stay out of the way (a familiar lesson from others)\nLove: clarity; building a product people actually want; giving the team purpose and shaping what we do\nHate: I’m on the outside- don’t always understand stuff"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#team-leader",
    "href": "presentations/2024-12-04_what-the-heck/index.html#team-leader",
    "title": "What the heck do I do all day?",
    "section": "Team leader",
    "text": "Team leader\n\nRecruitment\nWhat do we do- where are we going\nCulture &gt; Strategy (#recruitforvalues)"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#line-manager",
    "href": "presentations/2024-12-04_what-the-heck/index.html#line-manager",
    "title": "What the heck do I do all day?",
    "section": "Line manager",
    "text": "Line manager\n\nThe research says line managers are really important\nI ❤️❤️❤️ hearing about what my staff are doing and where they want to be in five years\nI want to live in a world where they prosper, because they’re all awesome\n\n(remember #recruitforvalues)"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#member-of-the-su-leadership-group",
    "href": "presentations/2024-12-04_what-the-heck/index.html#member-of-the-su-leadership-group",
    "title": "What the heck do I do all day?",
    "section": "Member of the SU leadership group",
    "text": "Member of the SU leadership group\n\nHonestly I didn’t think I would be all that bothered (finance reports!)\nIt turns out I am bothered\nI love the Strategy Unit and I want to help it however I can"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#head-of-data-science",
    "href": "presentations/2024-12-04_what-the-heck/index.html#head-of-data-science",
    "title": "What the heck do I do all day?",
    "section": "Head of data science",
    "text": "Head of data science\n\nA bit like team leader\nBut actually data science needs a voice in the Unit\nI’m not the only voice but it’s my job to lead"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#why-im-a-terrible-manager",
    "href": "presentations/2024-12-04_what-the-heck/index.html#why-im-a-terrible-manager",
    "title": "What the heck do I do all day?",
    "section": "Why I’m a terrible manager",
    "text": "Why I’m a terrible manager\n\nI’m introverted\nI’m disorganised\nI have an opinion about everything and can’t keep my mouth shut\nI say what’s on my mind; I can’t help it"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#why-im-a-good-manager",
    "href": "presentations/2024-12-04_what-the-heck/index.html#why-im-a-good-manager",
    "title": "What the heck do I do all day?",
    "section": "Why I’m a good manager",
    "text": "Why I’m a good manager\n\nI’m introverted\nI’m disorganised\nI have an opinion about everything and can’t keep my mouth shut\nI say what’s on my mind; I can’t help it"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#the-upside",
    "href": "presentations/2024-12-04_what-the-heck/index.html#the-upside",
    "title": "What the heck do I do all day?",
    "section": "The upside",
    "text": "The upside\n\nPinging around hearing about different stuff is fun\nShaping the team and its projects is fun\nMostly for me this job is about making a difference\n\nRecruiting is miserable- hiring great staff is wonderful\nFaffing around with budgets is miserable- growing the team is wonderful\nSitting typing emails all day is meh- connecting people and projects is wonderful"
  },
  {
    "objectID": "presentations/2024-12-04_what-the-heck/index.html#its-a-unique-role",
    "href": "presentations/2024-12-04_what-the-heck/index.html#its-a-unique-role",
    "title": "What the heck do I do all day?",
    "section": "It’s a unique role",
    "text": "It’s a unique role\n\nEvery day I wake up and don’t help the team with the code for the sprint\nEvery day I think about what I can bring to the team\nEmbrace the uniqueness and give it a try"
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#which-is-easier-to-read",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#which-is-easier-to-read",
    "title": "Coffee and Coding",
    "section": "Which is easier to read?",
    "text": "Which is easier to read?\n\nae_attendances |&gt;\n  filter(org_code %in% c(\"RNA\", \"RL4\")) |&gt;\n  mutate(performance = 1 + breaches / attendances) |&gt;\n  filter(type == 1) |&gt;\n  mutate(met_target = performance &gt;= 0.95)\n\nor\n\nae_attendances |&gt;\n  filter(\n    org_code %in% c(\"RNA\", \"RL4\"),\n    type == 1\n  ) |&gt;\n  mutate(\n    performance = 1 + breaches / attendances,\n    met_target = performance &gt;= 0.95\n  )\n\n\n  spending a few seconds to neatly format your code can greatly improve the legibility to future readers, making the intent of the code far clearer, and will make finding bugs easier to spot.\n\n\n  (have you spotted the mistake in the snippets above?)"
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#tidyverse-style-guide",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#tidyverse-style-guide",
    "title": "Coffee and Coding",
    "section": "Tidyverse Style Guide",
    "text": "Tidyverse Style Guide\n\nGood coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread\n\n\nAll style guides are fundamentally opinionated. Some decisions genuinely do make code easier to use (especially matching indenting to programming structure), but many decisions are arbitrary. The most important thing about a style guide is that it provides consistency, making code easier to write because you need to make fewer decisions.\n\ntidyverse style guide"
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#lintr-styler-are-your-new-best-friends",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#lintr-styler-are-your-new-best-friends",
    "title": "Coffee and Coding",
    "section": "{lintr} + {styler} are your new best friends",
    "text": "{lintr} + {styler} are your new best friends\n\n\n{lintr}\n\n{lintr} is a static code analysis tool that inspects your code (without running it)\nit checks for certain classes of errors (e.g. mismatched { and (’s)\nit warns about potential issues (e.g. using variables that aren’t defined)\nit warns about places where you are not adhering to the code style\n\n\n{styler}\n\n{styler} is an RStudio add in that automatically reformats your code, tidying it up to match the style guide\n99.9% of the time it will give you equivalent code, but there is the potential that it may change the behaviour of your code\nit will overwrite the files that you ask it to run on however, so it is vital to be using version control\na good workflow here is to save your file, “stage” the changes to your file, then run {styler}. You can then revert back to the staged changed if needed."
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#what-does-lintr-look-like",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#what-does-lintr-look-like",
    "title": "Coffee and Coding",
    "section": "What does {lintr} look like?",
    "text": "What does {lintr} look like?\n\n\n\nsource: Good practice for writing R code and R packages\n\nrunning lintr can be done in the console, e.g.\n\nlintr::lintr_dir(\".\")\n\nor via the Addins menu"
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#using-styler",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#using-styler",
    "title": "Coffee and Coding",
    "section": "Using {styler}",
    "text": "Using {styler}\n\nsource: Good practice for writing R code and R packages"
  },
  {
    "objectID": "presentations/2023-03-09_coffee-and-coding/index.html#further-thoughts-on-improving-code-legibility",
    "href": "presentations/2023-03-09_coffee-and-coding/index.html#further-thoughts-on-improving-code-legibility",
    "title": "Coffee and Coding",
    "section": "Further thoughts on improving code legibility",
    "text": "Further thoughts on improving code legibility\n\ndo not let files grow too big\nbreak up logic into separate files, then you can use source(\"filename.R) to run the code in that file\nidealy, break up your logic into separate functions, each function having it’s own file, and then call those functions within your analysis\ndo not repeat yourself - if you are copying and pasting your code then you should be thinking about how to write a single function to handle this repeated logic"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#what-is-computer-vision-cv",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#what-is-computer-vision-cv",
    "title": "Computer Vision",
    "section": "What is Computer Vision (CV)",
    "text": "What is Computer Vision (CV)\n\n\nComputer vision is a field of computer science that focuses on enabling computers to identify and understand objects and people in images and videos.\n\n\n\n\nLike other types of AI, computer vision seeks to perform and automate tasks that replicate human capabilities.\n\n\n\n\nIn this case, computer vision seeks to replicate both the way humans see, and the way humans make sense of what they see.\n\n\n\nSource: What is computer vision? Microsoft"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#classification",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#classification",
    "title": "Computer Vision",
    "section": "Classification",
    "text": "Classification\n\nAssign a single label to each image\n\n\n\n\n\n\n\nDog\n\n\nWelsh Spaniel\n\n\nAnimal in water\n\n\n\n\n\n\n\n\nDog\n\n\nSussex Spaniel\n\n\nAnimal on land\n\n\n\nImages from imagenet"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#multi-classification",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#multi-classification",
    "title": "Computer Vision",
    "section": "Multi-Classification",
    "text": "Multi-Classification\n\nAssign one or more labels to each image\n\n\n\n\n\nDog, Welsh Spaniel, Animal in water\n\n\n\n\n\n\nDog, Sussex Spaniel, Animal on land\n\n\n\nImages from imagenet"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#object-detection",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#object-detection",
    "title": "Computer Vision",
    "section": "Object Detection",
    "text": "Object Detection\n\n\nImage sourced from Wikimedia"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#event-detection",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#event-detection",
    "title": "Computer Vision",
    "section": "Event Detection",
    "text": "Event Detection\n\n\nIs camera-only the future of self-driving cars?"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#how-does-cv-work",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#how-does-cv-work",
    "title": "Computer Vision",
    "section": "How does CV work?",
    "text": "How does CV work?\nfor classification tasks\n\nget a very large corpus or labelled images\nconvert the images to a form that the computer can work with (tensors)\nfeed into a convolutional neural network\nprofit?"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#large-corpus-of-labelled-images",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#large-corpus-of-labelled-images",
    "title": "Computer Vision",
    "section": "Large corpus of labelled images",
    "text": "Large corpus of labelled images\nImageNet\n\na large visual database\nover 14 million hand-annotated images\nmore than 20,000 categories\neach category has “several hundred” images\nstarted in 2006"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#convert-the-images-to-tensors",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#convert-the-images-to-tensors",
    "title": "Computer Vision",
    "section": "Convert the images to tensors",
    "text": "Convert the images to tensors\n\n\nA fancy way of saying:\nturn the images into a 2d table\nof values between 0 and 1"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#convolutional-neural-networks",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#convolutional-neural-networks",
    "title": "Computer Vision",
    "section": "(Convolutional) Neural Networks",
    "text": "(Convolutional) Neural Networks\n\n\n3Blue1Brown, YouTube Channel"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#use-pre-trained-models",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#use-pre-trained-models",
    "title": "Computer Vision",
    "section": "Use pre-trained models",
    "text": "Use pre-trained models\nModels that have been pre-trained on some image datasets which can be downloaded and used\n\nResNet (Microsoft Research)\nInception (Google)\nTrending classifiers from Hugging Face\n\n\nTransfer learning is the concept of taking a pre-trained model as a basis, then fine-tuning it to classify based on your own images."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#how-can-cv-be-used-in-healthcare",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#how-can-cv-be-used-in-healthcare",
    "title": "Computer Vision",
    "section": "How can CV be used in Healthcare?",
    "text": "How can CV be used in Healthcare?\n\nclassification\nmulti-classification\nobject detection\nevent detection"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#how-can-cv-be-used-in-healthcare-1",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#how-can-cv-be-used-in-healthcare-1",
    "title": "Computer Vision",
    "section": "How can CV be used in Healthcare?",
    "text": "How can CV be used in Healthcare?\n\n\ndetecting disease or injury\nmonitoring patients vitals, e.g. respiratory rate\ndetecting bounds of a tumour when planning radiotherapy\nautomating cell counting\ncalculating the grade of a cancer\nmonitor for long-term changes, e.g. AAA\ndevices for patients with vision impairments\ndetecting when patients move (fall prevention)\nmonitoring the hygiene of a ward"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#issues-with-computer-vision",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#issues-with-computer-vision",
    "title": "Computer Vision",
    "section": "Issues with Computer Vision",
    "text": "Issues with Computer Vision\n\n… one neural network learned to differentiate between dogs and wolves. It didn’t learn the differences between dogs and wolves, but instead learned that wolves were on snow in their picture and dogs were on grass.\n\n\nDogs, Wolves, Data Science, and Why Machines Must Learn Like Humans Do (2017)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#issues-with-computer-vision-1",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#issues-with-computer-vision-1",
    "title": "Computer Vision",
    "section": "Issues with Computer Vision",
    "text": "Issues with Computer Vision\n\n\nArtificial intelligence could revolutionize medical care. But don’t trust it to read your x-ray just yet (2019)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#issues-with-computer-vision-2",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#issues-with-computer-vision-2",
    "title": "Computer Vision",
    "section": "Issues with Computer Vision",
    "text": "Issues with Computer Vision\n\n\nalgorithm trained at Mount Sinai Hospital, New York City\nBusy ICU, many elderly patients\n34% of their x-rays came from patients with pneumonia\n93% accuracy\n\n\n\n\ntested at other sites, pneumonia ~1% of x-rays\naccuracy dropped to 73%-80%\n\n\n\nArtificial intelligence could revolutionize medical care. But don’t trust it to read your x-ray just yet (2019)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#issues-with-computer-vision-3",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#issues-with-computer-vision-3",
    "title": "Computer Vision",
    "section": "Issues with Computer Vision",
    "text": "Issues with Computer Vision\n\nAt Mount Sinai, many of the infected patients were too sick to get out of bed, and so doctors used a portable chest x-ray machine. Portable x-ray images look very different from those created when a patient is standing up. Because of what it learned from Mount Sinai’s x-rays, the algorithm began to associate a portable x-ray with illness. It also anticipated a high rate of pneumonia.\n\n\nArtificial intelligence could revolutionize medical care. But don’t trust it to read your x-ray just yet (2019)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#the-unique-problems-of-medical-computer-vision",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#the-unique-problems-of-medical-computer-vision",
    "title": "Computer Vision",
    "section": "The Unique Problems of Medical Computer Vision",
    "text": "The Unique Problems of Medical Computer Vision\n\n\n\n\n\n\nThis is the very unique problem of medical computer vision: we are attempting to solve a small signal on the background of small noise whereas standard computer vision’s problem is a large signal on the background of large noise.\n\n\n\n\nThe Unique Problems of Medical Computer Vision"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#the-unique-problems-of-medical-computer-vision-1",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#the-unique-problems-of-medical-computer-vision-1",
    "title": "Computer Vision",
    "section": "The Unique Problems of Medical Computer Vision",
    "text": "The Unique Problems of Medical Computer Vision\n\n\n\n\nIs this a dog?\n\n\n\n\n\nEnglish Springer Spaniel in a Field, Wikipedia\n\n\n\n\n\n\n\nSmall-cell carcinoma of the lung, Wikipedia\n\n\n\n\n\n\nAnnotations of cats & dogs is cheaper than reviewing medical scans/slides. The latter adds an additional burden on health systems."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-tom/index.html#other-issues-with-cv",
    "href": "presentations/2024-10-10_what-is-ai-tom/index.html#other-issues-with-cv",
    "title": "Computer Vision",
    "section": "Other issues with CV",
    "text": "Other issues with CV\n\nEarly detection vs over diagnosis\nAdversarial attacks can trick CV algorithms to incorrectly classify images\nComputational power (environmental impact)\nGovernance: have we got consent to use images?\nExplainability of Neural Networks"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#health-data-in-the-headlines",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#health-data-in-the-headlines",
    "title": "System Dynamics in health and care",
    "section": "Health Data in the Headlines",
    "text": "Health Data in the Headlines\n\n\n\n\nUsed to seeing headlines that give a snapshot figure but doesn’t say much about the system.\nNow starting to see headlines that recognise flow through the system rather than snapshot in time of just one part.\nCan get better understanding of the issues in a system if we can map it as stocks and flows, but our datasets not designed to give up this information very readily. This talk is how I have tried to meet that challenge."
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#through-the-system-dynamics-lens",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#through-the-system-dynamics-lens",
    "title": "System Dynamics in health and care",
    "section": "Through the System Dynamics lens",
    "text": "Through the System Dynamics lens\n\nStock-flow model\nDynamic behaviour, feedback loops\n\nIn a few seconds, what is SD?\nAn approach to understanding the behaviour of complex systems over time. A method of mapping a system as stocks, whose levels can only change due to flows in and flows out. Stocks could be people on a waiting list, on a ward, money, …\nFlows are the rate at which things change in a given time period e.g. admissions per day, referrals per month.\nBehaviour of the system is determined by how the components interact with each other, not what each component does. Mapping the structure of a system like this leads us to identify feedback loops, and consequences of an action - both intended and unintended.\nIn this capacity-constrained model we only need 3 parameters to run the model (exogenous). All the behaviour within the grey box is determined by the interactions of those components (indogenous).\nHow do we get a value/values for referrals per day?\n(currently use specialist software to build and run our models, aim is to get to a point where we can run in open source.)"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#determining-flows",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#determining-flows",
    "title": "System Dynamics in health and care",
    "section": "Determining flows",
    "text": "Determining flows\n\n\n\n\n‘admissions per day’ is needed to populate the model.\n‘discharged’ could be used to verify the model against known data\n\nHow many admissions per day (or week, month…)\n\n\n\n\n\n   \n\n\nGoing to use very simple model shown to explain how to extract flow data for admissions. Will start with visual explainer before going into the code.\n1. generate list of key dates (in this case daily, could be weekly, monthly)\n2. take our patient-level ID with admission and discharge dates\n3. count of admissions on that day/week"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#determining-occupancy",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#determining-occupancy",
    "title": "System Dynamics in health and care",
    "section": "Determining occupancy",
    "text": "Determining occupancy\n\n\n\n\n‘on ward’ is used to verify the model against known data\n\nLogic statement testing if the key date is wholly between admission and discharge dates\nflag for a match \n\n\n\n\n     \n\n\nMight also want to generate occupancy, to compare the model output with actual data to verify/validate.\n1. generate list of key dates\n2. take our patient-level ID with admission and discharge dates\n3. going to take each date in our list of keydates, and see if there is an admission before that date and discharge after 4. this creates a wide data frame, the same length as patient data.\n5. once run through all the dates in the list, sum each column\nPatient A admitted on 2nd, so only starts being classed as resident on 3rd."
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---flows",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---flows",
    "title": "System Dynamics in health and care",
    "section": "in R - flows",
    "text": "in R - flows\nEasy to do with count, or group_by and summarise\n\n\nadmit_d &lt;- spell_dates |&gt;\n  group_by(date_admit) |&gt;\n  count(date_admit)\n\nhead(admit_d)\n\n\n# A tibble: 6 × 2\n# Groups:   date_admit [6]\n  date_admit     n\n  &lt;date&gt;     &lt;int&gt;\n1 2022-01-01    21\n2 2022-01-02    28\n3 2022-01-03    22\n4 2022-01-04    24\n5 2022-01-05    24\n6 2022-01-06    25"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---occupancy",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---occupancy",
    "title": "System Dynamics in health and care",
    "section": "in R - occupancy",
    "text": "in R - occupancy\nGenerate list of key dates\n\n\n\ndate_start &lt;- dmy(01012022)\ndate_end &lt;- dmy(31012022)\nrun_len &lt;- length(seq(from = date_start, to = date_end, by = \"day\"))\n\nkeydates &lt;- data.frame(\n  date = c(seq(date_start, by = \"day\", length.out = run_len))\n)\n\n\n\n\n        date\n1 2022-01-01\n2 2022-01-02\n3 2022-01-03\n4 2022-01-04\n5 2022-01-05\n6 2022-01-06\n\n\n\n\nStart by generating the list of keydates. In this example we’re running the model in days, and checking each day in 2022.\nNeed the run length for the next step, to know how many times to iterate over"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---occupancy-1",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#in-r---occupancy-1",
    "title": "System Dynamics in health and care",
    "section": "in R - occupancy",
    "text": "in R - occupancy\nIterate over each date - need to have been admitted before, and discharged after\n\noccupancy_flag &lt;- function(df) {\n  # pre-allocate tibble size to speed up iteration in loop\n  activity_all &lt;- tibble(nrow = nrow(df)) |&gt;\n    select()\n\n  for (i in 1:run_len) {\n    activity_period &lt;- case_when(\n      # creates 1 flag if resident for complete day\n      df$date_admit &lt; keydates$keydate[i] &\n        df$date_discharge &gt; keydates$keydate[i] ~\n        1,\n      TRUE ~ 0\n    )\n\n    # column bind this day's flags to previous\n    activity_all &lt;- bind_cols(activity_all, activity_period)\n  }\n\n  # rename column to match the day being counted\n  activity_all &lt;- activity_all |&gt;\n    setNames(paste0(\"d_\", keydates$date))\n\n  # bind flags columns to patient data\n  daily_adm &lt;- bind_cols(df, activity_all) |&gt;\n    pivot_longer(\n      cols = starts_with(\"d_\"),\n      names_to = \"date\",\n      values_to = \"count\"\n    ) |&gt;\n\n    group_by(date) |&gt;\n    summarise(resident = sum(count)) |&gt;\n    ungroup() |&gt;\n    mutate(date = str_remove(date, \"d_\"))\n}\n\n\nIs there a better way than using a for loop?\n\nPre-allocate tibbles\nactivity_all will end up as very wide tibble, with a column for each date in list of keydates.\nFor each date in the list of key dates, compares with admission date & discharge date; need to be admitted before the key date and discharged after the key date. If match, flag = 1.\nCreates a column for each day, then binds this to activity all.\nRename each column with the date it was checking (add a character to start of column name so column doesn’t start with numeric)\nPivot long, then group by date and sum the flags (other variables could be added here, such as TFC or provider code)"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods---flows",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods---flows",
    "title": "System Dynamics in health and care",
    "section": "Longer Time Periods - flows",
    "text": "Longer Time Periods - flows\nUse lubridate::floor_date to generate the date at start of week/month\n\nadmit_wk &lt;- spell_dates |&gt;\n  mutate(\n    week_start = floor_date(\n      date_admit,\n      unit = \"week\",\n      week_start = 1 # start week on Monday\n    )\n  ) |&gt;\n  count(week_start) # could add other parameters such as provider code, TFC etc\n\nhead(admit_wk)\n\n\n\n# A tibble: 6 × 2\n  week_start     n\n  &lt;date&gt;     &lt;int&gt;\n1 2021-12-27    49\n2 2022-01-03   178\n3 2022-01-10   188\n4 2022-01-17   187\n5 2022-01-24   207\n6 2022-01-31   183\n\n\n\nMight run SD model in weeks or months - e.g. months for care homes Use lubridate to create new variable with start date of week/month/year etc"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods---occupancy",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods---occupancy",
    "title": "System Dynamics in health and care",
    "section": "Longer Time Periods - occupancy",
    "text": "Longer Time Periods - occupancy\nKey dates to include the dates at the start and end of each time period\n\n\n\ndate_start &lt;- dmy(03012022) # first Monday of the year\ndate_end &lt;- dmy(01012023)\nrun_len &lt;- length(seq(from = date_start, to = date_end, by = \"week\"))\n\nkeydates &lt;- data.frame(\n  wk_start = c(seq(date_start, by = \"week\", length.out = run_len))\n) |&gt;\n  mutate(\n    wk_end = wk_start + 6\n  ) # last date in time period\n\n\n\n\n    wk_start     wk_end\n1 2022-01-03 2022-01-09\n2 2022-01-10 2022-01-16\n3 2022-01-17 2022-01-23\n4 2022-01-24 2022-01-30\n5 2022-01-31 2022-02-06\n6 2022-02-07 2022-02-13\n\n\n\n\nModel might make more sense to run in weeks or months (e.g. care home), so list of keydates need a start date and end date for each time period."
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#longer-time-periods",
    "title": "System Dynamics in health and care",
    "section": "Longer Time Periods",
    "text": "Longer Time Periods\nMore logic required if working in weeks or months - can only be in one place at any given time\n\n# flag for occupancy\nactivity_period &lt;- case_when(\n  # creates 1 flag if resident for complete week\n  df$date_admit &lt; keydates$wk_start[i] &\n    df$date_discharge &gt; keydates$wk_end[i] ~\n    1,\n  TRUE ~ 0\n)\n\n\nAnd a little bit more logic\nOccupancy requires the patient to have been admitted before the start of the week/month, and discharged after the end of the week/month"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#applying-the-data",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#applying-the-data",
    "title": "System Dynamics in health and care",
    "section": "Applying the data",
    "text": "Applying the data\n\n\nHow to apply this wrangling of data to the system dynamic model?\nAdmissions data used as an input to the flow - could be reduced to a single figure (average), or there may be variation by season/day of week etc.\nOccupancy (and discharges) used to verify the model output against known data."
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#next-steps",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#next-steps",
    "title": "System Dynamics in health and care",
    "section": "Next Steps",
    "text": "Next Steps\n\nGeneralise function to a state where it can be used by others - onto Github\nTurn this into a package\nOpen-source SD models and interfaces - R Shiny or Python"
  },
  {
    "objectID": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#questions-comments-suggestions",
    "href": "presentations/2023-10-09_nhs-r_conf_sd_in_health_social_care/index.html#questions-comments-suggestions",
    "title": "System Dynamics in health and care",
    "section": "Questions, comments, suggestions?",
    "text": "Questions, comments, suggestions?\n\n\n\nPlease get in touch!\n\nSally.Thompson37@nhs.net"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#intro",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#intro",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Intro",
    "text": "Intro\n\nDesign of the application\nDeployment and use\nUpdate and maintenance\nSome slides reused from Tom Jemmett’s HACA presentation"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#model-overview",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#model-overview",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Model Overview",
    "text": "Model Overview\n\nThe baseline data is a year worth of a provider’s HES data\nEach row in the baseline data is run through a series of steps\nEach step creates a factor that says how many times (on average) to sample that row\nThe factors are multiplied together and used to create a random Poisson value\nWe resample the rows using this random values\nEfficiencies are then applied, e.g. LoS reductions, type conversions"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#monte-carlo-simulation",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#monte-carlo-simulation",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Monte Carlo Simulation",
    "text": "Monte Carlo Simulation\n\nWe run the model N times, varying the input parameters each time slightly to handle the uncertainty.\nThe results of the model are aggregated at the end of each model run\nThe aggregated results are combined at the end into a single file"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#prequisites",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#prequisites",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Prequisites",
    "text": "Prequisites\n\nRobust to updated model releases\nReproducibility- across multiple versions\nSpeed- for the model and the reporting\nInterpretation of complex output"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#data-sources",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#data-sources",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Data sources",
    "text": "Data sources\n\nThe usual rule is that 80% of the work is on data\nNot a problem for us because we’re using trusted, curated data (HES)\nSo a mere 80% of our work is on data\nWait, what?"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#data-pipelines",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#data-pipelines",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Data pipelines",
    "text": "Data pipelines\n\nRAP is obviously crucial\n\nReproducibility is essential\nUndocumented mistakes severe (there are always bugs 🙂)\nSpeed and sharing within the project team\n\nThe original data pipelines were SQL\n\nUpdates took 4 days with frequent hangs and crashes\n\nThey now execute in 30 minutes on databricks"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#the-model-itself",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#the-model-itself",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "The model itself",
    "text": "The model itself\n\nWritten in Python for speed\nAn API spins up a Docker instance to run the model on demand (~ 5 minutes)\nUses .parquet, also for speed\nThe model outputs row level data conceptually, if not actually"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#outputs",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#outputs",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Outputs",
    "text": "Outputs\n\n“Final report” Word document, bespoke to current context\n“Outputs dashboard” - containing high level summaries\nDetailed model results - we offer a range of outputs but can always make more"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#overview",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#overview",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Overview",
    "text": "Overview"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#uses",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#uses",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Uses",
    "text": "Uses\n\nSizing hospitals\nSizing left shift\nPredicting, comparing, and monitoring activity mitigation"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#the-process",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#the-process",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "The process",
    "text": "The process\n\nJanuary 2023: Development phase\nApril-ish: Thinking about deployment\nOctober-ish: Model is on its way to full production and much remains to do"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#operational-mode",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#operational-mode",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Operational mode",
    "text": "Operational mode\n\nThe team was growing in November\nThere were two priorities\n\nIncrease bus factor (which I’ll come back to)\nDeploy the first production ready version"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#the-first-deployed-version",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#the-first-deployed-version",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "The first deployed version",
    "text": "The first deployed version\n\nDecember-ish: Many needs that were not anticipated\nThis first release kicked off lots of other work\nThe second release kicked off lots of other work\nIt was very hard to do any long term planning"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#scrum",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#scrum",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Scrum",
    "text": "Scrum\n\nWe are not doing “proper” scrum\nProduct owner, scrum master, everyone else\nFive week sprints with a one week recovery run between each one\nSprint planning, sprint catchup, sprint retro"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#sprint-retro",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#sprint-retro",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Sprint retro",
    "text": "Sprint retro\n\nWhat went well, what could have gone better, and what to improve next time\nLooking at process, not blaming individuals\nRequires maturity and trust to bring up issues, and to respond to them in a constructive way\nShould agree at the end on one process improvement which goes in the next sprint\nWe’ve had some really, really good retros and I think it’s a really important process for a team"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#what-did-scrum-give-the-team",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#what-did-scrum-give-the-team",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "What did scrum give the team?",
    "text": "What did scrum give the team?\n\nSimultaneous releases of linked repos\nThe team works autonomously in the sprint\nBetter conversations about “no”\nThe planning and retro process improves the team’s processes, not just the code"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#product-owner",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#product-owner",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Product owner",
    "text": "Product owner\n\nMy lessson- get out the way\nA better connection between high level and low level planning\nClear release dates and responsibilities\nClear what I should be doing"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#agility",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#agility",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Agility",
    "text": "Agility\n\nThis project was agile whether we liked it or not!\nMy 2022 agile definition:\n\nCustomers can’t make up their minds\nIt’s hard to design software all at once\nContinuous delivery keeps customers happy"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#some-highlights-from-the-agile-manifesto",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#some-highlights-from-the-agile-manifesto",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Some highlights from The agile manifesto",
    "text": "Some highlights from The agile manifesto\n\n“Welcome changing requirements, even late in development”\n“At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior”\n“Continuous attention to technical excellence and good design enhances agility”\n“Simplicity- the art of maximizing the amount of work not done- is essential”"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#how-scrum-helped",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#how-scrum-helped",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "How scrum helped",
    "text": "How scrum helped\n\nThose shaping the project wanted to be able to make quick changes- and see the long term plan\nAgility is a mindset, a mode of practice\nIf anything we were actually too agile\nBeing agile is all about being able to review and make decisions frequently\nBut it isn’t about changing what you’re doing all the time\nGood code and good teams are ready to change direction- whether they change or not"
  },
  {
    "objectID": "presentations/2025-02-27_nhp-deployment/index.html#current-work-and-future",
    "href": "presentations/2025-02-27_nhp-deployment/index.html#current-work-and-future",
    "title": "Deployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS",
    "section": "Current work and future",
    "text": "Current work and future\n\nWork on a “population based” model\nProviding support to elicit parameters for local and national use\nMore work understanding and building on the activity avoidance parameters"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#what-is-testing",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#what-is-testing",
    "title": "Unit testing in R",
    "section": "What is testing?",
    "text": "What is testing?\n\nSoftware testing is the act of examining the artifacts and the behavior of the software under test by validation and verification. Software testing can also provide an objective, independent view of the software to allow the business to appreciate and understand the risks of software implementation\nwikipedia"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-can-we-test-our-code",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-can-we-test-our-code",
    "title": "Unit testing in R",
    "section": "How can we test our code?",
    "text": "How can we test our code?\n\n\nStatically\n\n(without executing the code)\nhappens constantly, as we are writing code\nvia code reviews\ncompilers/interpreters/linters statically analyse the code for syntax errors\n\n\n\n\nDynamically"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-can-we-test-our-code-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-can-we-test-our-code-1",
    "title": "Unit testing in R",
    "section": "How can we test our code?",
    "text": "How can we test our code?\n\n\nStatically\n\n(without executing the code)\nhappens constantly, as we are writing code\nvia code reviews\ncompilers/interpreters/linters statically analyse the code for syntax errors\n\n\n\n\nDynamically\n\n(by executing the code)\nsplit into functional and non-functional testing\ntesting can be manual, or automated\n\n\n\n\nnon-functional testing covers things like performance, security, and usability testing"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests",
    "title": "Unit testing in R",
    "section": "Different types of functional tests",
    "text": "Different types of functional tests\nUnit Testing checks each component (or unit) for accuracy independently of one another.\n\nIntegration Testing integrates units to ensure that the code works together.\n\n\nEnd-to-End Testing (e2e) makes sure that the entire system functions correctly.\n\n\nUser Acceptance Testing (UAT) ensures that the product meets the real user’s requirements."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests-1",
    "title": "Unit testing in R",
    "section": "Different types of functional tests",
    "text": "Different types of functional tests\nUnit Testing checks each component (or unit) for accuracy independently of one another.\nIntegration Testing integrates units to ensure that the code works together.\nEnd-to-End Testing (e2e) makes sure that the entire system functions correctly.\n\nUser Acceptance Testing (UAT) ensures that the product meets the real user’s requirements.\n\n\nUnit, Integration, and E2E testing are all things we can automate in code, whereas UAT testing is going to be manual"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#different-types-of-functional-tests-2",
    "title": "Unit testing in R",
    "section": "Different types of functional tests",
    "text": "Different types of functional tests\nUnit Testing checks each component (or unit) for accuracy independently of one another.\n\nIntegration Testing integrates units to ensure that the code works together.\nEnd-to-End Testing (e2e) makes sure that the entire system functions correctly.\nUser Acceptance Testing (UAT) ensures that the product meets the real user’s requirements.\n\n\nOnly focussing on unit testing in this talk, but the techniques/packages could be extended to integration testing. Often other tools (potentially specific tools) are needed for E2E testing."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#example",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#example",
    "title": "Unit testing in R",
    "section": "Example",
    "text": "Example\nWe have a {shiny} app which grabs some data from a database, manipulates the data, and generates a plot.\n\nwe would write unit tests to check the data manipulation and plot functions work correctly (with pre-created sample/simple datasets)\nwe would write integration tests to check that the data manipulation function works with the plot function (with similar data to what we used for the unit tests)\nwe would write e2e tests to ensure that from start to finish the app grabs the data and produces a plot as required\n\n\nsimple (unit tests) to complex (e2e tests)"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-pyramid",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-pyramid",
    "title": "Unit testing in R",
    "section": "Testing Pyramid",
    "text": "Testing Pyramid\n\n\nImage source: The Testing Pyramid: Simplified for One and All headspin.io"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function",
    "title": "Unit testing in R",
    "section": "Let’s create a simple function…",
    "text": "Let’s create a simple function…\n\nmy_function &lt;- function(x, y) {\n  \n  stopifnot(\n    \"x must be numeric\" = is.numeric(x),\n    \"y must be numeric\" = is.numeric(y),\n    \"x must be same length as y\" = length(x) == length(y),\n    \"cannot divide by zero!\" = y != 0\n  )\n\n  x / y\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function-1",
    "title": "Unit testing in R",
    "section": "Let’s create a simple function…",
    "text": "Let’s create a simple function…\n\nmy_function &lt;- function(x, y) {\n  \n  stopifnot(\n    \"x must be numeric\" = is.numeric(x),\n    \"y must be numeric\" = is.numeric(y),\n    \"x must be same length as y\" = length(x) == length(y),\n    \"cannot divide by zero!\" = y != 0\n  )\n\n  x / y\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-create-a-simple-function-2",
    "title": "Unit testing in R",
    "section": "Let’s create a simple function…",
    "text": "Let’s create a simple function…\n\nmy_function &lt;- function(x, y) {\n  \n  stopifnot(\n    \"x must be numeric\" = is.numeric(x),\n    \"y must be numeric\" = is.numeric(y),\n    \"x must be same length as y\" = length(x) == length(y),\n    \"cannot divide by zero!\" = y != 0\n  )\n\n  x / y\n}\n\n\nThe Ten Rules of Defensive Programming in R"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test",
    "title": "Unit testing in R",
    "section": "… and create our first test",
    "text": "… and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-1",
    "title": "Unit testing in R",
    "section": "… and create our first test",
    "text": "… and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-2",
    "title": "Unit testing in R",
    "section": "… and create our first test",
    "text": "… and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-3",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-3",
    "title": "Unit testing in R",
    "section": "… and create our first test",
    "text": "… and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-4",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-4",
    "title": "Unit testing in R",
    "section": "… and create our first test",
    "text": "… and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-5",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#and-create-our-first-test-5",
    "title": "Unit testing in R",
    "section": "… and create our first test",
    "text": "… and create our first test\n\ntest_that(\"my_function correctly divides values\", {\n  expect_equal(\n    my_function(4, 2),\n    2\n  )\n  expect_equal(\n    my_function(1, 4),\n    0.25\n  )\n  expect_equal(\n    my_function(c(4, 1), c(2, 4)),\n    c(2, 0.25)\n  )\n})\n\nTest passed 😸"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#other-expect_-functions",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#other-expect_-functions",
    "title": "Unit testing in R",
    "section": "other expect_*() functions…",
    "text": "other expect_*() functions…\n\ntest_that(\"my_function correctly divides values\", {\n  expect_lt(\n    my_function(4, 2),\n    10\n  )\n  expect_gt(\n    my_function(1, 4),\n    0.2\n  )\n  expect_length(\n    my_function(c(4, 1), c(2, 4)),\n    2\n  )\n})\n\nTest passed 🌈\n\n\n\n{testthat} documentation"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert",
    "title": "Unit testing in R",
    "section": "Arrange, Act, Assert",
    "text": "Arrange, Act, Assert\n\n\n\n\n\ntest_that(\"my_function works\", {\n  # arrange\n  #  \n  #\n  #\n\n  # act\n  #\n\n  # assert\n  #\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-1",
    "title": "Unit testing in R",
    "section": "Arrange, Act, Assert",
    "text": "Arrange, Act, Assert\n\n\nwe arrange the environment, before running the function\n\nto create sample values\ncreate fake/temporary files\nset random seed\nset R options/environment variables\n\n\n\ntest_that(\"my_function works\", {\n  # arrange\n  x &lt;- 5\n  y &lt;- 7\n  expected &lt;- 0.714285\n\n  # act\n  #\n\n  # assert\n  #\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-2",
    "title": "Unit testing in R",
    "section": "Arrange, Act, Assert",
    "text": "Arrange, Act, Assert\n\n\nwe arrange the environment, before running the function\nwe act by calling the function\n\n\ntest_that(\"my_function works\", {\n  # arrange\n  x &lt;- 5\n  y &lt;- 7\n  expected &lt;- 0.714285\n\n  # act\n  actual &lt;- my_function(x, y)\n\n  # assert\n  #\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-3",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#arrange-act-assert-3",
    "title": "Unit testing in R",
    "section": "Arrange, Act, Assert",
    "text": "Arrange, Act, Assert\n\n\nwe arrange the environment, before running the function\nwe act by calling the function\nwe assert that the actual results match our expected results\n\n\ntest_that(\"my_function works\", {\n  # arrange\n  x &lt;- 5\n  y &lt;- 7\n  expected &lt;- 0.714285\n\n  # act\n  actual &lt;- my_function(x, y)\n\n  # assert\n  expect_equal(actual, expected)\n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#our-test-failed",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#our-test-failed",
    "title": "Unit testing in R",
    "section": "Our test failed!?! 😢",
    "text": "Our test failed!?! 😢\n\ntest_that(\"my_function works\", {\n  # arrange\n  x &lt;- 5\n  y &lt;- 7\n  expected &lt;- 0.714285\n\n  # act\n  actual &lt;- my_function(x, y)\n\n  # assert\n  expect_equal(actual, expected)\n})\n\n── Failure: my_function works ──────────────────────────────────────────────────\n`actual` not equal to `expected`.\n1/1 mismatches\n[1] 0.714 - 0.714 == 7.14e-07\n\n\nError:\n! Test failed"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#tolerance-to-the-rescue",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#tolerance-to-the-rescue",
    "title": "Unit testing in R",
    "section": "Tolerance to the rescue 🙂",
    "text": "Tolerance to the rescue 🙂\n\ntest_that(\"my_function works\", {\n  # arrange\n  x &lt;- 5\n  y &lt;- 7\n  expected &lt;- 0.714285\n\n  # act\n  actual &lt;- my_function(x, y)\n\n  # assert\n  expect_equal(actual, expected, tolerance = 1e-6)\n})\n\nTest passed 🌈\n\n\n\n(this is a slightly artificial example, usually the default tolerance is good enough)"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-edge-cases",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-edge-cases",
    "title": "Unit testing in R",
    "section": "Testing edge cases",
    "text": "Testing edge cases\n\n\nRemember the validation steps we built into our function to handle edge cases?\n\nLet’s write tests for these edge cases:\nwe expect errors\n\n\ntest_that(\"my_function works\", {\n  expect_error(my_function(5, 0))\n  expect_error(my_function(\"a\", 3))\n  expect_error(my_function(3, \"a\"))\n  expect_error(my_function(1:2, 4))\n})\n\nTest passed 🌈"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#another-simple-example",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#another-simple-example",
    "title": "Unit testing in R",
    "section": "Another (simple) example",
    "text": "Another (simple) example\n\n\n\nmy_new_function &lt;- function(x, y) {\n  if (x &gt; y) {\n    \"x\"\n  } else {\n    \"y\"\n  }\n}\n\n\nConsider this function - there is branched logic, so we need to carefully design tests to validate the logic works as intended."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#another-simple-example-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#another-simple-example-1",
    "title": "Unit testing in R",
    "section": "Another (simple) example",
    "text": "Another (simple) example\n\nmy_new_function &lt;- function(x, y) {\n  if (x &gt; y) {\n    \"x\"\n  } else {\n    \"y\"\n  }\n}\n\n\n\ntest_that(\"it returns 'x' if x is bigger than y\", {\n  expect_equal(my_new_function(4, 3), \"x\")\n})\n\nTest passed 🎉\n\ntest_that(\"it returns 'y' if y is bigger than x\", {\n  expect_equal(my_new_function(3, 4), \"y\")\n  expect_equal(my_new_function(3, 3), \"y\")\n})\n\nTest passed 🥳"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-to-design-good-tests",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#how-to-design-good-tests",
    "title": "Unit testing in R",
    "section": "How to design good tests",
    "text": "How to design good tests\na non-exhaustive list\n\nconsider all the functions arguments,\nwhat are the expected values for these arguments?\nwhat are unexpected values, and are they handled?\nare there edge cases that need to be handled?\nhave you covered all of the different paths in your code?\nhave you managed to create tests that check the range of results you expect?"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#but-why-create-tests",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#but-why-create-tests",
    "title": "Unit testing in R",
    "section": "But, why create tests?",
    "text": "But, why create tests?\nanother non-exhaustive list\n\ngood tests will help you uncover existing issues in your code\nwill defend you from future changes that break existing functionality\nwill alert you to changes in dependencies that may have changed the functionality of your code\ncan act as documentation for other developers"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-complex-functions",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#testing-complex-functions",
    "title": "Unit testing in R",
    "section": "Testing complex functions",
    "text": "Testing complex functions\n\n\n\nmy_big_function &lt;- function(type) {\n  con &lt;- dbConnect(RSQLite::SQLite(), \"data.db\")\n  df &lt;- tbl(con, \"data_table\") |&gt;\n    collect() |&gt;\n    mutate(across(date, lubridate::ymd))\n\n  conditions &lt;- read_csv(\n    \"conditions.csv\", col_types = \"cc\"\n  ) |&gt;\n    filter(condition_type == type)\n\n  df |&gt;\n    semi_join(conditions, by = \"condition\") |&gt;\n    count(date) |&gt;\n    ggplot(aes(date, n)) +\n    geom_line() +\n    geom_point()\n}\n\n\nWhere do you even begin to start writing tests for something so complex?\n\n\nNote: to get the code on the left to fit on one page, I skipped including a few library calls\n\nlibrary(tidyverse)\nlibrary(DBI)"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions",
    "title": "Unit testing in R",
    "section": "Split the logic into smaller functions",
    "text": "Split the logic into smaller functions\nFunction to get the data from the database\n\nget_data_from_sql &lt;- function() {\n  con &lt;- dbConnect(RSQLite::SQLite(), \"data.db\")\n  tbl(con, \"data_table\") |&gt;\n    collect() |&gt;\n    mutate(across(date, lubridate::ymd))\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-1",
    "title": "Unit testing in R",
    "section": "Split the logic into smaller functions",
    "text": "Split the logic into smaller functions\nFunction to get the relevant conditions\n\nget_conditions &lt;- function(type) {\n  read_csv(\n    \"conditions.csv\", col_types = \"cc\"\n  ) |&gt;\n    filter(condition_type == type)\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-2",
    "title": "Unit testing in R",
    "section": "Split the logic into smaller functions",
    "text": "Split the logic into smaller functions\nFunction to combine the data and create a count by date\n\nsummarise_data &lt;- function(df, conditions) {\n  df |&gt;\n    semi_join(conditions, by = \"condition\") |&gt;\n    count(date)\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-3",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-3",
    "title": "Unit testing in R",
    "section": "Split the logic into smaller functions",
    "text": "Split the logic into smaller functions\nFunction to generate a plot from the summarised data\n\ncreate_plot &lt;- function(df) {\n  df |&gt;\n    ggplot(aes(date, n)) +\n    geom_line() +\n    geom_point()\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-4",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#split-the-logic-into-smaller-functions-4",
    "title": "Unit testing in R",
    "section": "Split the logic into smaller functions",
    "text": "Split the logic into smaller functions\nThe original function refactored to use the new functions\n\nmy_big_function &lt;- function(type) {\n  conditions &lt;- get_conditions(type)\n\n  get_data_from_sql() |&gt;\n    summarise_data(conditions) |&gt;\n    create_plot()\n}\n\n\nThis is going to be significantly easier to test, because we now can verify that the individual components work correctly, rather than having to consider all of the possibilities at once."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data",
    "title": "Unit testing in R",
    "section": "Let’s test summarise_data",
    "text": "Let’s test summarise_data\nsummarise_data &lt;- function(df, conditions) {\n  df |&gt;\n    semi_join(conditions, by = \"condition\") |&gt;\n    count(date)\n}"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-1",
    "title": "Unit testing in R",
    "section": "Let’s test summarise_data",
    "text": "Let’s test summarise_data\ntest_that(\"it summarises the data\", {\n  # arrange\n  \n\n\n\n\n\n\n  \n\n  \n  # act\n  \n  # assert\n  \n})"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-2",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-2",
    "title": "Unit testing in R",
    "section": "Let’s test summarise_data",
    "text": "Let’s test summarise_data\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  \n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  \n\n\n\n\n  # act\n  \n  # assert\n  \n})\n\nGenerate some random data to build a reasonably sized data frame.\nYou could also create a table manually, but part of the trick of writing good tests for this function is to make it so the dates don’t all have the same count.\nThe reason for this is it’s harder to know for sure that the count worked if every row returns the same value.\nWe don’t need the values to be exactly like they are in the real data, just close enough. Instead of dates, we can use numbers, and instead of actual conditions, we can use letters."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-3",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-3",
    "title": "Unit testing in R",
    "section": "Let’s test summarise_data",
    "text": "Let’s test summarise_data\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  set.seed(123)\n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  \n\n\n\n\n  # act\n  \n  # assert\n  \n})\n\nTests need to be reproducible, and generating our table at random will give us unpredictable results.\nSo, we need to set the random seed; now every time this test runs we will generate the same data."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-4",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-4",
    "title": "Unit testing in R",
    "section": "Let’s test summarise_data",
    "text": "Let’s test summarise_data\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  set.seed(123)\n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  conditions &lt;- tibble(condition = c(\"a\", \"b\"))    \n  \n\n\n\n  # act\n  \n  # assert\n  \n})\n\nCreate the conditions table. We don’t need all of the columns that are present in the real csv, just the ones that will make our code work.\nWe also need to test that the filtering join (semi_join) is working, so we want to use a subset of the conditions that were used in df."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-5",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-5",
    "title": "Unit testing in R",
    "section": "Let’s test summarise_data",
    "text": "Let’s test summarise_data\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  set.seed(123)\n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  conditions &lt;- tibble(condition = c(\"a\", \"b\"))    \n  \n  \n\n  \n  # act\n  actual &lt;- summarise_data(df, conditions)\n  # assert\n  \n})\n\nBecause we are generating df randomly, to figure out what our “expected” results are, I simply ran the code inside of the test to generate the “actual” results.\nGenerally, this isn’t a good idea. You are creating the results of your test from the code; ideally, you want to be thinking about what the results of your function should be.\nImagine your function doesn’t work as intended, there is some subtle bug that you are not yet aware of. By writing tests “backwards” you may write test cases that confirm the results, but not expose the bug. This is why it’s good to think about edge cases."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-6",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-6",
    "title": "Unit testing in R",
    "section": "Let’s test summarise_data",
    "text": "Let’s test summarise_data\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  set.seed(123)\n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  conditions &lt;- tibble(condition = c(\"a\", \"b\"))    \n  expected &lt;- tibble(\n    date = 1:10,\n    n = c(19, 18, 12, 14, 17, 18, 24, 18, 31, 21)\n  )  \n  # act\n  actual &lt;- summarise_data(df, conditions)\n  # assert\n  \n})\n\nThat said, in cases where we can be confident (say by static analysis of our code) that it is correct, building tests in this way will give us the confidence going forwards that future changes do not break existing functionality.\nIn this case, I have created the expected data frame using the results from running the function."
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-7",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#lets-test-summarise_data-7",
    "title": "Unit testing in R",
    "section": "Let’s test summarise_data",
    "text": "Let’s test summarise_data\n\n\n\ntest_that(\"it summarises the data\", {\n  # arrange\n  set.seed(123)\n  df &lt;- tibble(\n    date = sample(1:10, 300, TRUE),\n    condition = sample(c(\"a\", \"b\", \"c\"), 300, TRUE)\n  )\n  conditions &lt;- tibble(condition = c(\"a\", \"b\"))\n  expected &lt;- tibble(\n    date = 1:10,\n    n = c(19, 18, 12, 14, 17, 18, 24, 18, 31, 21)\n  )\n  # act\n  actual &lt;- summarise_data(df, conditions)\n  # assert\n  expect_equal(actual, expected)\n})\n\nTest passed 😸\n\n\n\nThe test works!"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#next-steps",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#next-steps",
    "title": "Unit testing in R",
    "section": "Next steps",
    "text": "Next steps\n\nYou can add tests to any R project (to test functions),\nBut {testthat} works best with Packages\nThe R Packages book has 3 chapters on testing\nThere are two useful helper functions in {usethis}\n\nuse_testthat() will set up the folders for test scripts\nuse_test() will create a test file for the currently open script"
  },
  {
    "objectID": "presentations/2023-08-23_nhs-r_unit-testing/index.html#next-steps-1",
    "href": "presentations/2023-08-23_nhs-r_unit-testing/index.html#next-steps-1",
    "title": "Unit testing in R",
    "section": "Next steps",
    "text": "Next steps\n\nIf your test needs to temporarily create a file, or change some R-options, the {withr} package has a lot of useful functions that will automatically clean things up when the test finishes\nIf you are writing tests that involve calling out to a database, or you want to test my_big_function (from before) without calling the intermediate functions, then you should look at the {mockery} package"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#how-did-we-get-here",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#how-did-we-get-here",
    "title": "Agile and scrum working",
    "section": "How did we get here?",
    "text": "How did we get here?\n\nWaterfall approaches were used in the early days of software development\n\nRequirements; Design; Development; Integration; Testing; Deployment\n\nYou only move to the next stage when the first one is complete\n(although actually it turns out you kind of don’t…)"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#the-road-to-agile",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#the-road-to-agile",
    "title": "Agile and scrum working",
    "section": "The road to agile",
    "text": "The road to agile\n\nSome of the ideas for agile floated around in the 20th century\nShewart’s Plan-Do-Study-Act cycle\nThe New New Product Development Game in 1986\nScrum (which we’ll return to) was proposed in 1993\nIn 2001 the Manifesto for Agile Software Development was published"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#the-agile-manifesto",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#the-agile-manifesto",
    "title": "Agile and scrum working",
    "section": "The agile manifesto",
    "text": "The agile manifesto\n\nCopyright © 2001 Kent Beck, Mike Beedle, Arie van Bennekum, Alistair Cockburn, Ward Cunningham, Martin Fowler, James Grenning, Jim Highsmith, Andrew Hunt, Ron Jeffries, Jon Kern, Brian Marick\nRobert C. Martin, Steve Mellor, Ken Schwaber, Jeff Sutherland, Dave Thomas\nthis declaration may be freely copied in any form, but only in its entirety through this notice."
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#agile-principles--software-and-the-mvp",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#agile-principles--software-and-the-mvp",
    "title": "Agile and scrum working",
    "section": "Agile principles- software and the MVP",
    "text": "Agile principles- software and the MVP\n\nOur highest priority is to satisfy the customer through early and continuous delivery of valuable software.\nDeliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale.\nWorking software is the primary measure of progress.\n\n(these principles and those on following slides copyright Ibid.)"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#agile-principles--working-with-customers",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#agile-principles--working-with-customers",
    "title": "Agile and scrum working",
    "section": "Agile principles- working with customers",
    "text": "Agile principles- working with customers\n\nWelcome changing requirements, even late in development. Agile processes harness change for the customer’s competitive advantage.\nBusiness people and developers must work together daily throughout the project."
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#agile-principles--teamwork",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#agile-principles--teamwork",
    "title": "Agile and scrum working",
    "section": "Agile principles- teamwork",
    "text": "Agile principles- teamwork\n\nBuild projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done.\nThe most efficient and effective method of conveying information to and within a development team is face-to-face conversation.\nThe best architectures, requirements, and designs emerge from self-organizing teams.\nAt regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly."
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#agile-principles--project-management",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#agile-principles--project-management",
    "title": "Agile and scrum working",
    "section": "Agile principles- project management",
    "text": "Agile principles- project management\n\nAgile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely.\nContinuous attention to technical excellence and good design enhances agility.\nSimplicity–the art of maximizing the amount of work not done–is essential."
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#the-agile-advantage",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#the-agile-advantage",
    "title": "Agile and scrum working",
    "section": "The agile advantage",
    "text": "The agile advantage\n\nBetter use of fixed resources to deliver an unknown outcome, rather than unknown resources to deliver a fixed outcome\nContinuous delivery"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#feature-creep",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#feature-creep",
    "title": "Agile and scrum working",
    "section": "Feature creep",
    "text": "Feature creep\n\nUsers ask for: everything they need, everything they think they may need, everything they want, everything they think they may want\n\n“every program attempts to expand until it can read mail. Those programs which cannot so expand are replaced by ones which can”\n\nZawinski’s Law- Source"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#regular-stakeholder-feedback",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#regular-stakeholder-feedback",
    "title": "Agile and scrum working",
    "section": "Regular stakeholder feedback",
    "text": "Regular stakeholder feedback\n\nAgile teams are very responsive to product feedback\nThe project we’re curently working on is very agile whether we like it or not\nOur customers never know what they want until we show them something they don’t want"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#more-agile-advantages",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#more-agile-advantages",
    "title": "Agile and scrum working",
    "section": "More agile advantages",
    "text": "More agile advantages\n\nEarly and cheap failure\nContinuous testing and QA\nReduction in unproductive work\nTeam can improve regularly, not just the product"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#agile-methods",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#agile-methods",
    "title": "Agile and scrum working",
    "section": "Agile methods",
    "text": "Agile methods\n\nThere are lots of agile methodologies\nI’m not going to embarrass myself by pretending to understand them\nExamples include Lean, Crystal, and Extreme Programming"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#scrum",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#scrum",
    "title": "Agile and scrum working",
    "section": "Scrum",
    "text": "Scrum\n\nScrum is the agile methodology we have adopted\nDespite dire warnings to the contrary we have not adopted it wholesale but most of its principles\nThe fundamental organising principle of work in scrum is a sprint lasting 1-4 weeks\nEach sprint finishes with a defined and useful piece of software that can be shown to/ used by customers"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#product-owner",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#product-owner",
    "title": "Agile and scrum working",
    "section": "Product owner",
    "text": "Product owner\n\nThis person is responsible for the backlog- what goes in to the sprint\nThe backlog should be inclusive of all of the things that customers want or might want\nThe backlog should be prioritised\nThe product owner does this through deep and frequent conversations with customers"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#scrum-master-helps-the-scrum-team",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#scrum-master-helps-the-scrum-team",
    "title": "Agile and scrum working",
    "section": "Scrum master helps the scrum team",
    "text": "Scrum master helps the scrum team\n\n“By coaching the team members in self-management and cross-functionality\nFocus on creating high-value Increments that meet the Definition of Done\nInfluence the removal of impediments to the Scrum Team’s progress\nEnsure that all Scrum events take place and are positive, productive, and kept within the timebox.”\n\nSource"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#the-backlog",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#the-backlog",
    "title": "Agile and scrum working",
    "section": "The backlog",
    "text": "The backlog\n\nHaving an accurate and well prioritised backlog is key\nDon’t estimate the backlog in hours- use “T shirt sizes” or “points”\nPeople are terrible at estimating how long things take- particularly in software\nEverything in the backlog needs a defined “Done” state"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#sprint-planning",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#sprint-planning",
    "title": "Agile and scrum working",
    "section": "Sprint planning",
    "text": "Sprint planning\n\nThe team, the product owner, and the scrum master plan the sprint\nSprints should be a fixed length of time less than one month\nThe sprint cannot be changed or added to (we break this rule)\nThe team works autonomously in the sprint- nobody decides who does what except the team\nCan take three hours and should if it needs to"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#standup",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#standup",
    "title": "Agile and scrum working",
    "section": "Standup",
    "text": "Standup\n\nEvery day, for no more than 15 minutes (teams often stand up to reinforce this rule) team and scrum master meet\nEach person answers three questions\n\nWhat did you do yesterday to help the team finish the sprint?\nWhat will you do today to help the team finish the sprint?\nIs there an obstacle blocking you or the team from achieveing the sprint goal"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#sprint-retro",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#sprint-retro",
    "title": "Agile and scrum working",
    "section": "Sprint retro",
    "text": "Sprint retro\n\nWhat went well, what could have gone better, and what to improve next time\nLooking at process, not blaming individuals\nRequires maturity and trust to bring up issues, and to respond to them in a constructive way\nShould agree at the end on one process improvement which goes in the next sprint\nWe’ve had some really, really good retros and I think it’s a really important process for a team"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#team-perspective",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#team-perspective",
    "title": "Agile and scrum working",
    "section": "Team perspective",
    "text": "Team perspective\n\nProduct owner- that’s me\n\nFocus, clarity and transparency, team delivery, clear and appropriate responsibilities\n\nScrum master- YiWen\nTeam member- Matt\nTeam member- Rhian"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#scrum-values",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#scrum-values",
    "title": "Agile and scrum working",
    "section": "Scrum values",
    "text": "Scrum values\n\nCourage\nFocus\nCommitment\nRespect\nOpenness"
  },
  {
    "objectID": "presentations/2024-08-22_agile-and-scrum/index.html#using-agile-outside-of-software",
    "href": "presentations/2024-08-22_agile-and-scrum/index.html#using-agile-outside-of-software",
    "title": "Agile and scrum working",
    "section": "Using agile outside of software",
    "text": "Using agile outside of software\n\nData science is outside of software (IMHO)\n\nWe don’t have daily standups and some of our processes run longer than in software development\n\nYou can build cars with Agile\nMarketing and UX design"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science @ The Strategy Unit",
    "section": "",
    "text": "This is the home of Data Science activities at The Strategy Unit.\nHere, we host information about how we work, links to presentations, and blogposts relating to how we utilise data science tools.\nAll members of the Strategy Unit are welcome to contribute."
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "Data Science Blog",
    "section": "",
    "text": "Imputing missing data\n\n\n(Sometimes) making things up can improve the quality of your analysis\n\n\n\nR\n\nImputation\n\nLearning\n\nMICE\n\n\n\n\n\n\n\n\n\nJul 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does a scrummaster do, anyway?\n\n\nHow we manage sprints at the Strategy Unit Data Science team\n\n\n\nGitHub\n\nScrum\n\nAgile\n\n\n\n\n\n\n\n\n\nJun 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nData science as a product\n\n\nBringing together analysts, data scientists, and customers to better serve patients and the whole NHS\n\n\n\nGitHub\n\nScrum\n\nAgile\n\n\n\n\n\n\n\n\n\nMay 16, 2025\n\n\nClaire Welsh\n\n\n\n\n\n\n\n\n\n\n\n\nTaking a Hackathon approach to exploring new methods in NLP\n\n\n\n\n\n\nlearning\n\nNLP\n\nAI\n\n\n\n\n\n\n\n\n\nMay 6, 2025\n\n\nYiWen Hon\n\n\n\n\n\n\n\n\n\n\n\n\nUsing GitHub Apps for Authentication\n\n\n\n\n\n\nGitHub\n\nlearning\n\ndeployment\n\n\n\n\n\n\n\n\n\nApr 30, 2025\n\n\nThomas Jemmett\n\n\n\n\n\n\n\n\n\n\n\n\nMapping my R journey so far: ten things that I have done along the way\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\nMar 10, 2025\n\n\nSheila Ali\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to text vectorization\n\n\n\n\n\n\nNLP\n\nPython\n\nTutorial\n\n\n\nWhat is it and why do we care? First steps in Natural Language Processing\n\n\n\n\n\nJan 3, 2025\n\n\nYiWen Hon\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy previews with GitHub pages\n\n\n\n\n\n\nGitHub\n\nlearning\n\ndeployment\n\n\n\nWouldn’t it be nice if when a PR is created, you automatically got a deployed version of your changes to look at?\n\n\n\n\n\nDec 4, 2024\n\n\nRhian Davies\n\n\n\n\n\n\n\n\n\n\n\n\nUsing GitHub to plan and organise Coffee & Coding\n\n\n\n\n\n\nGitHub\n\nlearning\n\n\n\n\n\n\n\n\n\nNov 12, 2024\n\n\nYiWen Hon\n\n\n\n\n\n\n\n\n\n\n\n\nMap and Nest\n\n\n\n\n\n\npurrr\n\nR\n\ntutorial\n\n\n\n\n\n\n\n\n\nAug 8, 2024\n\n\nRhian Davies\n\n\n\n\n\n\n\n\n\n\n\n\nStoring data safely\n\n\n\n\n\n\nlearning\n\nR\n\nPython\n\n\n\n\n\n\n\n\n\nMay 22, 2024\n\n\nYiWen Hon, Matt Dray, Claire Welsh\n\n\n\n\n\n\n\n\n\n\n\n\nOne year of coffee & coding\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\nRhian Davies\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio Tips and Tricks\n\n\n\n\n\n\nlearning\n\nR\n\n\n\n\n\n\n\n\n\nMar 21, 2024\n\n\nMatt Dray\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising participant recruitment in R using Sankey plots\n\n\n\n\n\n\nlearning\n\ntutorial\n\nvisualisation\n\nR\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nCraig Parylo\n\n\n\n\n\n\n\n\n\n\n\n\nNearest neighbour imputation\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nJacqueline Grout\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code and Test Driven Development\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\nJan 10, 2024\n\n\nYiWen Hon\n\n\n\n\n\n\n\n\n\n\n\n\nReinstalling R Packages\n\n\n\n\n\n\ngit\n\ntutorial\n\n\n\n\n\n\n\n\n\nApr 26, 2023\n\n\nTom Jemmett\n\n\n\n\n\n\n\n\n\n\n\n\nAlternative remote repositories\n\n\n\n\n\n\ngit\n\ntutorial\n\n\n\n\n\n\n\n\n\nApr 26, 2023\n\n\nTom Jemmett\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a hotfix with git\n\n\n\n\n\n\ngit\n\ntutorial\n\n\n\n\n\n\n\n\n\nMar 24, 2023\n\n\nTom Jemmett\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs/posts/2024-05-22_storing-data-safely/azure_python.html",
    "href": "blogs/posts/2024-05-22_storing-data-safely/azure_python.html",
    "title": "Data Science @ The Strategy Unit",
    "section": "",
    "text": "import os\nimport io\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom azure.identity import DefaultAzureCredential\nfrom azure.storage.blob import ContainerClient\n\n\n# Load all environment variables\nload_dotenv()\naccount_url = os.getenv('AZ_STORAGE_EP')\ncontainer_name = os.getenv('AZ_STORAGE_CONTAINER')\n\n\n# Authenticate\ndefault_credential = DefaultAzureCredential()\n\nFor the first time, you might need to authenticate via the Azure CLI\nDownload it from https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-windows?tabs=azure-cli\nInstall then run az login in your terminal. Once you have logged in with your browser try the DefaultAzureCredential() again!\n\n# Connect to container\ncontainer_client = ContainerClient(account_url, container_name, default_credential)\n\n\n# List files in container - should be empty\nblob_list = container_client.list_blob_names()\nfor blob in blob_list:\n    if blob.startswith('newdir'):\n        print(blob)\n\nnewdir/cats.parquet\nnewdir/ronald.jpeg\n\n\n\n# Upload file to container\nwith open(file='data/cats.csv', mode=\"rb\") as data:\n    blob_client = container_client.upload_blob(name='newdir/cats.csv', \n                                               data=data, \n                                               overwrite=True)\n\n\n# # Check files have uploaded - List files in container again\nblob_list = container_client.list_blobs()\nfor blob in blob_list:\n    if blob['name'].startswith('newdir'):\n        print(blob['name'])\n\nnewdir/cats.csv\nnewdir/cats.parquet\nnewdir/ronald.jpeg\n\n\n\n# Download file from Azure container to temporary filepath\n\n# Connect to blob\nblob_client = container_client.get_blob_client('newdir/cats.csv')\n\n# Write to local file from blob\ntemp_filepath = os.path.join('temp_data', 'cats.csv')\nwith open(file=temp_filepath, mode=\"wb\") as sample_blob:\n    download_stream = blob_client.download_blob()\n    sample_blob.write(download_stream.readall())\ncat_data = pd.read_csv(temp_filepath)\ncat_data.head()\n\n\n\n\n\n\n\n\nName\nPhysical_characteristics\nBehaviour\n\n\n\n\n0\nRonald\nWhite and ginger\nLazy and greedy but undoubtedly cutest and best\n\n\n1\nKaspie\nSmall calico\nSweet and very shy but adventurous\n\n\n2\nHennimore\nPale orange\nUnhinged and always in a state of panic\n\n\n3\nThug cat\nBlack and white - very large\nLocal bully\n\n\n4\nSon of Stripey\nGrey tabby\nVery vocal\n\n\n\n\n\n\n\n\n# Load directly from Azure - no local copy\n\ndownload_stream = blob_client.download_blob()\nstream_object = io.BytesIO(download_stream.readall())\ncat_data = pd.read_csv(stream_object)\ncat_data\n\n\n\n\n\n\n\n\nName\nPhysical_characteristics\nBehaviour\n\n\n\n\n0\nRonald\nWhite and ginger\nLazy and greedy but undoubtedly cutest and best\n\n\n1\nKaspie\nSmall calico\nSweet and very shy but adventurous\n\n\n2\nHennimore\nPale orange\nUnhinged and always in a state of panic\n\n\n3\nThug cat\nBlack and white - very large\nLocal bully\n\n\n4\nSon of Stripey\nGrey tabby\nVery vocal\n\n\n\n\n\n\n\n\n# !!!!!!!!! Delete from Azure container !!!!!!!!!\nblob_client = container_client.get_blob_client('newdir/cats.csv')\nblob_client.delete_blob()\n\n\nblob_list = container_client.list_blobs()\nfor blob in blob_list:\n    if blob['name'].startswith('newdir'):\n        print(blob['name'])\n\nnewdir/cats.parquet\nnewdir/ronald.jpeg"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "This post is comprised of the Jupyter notebook that was used during a Coffee & Coding session providing an overview of text vectorization, a key concept in Natural Language Processing.\nLet’s take as our first example a dataset of reviews from IMDB. The aim is to try and classify if the review had a positive or negative score, based on the words in the text.\n\nimport pandas as pd\npd.set_option('display.max_colwidth', 400)\n# Dataset from  https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download\ndata = pd.read_csv('IMDB Dataset.csv')\ndata\n\n\n\n\n\n\n\n\nreview\nsentiment\n\n\n\n\n0\nOne of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.&lt;br /&gt;&lt;br /&gt;The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regard...\npositive\n\n\n1\nA wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. &lt;br /&gt;&lt;br /&gt;The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the ref...\npositive\n\n\n2\nI thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof...\npositive\n\n\n3\nBasically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.&lt;br /&gt;&lt;br /&gt;This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.&lt;br /&gt;&lt;br /&gt;OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Paren...\nnegative\n\n\n4\nPetter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. &lt;br /&gt;&lt;br /&gt;This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action t...\npositive\n\n\n...\n...\n...\n\n\n49995\nI thought this movie did a down right good job. It wasn't as creative or original as the first, but who was expecting it to be. It was a whole lotta fun. the more i think about it the more i like it, and when it comes out on DVD I'm going to pay the money for it very proudly, every last cent. Sharon Stone is great, she always is, even if her movie is horrible(Catwoman), but this movie isn't, t...\npositive\n\n\n49996\nBad plot, bad dialogue, bad acting, idiotic directing, the annoying porn groove soundtrack that ran continually over the overacted script, and a crappy copy of the VHS cannot be redeemed by consuming liquor. Trust me, because I stuck this turkey out to the end. It was so pathetically bad all over that I had to figure it was a fourth-rate spoof of Springtime for Hitler.&lt;br /&gt;&lt;br /&gt;The girl who ...\nnegative\n\n\n49997\nI am a Catholic taught in parochial elementary schools by nuns, taught by Jesuit priests in high school & college. I am still a practicing Catholic but would not be considered a \"good Catholic\" in the church's eyes because I don't believe certain things or act certain ways just because the church tells me to.&lt;br /&gt;&lt;br /&gt;So back to the movie...its bad because two people are killed by this nun w...\nnegative\n\n\n49998\nI'm going to have to disagree with the previous comment and side with Maltin on this one. This is a second rate, excessively vicious Western that creaks and groans trying to put across its central theme of the Wild West being tamed and kicked aside by the steady march of time. It would like to be in the tradition of \"Butch Cassidy and the Sundance Kid\", but lacks that film's poignancy and char...\nnegative\n\n\n49999\nNo one expects the Star Trek movies to be high art, but the fans do expect a movie that is as good as some of the best episodes. Unfortunately, this movie had a muddled, implausible plot that just left me cringing - this is by far the worst of the nine (so far) movies. Even the chance to watch the well known characters interact in another movie can't save this movie - including the goofy scene...\nnegative\n\n\n\n\n50000 rows × 2 columns\n\n\n\n\n# Our dataset is quite balanced, with an equal number of positive and negative reviews.\ndata['sentiment'].value_counts()\n\nsentiment\npositive    25000\nnegative    25000\nName: count, dtype: int64\n\n\n\n# In this cell, we are trying to use a very basic machine learning model (Multinomial Naive Bayes) \n# to predict the sentiment of the text (whether it was positive or negative) based on the text.\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_validate\n\nnaivebayes = MultinomialNB()\n\nX = data.review\n\ncv_nb = cross_validate(\n    naivebayes,\n    X,\n    data.sentiment,\n    scoring = \"accuracy\"\n)\n\nround(cv_nb['test_score'].mean(),2)\n\n# ⚠️ Uh oh!! we're getting an error... let's decode it together\n# ValueError: could not convert string to float (it doesn't like the text being as a string!)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[3], line 11\n      7 naivebayes = MultinomialNB()\n      9 X = data.review\n---&gt; 11 cv_nb = cross_validate(\n     12     naivebayes,\n     13     X,\n     14     data.sentiment,\n     15     scoring = \"accuracy\"\n     16 )\n     18 round(cv_nb['test_score'].mean(),2)\n\nFile c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211, in validate_params.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs)\n    205 try:\n    206     with config_context(\n    207         skip_parameter_validation=(\n    208             prefer_skip_nested_validation or global_skip_validation\n    209         )\n    210     ):\n--&gt; 211         return func(*args, **kwargs)\n    212 except InvalidParameterError as e:\n    213     # When the function is just a wrapper around an estimator, we allow\n    214     # the function to delegate validation to the estimator, but we replace\n    215     # the name of the estimator by the name of the function in the error\n    216     # message to avoid confusion.\n    217     msg = re.sub(\n    218         r\"parameter of \\w+ must be\",\n    219         f\"parameter of {func.__qualname__} must be\",\n    220         str(e),\n    221     )\n\nFile c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:328, in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\n    308 parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n    309 results = parallel(\n    310     delayed(_fit_and_score)(\n    311         clone(estimator),\n   (...)\n    325     for train, test in indices\n    326 )\n--&gt; 328 _warn_or_raise_about_fit_failures(results, error_score)\n    330 # For callable scoring, the return type is only know after calling. If the\n    331 # return type is a dictionary, the error scores can now be inserted with\n    332 # the correct key.\n    333 if callable(scoring):\n\nFile c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:414, in _warn_or_raise_about_fit_failures(results, error_score)\n    407 if num_failed_fits == num_fits:\n    408     all_fits_failed_message = (\n    409         f\"\\nAll the {num_fits} fits failed.\\n\"\n    410         \"It is very likely that your model is misconfigured.\\n\"\n    411         \"You can try to debug the error by setting error_score='raise'.\\n\\n\"\n    412         f\"Below are more details about the failures:\\n{fit_errors_summary}\"\n    413     )\n--&gt; 414     raise ValueError(all_fits_failed_message)\n    416 else:\n    417     some_fits_failed_message = (\n    418         f\"\\n{num_failed_fits} fits failed out of a total of {num_fits}.\\n\"\n    419         \"The score on these train-test partitions for these parameters\"\n   (...)\n    423         f\"Below are more details about the failures:\\n{fit_errors_summary}\"\n    424     )\n\nValueError: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 745, in fit\n    X, y = self._check_X_y(X, y)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 578, in _check_X_y\n    return self._validate_data(X, y, accept_sparse=\"csr\", reset=reset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1147, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\pandas\\core\\series.py\", line 1022, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: '\"Sorte Nula\" is the #1 Box Office Portuguese movie of 2004. This extreme low budget production (estimated USD$150,000) opened during Christmas opposite American Blockbusters like National Treasure, Polar Express, The Incredibles and Alexander but rapidly caught the adulation of the Portuguese moviegoers. Despite the harsh competition, the small film did surprisingly well, topping all other Portuguese films of the past two years in its first weeks. The film is a mystery/murder with a humorous tone cleverly written and directed by Fernando Fragata who has become a solid reference in the European independent film arena. Did I like the film? Oh, yes!'\n\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 745, in fit\n    X, y = self._check_X_y(X, y)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 578, in _check_X_y\n    return self._validate_data(X, y, accept_sparse=\"csr\", reset=reset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1147, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Yiwen.Hon\\AppData\\Local\\miniconda3\\envs\\nlp\\Lib\\site-packages\\pandas\\core\\series.py\", line 1022, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: \"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.&lt;br /&gt;&lt;br /&gt;The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.&lt;br /&gt;&lt;br /&gt;It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.&lt;br /&gt;&lt;br /&gt;I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\"\n\n\n\n\n\n\nWhen working with text data, computers need to convert the words into numbers first before being able to work with them. Hence vectorisation - the process of converting numbers into words. There are a few different approaches and concepts which we’ll explore today\n\nTokenization\nBag of words\nTF-IDF\nn-grams\nWord2Vec embeddings\n\nFinally we’ll look at (at a very very high level!) how Transformer/attention-based approaches to word vectorisation have transformed NLP\n\n\n\nBreaking up texts into their individual components, or tokens\n\nfrom nltk.tokenize import word_tokenize\n\ntext = \"Had a slight weapons malfunction but, uh everything's perfectly all right now. We're fine. We're all fine here now. Thank you. How are you?\"\n\n# Document before tokenization\nprint(text)\n\nHad a slight weapons malfunction but, uh everything's perfectly all right now. We're fine. We're all fine here now. Thank you. How are you?\n\n\n\nword_tokens = word_tokenize(text)\n\n# Document after tokenization - each word is separated out. Compound words like \"everything's\" are now two words: \"everything\" and \"'s\"\nprint(word_tokens)\n\n['Had', 'a', 'slight', 'weapons', 'malfunction', 'but', ',', 'uh', 'everything', \"'s\", 'perfectly', 'all', 'right', 'now', '.', 'We', \"'re\", 'fine', '.', 'We', \"'re\", 'all', 'fine', 'here', 'now', '.', 'Thank', 'you', '.', 'How', 'are', 'you', '?']\n\n\n\n\n\n\nTokens: how we’ve broken down the text into smaller units\nDocument: the unit of text we’re analysing. Could be sentences, could be paragraphs, could be a whole book. Different breakdowns for different purposes\nCorpus: The collection of documents being analysed\n\n\n\n\n\ntexts = [\n    'I love to run',\n    'the cat does not eat fruit',\n    'run to the cat',\n    'I love to eat fruit. fruit fruit fruit fruit'\n]\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer = CountVectorizer()\nX = count_vectorizer.fit_transform(texts)\nX.toarray()\n\narray([[0, 0, 0, 0, 1, 0, 1, 0, 1],\n       [1, 1, 1, 1, 0, 1, 0, 1, 0],\n       [1, 0, 0, 0, 0, 0, 1, 1, 1],\n       [0, 0, 1, 5, 1, 0, 0, 0, 1]], dtype=int64)\n\n\nEach column is a different word, and the count vectorizer simply counts how many appearances of each word are in each sentence.\n🤔 Can you guess which column represents which word?\nIt’s column 4: the word “fruit” appears 5 times in the last sentence.\n\n# Visualising what the vectorizer has done\n\nvectorized_texts = pd.DataFrame(\n    X.toarray(),\n    columns = count_vectorizer.get_feature_names_out(),\n    index = texts\n)\n\nvectorized_texts\n\n\n\n\n\n\n\n\ncat\ndoes\neat\nfruit\nlove\nnot\nrun\nthe\nto\n\n\n\n\nI love to run\n0\n0\n0\n0\n1\n0\n1\n0\n1\n\n\nthe cat does not eat fruit\n1\n1\n1\n1\n0\n1\n0\n1\n0\n\n\nrun to the cat\n1\n0\n0\n0\n0\n0\n1\n1\n1\n\n\nI love to eat fruit. fruit fruit fruit fruit\n0\n0\n1\n5\n1\n0\n0\n0\n1\n\n\n\n\n\n\n\nWe will try the same code from above - this time on the vectorised text instead of the raw text! This time we shouldn’t get any errors.\n\nnaivebayes = MultinomialNB()\ncount_vectorizer = CountVectorizer()\n\nX = count_vectorizer.fit_transform(data.review)\n\ncv_nb = cross_validate(\n    naivebayes,\n    X,\n    data.sentiment,\n    scoring = \"accuracy\"\n)\n\nround(cv_nb['test_score'].mean(),2)\n\n0.85\n\n\nOur accuracy score is 85% which isn’t too bad\nWhat are the limitations of this approach?\n\nNo context\nWord order not available\nAll words treated the same\nVery simplistic approach!\n\n\n\n\nTERM FREQUENCY (TF)\nThe more often a word appears in a document relative to others, the more likely it is that it will be important to this document\nExample: if a word appears relatively frequently in a document, it is obvious that this word is important to the overall meaning of the document.\n\n\n\nimage.png\n\n\n\ntexts = [\n    'I love to run',\n    'the cat does not eat the fruit',\n    'run to the cat',\n    'I love to eat the fruit. fruit fruit fruit fruit'\n]\n\n\n# In document 4, the Term Frequency (TF) of the word FRUIT is?\n# The word fruit appears 5 times\n# There are 10 words in the document\n\n5/10\n\n0.5\n\n\nDOCUMENT FREQUENCY (DF)\nIf a word appears in many documents of a corpus, it’s not important to understand a particular document.\nExample: on eurosport.com/football, the word “football” appears in every article, hence why the word football on this website is an unimportant word!\n\n\n\nimage.png\n\n\nFor the word “football” on Eurosport, we would expect this formula to be close to 1 since the number of docs containing the word “football” will probably only be slightly less than the total number of docs (out of 100 maybe only 5 don’t have the word “football”, so we get 95/100).\nif the word “football” appears in all the articles it is not very useful for helping us identify between two articles, but if only a few documents contain words like “concussion” or “wellbeing”, (e.g. they appear in 2/100 articles) it will be much more useful in determining the topic of that article (they are probably specifically about player wellfare).\n💡 Thus the intuition of the term frequency - inverse document frequency approach is to give a high weight to any term which appears frequently in a single document, but not in too many documents of the corpus.\n\n## ?? Which words appear frequently in our small corpus \n# and might not be useful for deriving meaning?\n\ntexts = [\n    'I love to run',\n    'the cat does not eat the fruit',\n    'run to the cat',\n    'I love to eat the fruit. fruit fruit fruit fruit'\n]\n\n# the\n# to\n\n\n\n\nimage.png\n\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Instantiating the TfidfVectorizer\ntf_idf_vectorizer = TfidfVectorizer()\n\n# Training it on the texts\nweighted_words = pd.DataFrame(tf_idf_vectorizer.fit_transform(texts).toarray(),\n                 columns = tf_idf_vectorizer.get_feature_names_out(),\n                index = texts)\n\nweighted_words\n\n\n\n\n\n\n\n\ncat\ndoes\neat\nfruit\nlove\nnot\nrun\nthe\nto\n\n\n\n\nI love to run\n0.000000\n0.000000\n0.000000\n0.000000\n0.613667\n0.000000\n0.613667\n0.000000\n0.496816\n\n\nthe cat does not eat the fruit\n0.336350\n0.426618\n0.336350\n0.336350\n0.000000\n0.426618\n0.000000\n0.544609\n0.000000\n\n\nrun to the cat\n0.549578\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.549578\n0.444931\n0.444931\n\n\nI love to eat the fruit. fruit fruit fruit fruit\n0.000000\n0.000000\n0.187942\n0.939709\n0.187942\n0.000000\n0.000000\n0.152155\n0.152155\n\n\n\n\n\n\n\nWeaknesses of this approach?\n\nword order still missing\nrelationship between words still missing\n\n\n\n\n\n# The two following sentences have the exact same representation in bag of words/ TFIDF approaches\n# However, they have very different meanings!\n\nsentences = [\n    \"I like cats but not dogs\",\n    \"I like dogs but not cats\"\n]\n\n\n# Vectorize the sentences\ncount_vectorizer = CountVectorizer()\nsentences_vectorized = count_vectorizer.fit_transform(sentences)\n\n# Show the representations in a nice DataFrame\nsentences_vectorized = pd.DataFrame(\n    sentences_vectorized.toarray(),\n    columns = count_vectorizer.get_feature_names_out(),\n    index = sentences\n)\n\n# Show the vectorized words\nsentences_vectorized\n\n\n\n\n\n\n\n\nbut\ncats\ndogs\nlike\nnot\n\n\n\n\nI like cats but not dogs\n1\n1\n1\n1\n1\n\n\nI like dogs but not cats\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n🧑🏻‍🏫 When using a bag-of-words representation, an efficient way to capture context is to consider:\n\nthe count of single tokens (unigrams)\nthe count of pairs (bigrams), triplets (trigrams), and more generally sequences of n words, also known as n-grams\n\n 4)\n😥 With a unigram vectorization, we couldn’t distinguish two sentences with the same words, despite their meaning being quite different\n\nsentences_vectorized\n\n\n\n\n\n\n\n\nbut\ncats\ndogs\nlike\nnot\n\n\n\n\nI like cats but not dogs\n1\n1\n1\n1\n1\n\n\nI like dogs but not cats\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n👩🏻‍🔬 What about a bigram vectorization?\n\n# Vectorize the sentences\ncount_vectorizer_n_gram = CountVectorizer(ngram_range = (1,2)) # BI-GRAMS\nsentences_vectorized_n_gram = count_vectorizer_n_gram.fit_transform(sentences)\n\n# Show the representations in a nice DataFrame\nsentences_vectorized_n_gram = pd.DataFrame(\n    sentences_vectorized_n_gram.toarray(),\n    columns = count_vectorizer_n_gram.get_feature_names_out(),\n    index = sentences\n)\n\n# Show the vectorized movies with bigrams (pairs of words)\nsentences_vectorized_n_gram\n\n\n\n\n\n\n\n\nbut\nbut not\ncats\ncats but\ndogs\ndogs but\nlike\nlike cats\nlike dogs\nnot\nnot cats\nnot dogs\n\n\n\n\nI like cats but not dogs\n1\n1\n1\n1\n1\n0\n1\n1\n0\n1\n0\n1\n\n\nI like dogs but not cats\n1\n1\n1\n0\n1\n1\n1\n0\n1\n1\n1\n0\n\n\n\n\n\n\n\n\n\n\nAttempting to capture semantic meaning of words in numerical format\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\nimport gensim.downloader\n\n# Lots of different pretrained embeddings we can use for free!\nprint(list(gensim.downloader.info()['models'].keys()))\n\n['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n\n\n\n# We will use a vectoriser trained on Wikipedia today\nmodel_wiki = gensim.downloader.load('glove-wiki-gigaword-50')\n\n\n# Vectors based on 2B tweets, 27B tokens, 1.2M vocab!\n# 50 dimensions\n# N.B. Words not in glove-wiki-gigaword-50 will not have vectors computed. For example, if there was a niche word or acronym like \"NHS-R\" there would not be a vector for this word.\n\nmodel_wiki[\"cat\"]\n\narray([ 0.45281 , -0.50108 , -0.53714 , -0.015697,  0.22191 ,  0.54602 ,\n       -0.67301 , -0.6891  ,  0.63493 , -0.19726 ,  0.33685 ,  0.7735  ,\n        0.90094 ,  0.38488 ,  0.38367 ,  0.2657  , -0.08057 ,  0.61089 ,\n       -1.2894  , -0.22313 , -0.61578 ,  0.21697 ,  0.35614 ,  0.44499 ,\n        0.60885 , -1.1633  , -1.1579  ,  0.36118 ,  0.10466 , -0.78325 ,\n        1.4352  ,  0.18629 , -0.26112 ,  0.83275 , -0.23123 ,  0.32481 ,\n        0.14485 , -0.44552 ,  0.33497 , -0.95946 , -0.097479,  0.48138 ,\n       -0.43352 ,  0.69455 ,  0.91043 , -0.28173 ,  0.41637 , -1.2609  ,\n        0.71278 ,  0.23782 ], dtype=float32)\n\n\n\n# King is to Queen as Man is to ...\n\nexample_1 = model_wiki[\"queen\"] - model_wiki[\"king\"] + model_wiki[\"man\"]\nmodel_wiki.most_similar(example_1)[0]\n\n('woman', 0.8903914093971252)\n\n\n\n# Similar words to cat\n\nmodel_wiki.most_similar(model_wiki[\"cat\"])\n\n[('cat', 1.0),\n ('dog', 0.9218006134033203),\n ('rabbit', 0.8487820625305176),\n ('monkey', 0.804108202457428),\n ('rat', 0.7891963124275208),\n ('cats', 0.7865270972251892),\n ('snake', 0.7798910140991211),\n ('dogs', 0.7795815467834473),\n ('pet', 0.7792249917984009),\n ('mouse', 0.7731667160987854)]\n\n\n\n# Opposite of cold...?\n\nexample_2 = model_wiki[\"good\"] - model_wiki[\"evil\"] + model_wiki[\"cold\"]\nmodel_wiki.most_similar(example_2)[0]\n\n('warm', 0.7870427966117859)\n\n\n\n\n\nThe basis of transformer-based neural networks like ChatGPT! The paper that started it all: Attention is all you need\n\nEach token (word) embedding gets projected ➡️ into 3 further vectors: the query, key and value vectors (usually 768 dimensions each)!!\nWe compute a scaled dot-product 🔴 on the query and key vectors to work out how much each word relates to those around it\nTake these scores and normalize with softmax ⤵️\nMultiply by our value vectors ❎, sum and pass to our dense neural network\n\n⚠️ TLDR: The main point is that each word is now represented by 768 * 3 numbers! This is partly what makes LLMs so powerful (and resource-hungry) ⚠️\nIn the simple bag-of-words and TFIDF approaches, each word was represented by only 1 number each\nIn more complex word embeddings each word was represented by between 50 to 300 numbers each\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\n\nR text mining book https://www.tidytextmining.com/\nHuggingface tutorials (python) https://huggingface.co/learn/nlp-course/chapter1/1\nGreat video on attention https://www.youtube.com/watch?v=zxQyTK8quyY"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#key-learning",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#key-learning",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "When working with text data, computers need to convert the words into numbers first before being able to work with them. Hence vectorisation - the process of converting numbers into words. There are a few different approaches and concepts which we’ll explore today\n\nTokenization\nBag of words\nTF-IDF\nn-grams\nWord2Vec embeddings\n\nFinally we’ll look at (at a very very high level!) how Transformer/attention-based approaches to word vectorisation have transformed NLP"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#tokenization",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#tokenization",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "Breaking up texts into their individual components, or tokens\n\nfrom nltk.tokenize import word_tokenize\n\ntext = \"Had a slight weapons malfunction but, uh everything's perfectly all right now. We're fine. We're all fine here now. Thank you. How are you?\"\n\n# Document before tokenization\nprint(text)\n\nHad a slight weapons malfunction but, uh everything's perfectly all right now. We're fine. We're all fine here now. Thank you. How are you?\n\n\n\nword_tokens = word_tokenize(text)\n\n# Document after tokenization - each word is separated out. Compound words like \"everything's\" are now two words: \"everything\" and \"'s\"\nprint(word_tokens)\n\n['Had', 'a', 'slight', 'weapons', 'malfunction', 'but', ',', 'uh', 'everything', \"'s\", 'perfectly', 'all', 'right', 'now', '.', 'We', \"'re\", 'fine', '.', 'We', \"'re\", 'all', 'fine', 'here', 'now', '.', 'Thank', 'you', '.', 'How', 'are', 'you', '?']"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#some-terminology",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#some-terminology",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "Tokens: how we’ve broken down the text into smaller units\nDocument: the unit of text we’re analysing. Could be sentences, could be paragraphs, could be a whole book. Different breakdowns for different purposes\nCorpus: The collection of documents being analysed"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#bag-of-words",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#bag-of-words",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "texts = [\n    'I love to run',\n    'the cat does not eat fruit',\n    'run to the cat',\n    'I love to eat fruit. fruit fruit fruit fruit'\n]\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer = CountVectorizer()\nX = count_vectorizer.fit_transform(texts)\nX.toarray()\n\narray([[0, 0, 0, 0, 1, 0, 1, 0, 1],\n       [1, 1, 1, 1, 0, 1, 0, 1, 0],\n       [1, 0, 0, 0, 0, 0, 1, 1, 1],\n       [0, 0, 1, 5, 1, 0, 0, 0, 1]], dtype=int64)\n\n\nEach column is a different word, and the count vectorizer simply counts how many appearances of each word are in each sentence.\n🤔 Can you guess which column represents which word?\nIt’s column 4: the word “fruit” appears 5 times in the last sentence.\n\n# Visualising what the vectorizer has done\n\nvectorized_texts = pd.DataFrame(\n    X.toarray(),\n    columns = count_vectorizer.get_feature_names_out(),\n    index = texts\n)\n\nvectorized_texts\n\n\n\n\n\n\n\n\ncat\ndoes\neat\nfruit\nlove\nnot\nrun\nthe\nto\n\n\n\n\nI love to run\n0\n0\n0\n0\n1\n0\n1\n0\n1\n\n\nthe cat does not eat fruit\n1\n1\n1\n1\n0\n1\n0\n1\n0\n\n\nrun to the cat\n1\n0\n0\n0\n0\n0\n1\n1\n1\n\n\nI love to eat fruit. fruit fruit fruit fruit\n0\n0\n1\n5\n1\n0\n0\n0\n1\n\n\n\n\n\n\n\nWe will try the same code from above - this time on the vectorised text instead of the raw text! This time we shouldn’t get any errors.\n\nnaivebayes = MultinomialNB()\ncount_vectorizer = CountVectorizer()\n\nX = count_vectorizer.fit_transform(data.review)\n\ncv_nb = cross_validate(\n    naivebayes,\n    X,\n    data.sentiment,\n    scoring = \"accuracy\"\n)\n\nround(cv_nb['test_score'].mean(),2)\n\n0.85\n\n\nOur accuracy score is 85% which isn’t too bad\nWhat are the limitations of this approach?\n\nNo context\nWord order not available\nAll words treated the same\nVery simplistic approach!"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#tf-idf",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#tf-idf",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "TERM FREQUENCY (TF)\nThe more often a word appears in a document relative to others, the more likely it is that it will be important to this document\nExample: if a word appears relatively frequently in a document, it is obvious that this word is important to the overall meaning of the document.\n\n\n\nimage.png\n\n\n\ntexts = [\n    'I love to run',\n    'the cat does not eat the fruit',\n    'run to the cat',\n    'I love to eat the fruit. fruit fruit fruit fruit'\n]\n\n\n# In document 4, the Term Frequency (TF) of the word FRUIT is?\n# The word fruit appears 5 times\n# There are 10 words in the document\n\n5/10\n\n0.5\n\n\nDOCUMENT FREQUENCY (DF)\nIf a word appears in many documents of a corpus, it’s not important to understand a particular document.\nExample: on eurosport.com/football, the word “football” appears in every article, hence why the word football on this website is an unimportant word!\n\n\n\nimage.png\n\n\nFor the word “football” on Eurosport, we would expect this formula to be close to 1 since the number of docs containing the word “football” will probably only be slightly less than the total number of docs (out of 100 maybe only 5 don’t have the word “football”, so we get 95/100).\nif the word “football” appears in all the articles it is not very useful for helping us identify between two articles, but if only a few documents contain words like “concussion” or “wellbeing”, (e.g. they appear in 2/100 articles) it will be much more useful in determining the topic of that article (they are probably specifically about player wellfare).\n💡 Thus the intuition of the term frequency - inverse document frequency approach is to give a high weight to any term which appears frequently in a single document, but not in too many documents of the corpus.\n\n## ?? Which words appear frequently in our small corpus \n# and might not be useful for deriving meaning?\n\ntexts = [\n    'I love to run',\n    'the cat does not eat the fruit',\n    'run to the cat',\n    'I love to eat the fruit. fruit fruit fruit fruit'\n]\n\n# the\n# to\n\n\n\n\nimage.png\n\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Instantiating the TfidfVectorizer\ntf_idf_vectorizer = TfidfVectorizer()\n\n# Training it on the texts\nweighted_words = pd.DataFrame(tf_idf_vectorizer.fit_transform(texts).toarray(),\n                 columns = tf_idf_vectorizer.get_feature_names_out(),\n                index = texts)\n\nweighted_words\n\n\n\n\n\n\n\n\ncat\ndoes\neat\nfruit\nlove\nnot\nrun\nthe\nto\n\n\n\n\nI love to run\n0.000000\n0.000000\n0.000000\n0.000000\n0.613667\n0.000000\n0.613667\n0.000000\n0.496816\n\n\nthe cat does not eat the fruit\n0.336350\n0.426618\n0.336350\n0.336350\n0.000000\n0.426618\n0.000000\n0.544609\n0.000000\n\n\nrun to the cat\n0.549578\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.549578\n0.444931\n0.444931\n\n\nI love to eat the fruit. fruit fruit fruit fruit\n0.000000\n0.000000\n0.187942\n0.939709\n0.187942\n0.000000\n0.000000\n0.152155\n0.152155\n\n\n\n\n\n\n\nWeaknesses of this approach?\n\nword order still missing\nrelationship between words still missing"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#n-grams",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#n-grams",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "# The two following sentences have the exact same representation in bag of words/ TFIDF approaches\n# However, they have very different meanings!\n\nsentences = [\n    \"I like cats but not dogs\",\n    \"I like dogs but not cats\"\n]\n\n\n# Vectorize the sentences\ncount_vectorizer = CountVectorizer()\nsentences_vectorized = count_vectorizer.fit_transform(sentences)\n\n# Show the representations in a nice DataFrame\nsentences_vectorized = pd.DataFrame(\n    sentences_vectorized.toarray(),\n    columns = count_vectorizer.get_feature_names_out(),\n    index = sentences\n)\n\n# Show the vectorized words\nsentences_vectorized\n\n\n\n\n\n\n\n\nbut\ncats\ndogs\nlike\nnot\n\n\n\n\nI like cats but not dogs\n1\n1\n1\n1\n1\n\n\nI like dogs but not cats\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n🧑🏻‍🏫 When using a bag-of-words representation, an efficient way to capture context is to consider:\n\nthe count of single tokens (unigrams)\nthe count of pairs (bigrams), triplets (trigrams), and more generally sequences of n words, also known as n-grams\n\n 4)\n😥 With a unigram vectorization, we couldn’t distinguish two sentences with the same words, despite their meaning being quite different\n\nsentences_vectorized\n\n\n\n\n\n\n\n\nbut\ncats\ndogs\nlike\nnot\n\n\n\n\nI like cats but not dogs\n1\n1\n1\n1\n1\n\n\nI like dogs but not cats\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n👩🏻‍🔬 What about a bigram vectorization?\n\n# Vectorize the sentences\ncount_vectorizer_n_gram = CountVectorizer(ngram_range = (1,2)) # BI-GRAMS\nsentences_vectorized_n_gram = count_vectorizer_n_gram.fit_transform(sentences)\n\n# Show the representations in a nice DataFrame\nsentences_vectorized_n_gram = pd.DataFrame(\n    sentences_vectorized_n_gram.toarray(),\n    columns = count_vectorizer_n_gram.get_feature_names_out(),\n    index = sentences\n)\n\n# Show the vectorized movies with bigrams (pairs of words)\nsentences_vectorized_n_gram\n\n\n\n\n\n\n\n\nbut\nbut not\ncats\ncats but\ndogs\ndogs but\nlike\nlike cats\nlike dogs\nnot\nnot cats\nnot dogs\n\n\n\n\nI like cats but not dogs\n1\n1\n1\n1\n1\n0\n1\n1\n0\n1\n0\n1\n\n\nI like dogs but not cats\n1\n1\n1\n0\n1\n1\n1\n0\n1\n1\n1\n0"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#word2vec-embeddings",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#word2vec-embeddings",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "Attempting to capture semantic meaning of words in numerical format\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\nimport gensim.downloader\n\n# Lots of different pretrained embeddings we can use for free!\nprint(list(gensim.downloader.info()['models'].keys()))\n\n['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n\n\n\n# We will use a vectoriser trained on Wikipedia today\nmodel_wiki = gensim.downloader.load('glove-wiki-gigaword-50')\n\n\n# Vectors based on 2B tweets, 27B tokens, 1.2M vocab!\n# 50 dimensions\n# N.B. Words not in glove-wiki-gigaword-50 will not have vectors computed. For example, if there was a niche word or acronym like \"NHS-R\" there would not be a vector for this word.\n\nmodel_wiki[\"cat\"]\n\narray([ 0.45281 , -0.50108 , -0.53714 , -0.015697,  0.22191 ,  0.54602 ,\n       -0.67301 , -0.6891  ,  0.63493 , -0.19726 ,  0.33685 ,  0.7735  ,\n        0.90094 ,  0.38488 ,  0.38367 ,  0.2657  , -0.08057 ,  0.61089 ,\n       -1.2894  , -0.22313 , -0.61578 ,  0.21697 ,  0.35614 ,  0.44499 ,\n        0.60885 , -1.1633  , -1.1579  ,  0.36118 ,  0.10466 , -0.78325 ,\n        1.4352  ,  0.18629 , -0.26112 ,  0.83275 , -0.23123 ,  0.32481 ,\n        0.14485 , -0.44552 ,  0.33497 , -0.95946 , -0.097479,  0.48138 ,\n       -0.43352 ,  0.69455 ,  0.91043 , -0.28173 ,  0.41637 , -1.2609  ,\n        0.71278 ,  0.23782 ], dtype=float32)\n\n\n\n# King is to Queen as Man is to ...\n\nexample_1 = model_wiki[\"queen\"] - model_wiki[\"king\"] + model_wiki[\"man\"]\nmodel_wiki.most_similar(example_1)[0]\n\n('woman', 0.8903914093971252)\n\n\n\n# Similar words to cat\n\nmodel_wiki.most_similar(model_wiki[\"cat\"])\n\n[('cat', 1.0),\n ('dog', 0.9218006134033203),\n ('rabbit', 0.8487820625305176),\n ('monkey', 0.804108202457428),\n ('rat', 0.7891963124275208),\n ('cats', 0.7865270972251892),\n ('snake', 0.7798910140991211),\n ('dogs', 0.7795815467834473),\n ('pet', 0.7792249917984009),\n ('mouse', 0.7731667160987854)]\n\n\n\n# Opposite of cold...?\n\nexample_2 = model_wiki[\"good\"] - model_wiki[\"evil\"] + model_wiki[\"cold\"]\nmodel_wiki.most_similar(example_2)[0]\n\n('warm', 0.7870427966117859)"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#attention-mechanism",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#attention-mechanism",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "The basis of transformer-based neural networks like ChatGPT! The paper that started it all: Attention is all you need\n\nEach token (word) embedding gets projected ➡️ into 3 further vectors: the query, key and value vectors (usually 768 dimensions each)!!\nWe compute a scaled dot-product 🔴 on the query and key vectors to work out how much each word relates to those around it\nTake these scores and normalize with softmax ⤵️\nMultiply by our value vectors ❎, sum and pass to our dense neural network\n\n⚠️ TLDR: The main point is that each word is now represented by 768 * 3 numbers! This is partly what makes LLMs so powerful (and resource-hungry) ⚠️\nIn the simple bag-of-words and TFIDF approaches, each word was represented by only 1 number each\nIn more complex word embeddings each word was represented by between 50 to 300 numbers each\n\n\n\nimage.png\n\n\n\n\n\nimage.png"
  },
  {
    "objectID": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#resources",
    "href": "blogs/posts/2025-01-03_text-vectorization/NLP - text vectorisation.html#resources",
    "title": "Introduction to text vectorization",
    "section": "",
    "text": "R text mining book https://www.tidytextmining.com/\nHuggingface tutorials (python) https://huggingface.co/learn/nlp-course/chapter1/1\nGreat video on attention https://www.youtube.com/watch?v=zxQyTK8quyY"
  },
  {
    "objectID": "blogs/posts/2025-06-20-how-we-sprint/index.html",
    "href": "blogs/posts/2025-06-20-how-we-sprint/index.html",
    "title": "What does a scrummaster do, anyway?",
    "section": "",
    "text": "We use sprints to manage our workload developing the open source NHP Model and several adjacent products. We’ve written before about sprint roles: Claire as Product Owner and Chris as Team Leader. This blogpost attempts to distil what it is that a scrummaster does, both as a form of internal documentation and as a way of sharing our learning with others."
  },
  {
    "objectID": "blogs/posts/2025-06-20-how-we-sprint/index.html#practical-tasks",
    "href": "blogs/posts/2025-06-20-how-we-sprint/index.html#practical-tasks",
    "title": "What does a scrummaster do, anyway?",
    "section": "Practical tasks",
    "text": "Practical tasks\n\nOrganising meetings\nOn a practical level, a scrummaster organises and conducts all the meetings that are a regular part of the sprint. These are:\n\nSprint planning (1 hour) with Product Owner before sprint starts\nSprint kickoff (2.5 hours) at start of sprint, with all team members\nSprint catchups (1 hour) weekly during sprint duration, with all team members\nSprint retro (2 hours) at end of sprint, with all team members\n\nWe have created a GitHub template which acts as a checklist to help us with sorting these, including suggested agendas and useful links. We currently work in 3-week sprints, with 2 weeks for coding and 1 week for Quality Assurance (QA). We then work on other projects between sprints (a “fallow week”), effectively working in 4 week cycles.\n\n\nManaging meetings and tasks\nThe scrummaster leads all meetings during the sprint, taking team members through the agenda. They help people working on the sprint to work out what they are going to do and how they are going to do it. If there are any blockers, the scrummaster helps scrum participants with unblocking them. The scrummaster is responsible for helping members of the sprint keep to deadlines by regularly checking in on progress.\nThe scrummaster works with the Product Owner to work out the priority order of the tasks in the sprint. If there are any unexpected development requests or changes in priority during the sprint, the scrummaster helps to work out what can and can’t move, and helps to distribute tasks equally amongst the team and manage workloads.\nWe manage our sprints using GitHub Projects, and the scrummaster is responsible for helping keep this tidy - for example, by ensuring that any uncompleted issues at the end of a sprint are either closed, or assigned to a new sprint.\n\n\n\nScrummaster in action"
  },
  {
    "objectID": "blogs/posts/2025-06-20-how-we-sprint/index.html#knowledge-and-skills-required",
    "href": "blogs/posts/2025-06-20-how-we-sprint/index.html#knowledge-and-skills-required",
    "title": "What does a scrummaster do, anyway?",
    "section": "Knowledge and skills required",
    "text": "Knowledge and skills required\n\nSeeing the big picture: Knowledge of the project as a whole\nThe scrummaster should have an understanding of the project as a whole, including an awareness of who the key stakeholders are, and what their priorities might be. The project that we’re working on has lots of interconnected parts, and there are often external time-sensitive pressures impacting our work as well. Having this broad overview enables the scrummaster to spot potential blockers and issues, and ensure that the product develops in a way that meets the needs of all stakeholders.\n\n\nAn eye for detail: Technical understanding\nThe scrummaster does not necessarily have to be actively involved in the sprint in terms of contributing code, but they should have enough technical knowledge to be able to understand (in broad terms) what the requirements are for each piece of work forming part of the sprint. They should also be able to signpost sprint participants to relevant resources and help them with decision making, where required.\n\n\nCommunication skills\nThe scrummaster coordinates communication between team members working on interconnected elements of tasks, and between the sprint team and key stakeholders. They ensure that development team members have all the information they need to accomplish their sprint goals.\nIn our team, the scrummaster is responsible for writing up the user-facing “model updates”, which provide a broad overview of the developments at the end of each sprint. This often involves translation of complex technical details into human readable terms."
  },
  {
    "objectID": "blogs/posts/2025-06-20-how-we-sprint/index.html#sharing-the-load",
    "href": "blogs/posts/2025-06-20-how-we-sprint/index.html#sharing-the-load",
    "title": "What does a scrummaster do, anyway?",
    "section": "Sharing the load",
    "text": "Sharing the load\nWhile there are benefits to maintaining the same person as scrummaster, we’ve switched recently to rotating the role among team members. This will provide respite for everyone and also improve big-picture knowledge across the team. In turn, this will reduce bus factor so that we won’t depend on any one individual to keep all the scrummaster knowledge in their brain."
  },
  {
    "objectID": "blogs/posts/2023-04-26_alternative_remotes/index.html",
    "href": "blogs/posts/2023-04-26_alternative_remotes/index.html",
    "title": "Alternative remote repositories",
    "section": "",
    "text": "It’s great when someone send’s you a pull request on GitHub to fix bugs or add new features to your project, but you probably always want to check the other persons work in someway before merging that pull request.\nAll of the steps below are intended to be entered via a terminal.\nLet’s imagine that we have a GitHub account called example and a repository called test, and we use https rather than ssh.\n$ git remote get-url origin\n# https://github.com/example/test.git\nNow, let’s say we have someone who has submitted a Pull Request (PR), and their username is friend. We can add a new remote for their fork with\n$ git remote add friend https://github.com/friend/test.git\nHere, I name the remote exactly as per the persons GitHub username for no other reason than making it easier to track things later on. You could name this remote whatever you like, but you will need to make sure that the remote url matches their repository correctly.\nWe are now able to checkout their remote branch. First, we will want to fetch their work:\n# make sure to replace the remote name to what you set it to before\n$ git fetch friend\nNow, hopefully they have commited to a branch with a name that you haven’t used. Let’s say they created a branch called my_work. You can then simply run\n$ git switch friend/my_work\nThis should checkout the my_work branch locally for you.\nNow, if they have happened to use a branch name that you are already using, or more likely, directly commited to their own main branch, you will need to do checkout to a new branch:\n# replace friend as above to be the name of the remote, and main to be the branch\n# that they have used\n# replace their_work with whatever you want to call this branch locally\n$ git checkout friend/main -b their_work\nYou are now ready to run their code and check everything is good to merge!\nFinally, If you want to clean up your local repository you can remove the new branch that you checked out and the new remote with the following steps:\n# switch back to one of your branches, e.g. main\n$ git checkout main\n\n# then remove the branch that you created above\n$ git branch -D their_work\n\n# you can remove the remote\n$ git remote remove friend"
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html",
    "title": "RStudio Tips and Tricks",
    "section": "",
    "text": "In a recent Coffee & Coding session we chatted about tips and tricks for RStudio, the popular and free Integrated Development Environment (IDE) that many Strategy Unit analysts use to write R code.\nRStudio has lots of neat features but many are tucked away in submenus. This session was a chance for the community to uncover and discuss some hidden gems to make our work easier and faster."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#coffee-coding",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#coffee-coding",
    "title": "RStudio Tips and Tricks",
    "section": "",
    "text": "In a recent Coffee & Coding session we chatted about tips and tricks for RStudio, the popular and free Integrated Development Environment (IDE) that many Strategy Unit analysts use to write R code.\nRStudio has lots of neat features but many are tucked away in submenus. This session was a chance for the community to uncover and discuss some hidden gems to make our work easier and faster."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#official-guidance",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#official-guidance",
    "title": "RStudio Tips and Tricks",
    "section": "Official guidance",
    "text": "Official guidance\nPosit is the company who build and maintain RStudio. They host a number of cheatsheets on their website, including one for RStudio. They also have a more in-depth user guide."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#command-palette",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#command-palette",
    "title": "RStudio Tips and Tricks",
    "section": "Command palette",
    "text": "Command palette\nRStudio has a powerful built-in Command Palette, which is a special search box that gives instant access to features and settings without needing to find them in the menus. Many of the tips and tricks we discussed can be found by searching in the Palette. Open it with the keyboard shortcut Ctrl + Shift + P.\n\n\n\nOpening the Command Palette.\n\n\nFor example, let’s say you forgot how to restart R. If you open the Command Palette and start typing ‘restart’, you’ll see the option ‘Restart R Session’. Clicking it will do exactly that. Handily, the Palette also displays the keyboard shortcut (Control + Shift + F10 on Windows) as a reminder.\nAs for settings, a search for ‘rainbow’ in the Command Palette will find ‘Use rainbow parentheses’, an option to help prevent bracket-mismatch errors by colouring pairs of parentheses. What’s nice is that the checkbox to toggle the feature appears right there in the palette so you can change it immediately.\nI refer to menu paths and keyboard shortcuts in the rest of this post, but bear in mind that you can use the Command Palette instead."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#options",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#options",
    "title": "RStudio Tips and Tricks",
    "section": "Options",
    "text": "Options\nIn general, most settings can be found under Tools &gt; Global Options… and many of these are discussed in the rest of this post.\n\n\n\nAdjusting workspace and history settings.\n\n\nBut there’s a few settings in particular that we recommend you change to help maximise reproducibility and reduce the chance of confusion. Under General &gt; Basic, uncheck ‘Restore .Rdata into workspace at startup’ and select ‘Never’ from the dropdown options next to ‘Save workspace to .Rdata on exit’. These options mean you start with the ‘blank slate’ of an empty environment when you open a project, allowing you to rebuild objects from scratch1."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#keyboard-shortcuts",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#keyboard-shortcuts",
    "title": "RStudio Tips and Tricks",
    "section": "Keyboard shortcuts",
    "text": "Keyboard shortcuts\nYou can speed up day-to-day coding with keyboard shortcuts instead of clicking buttons in the interface.\nYou can see some available shortcuts in RStudio if you navigate to Help &gt; Keyboard Shortcuts Help, or use the shortcut Alt + Shift + K (how meta). You can go to Help &gt; Modify Keyboard Shortcuts… to search all shortcuts and change them to what you prefer2.\nWe discussed a number of handy shortcuts that we use frequently3. You can:\n\nre-indent lines to the appropriate depth with Control + I\nreformat code with Control + Shift + A\nturn one or more lines into a comment with Control + Shift + C\ninsert the pipe operator (%&gt;% or |&gt;4) with Control + Shift + M5\ninsert the assignment arrow (&lt;-) with Alt + - (hyphen)\nhighlight a function in the script or console and press F1 to open the function documentation in the ‘Help’ pane\nuse ‘Find in Files’ to search for a particular variable, function or string across all the files in your project, with Control + Shift + F"
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#themes",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#themes",
    "title": "RStudio Tips and Tricks",
    "section": "Themes",
    "text": "Themes\nYou can change a number of settings to alter RStudio’s theme, colours and fonts to whatever you desire.\nYou can change the default theme in Tools &gt; Global Options… &gt; Appearance &gt; Editor theme and select one from the pre-installed list. You can upload new themes by clicking the ‘Add’ button and selecting a theme from your computer. They typically have the file extension .rsthemes and can be downloaded from the web, or you can create or tweak one yourself. The {rsthemes} package has a number of options and also allows you to switch between themes and automatically switch between light and dark themes depending on the time of day.\n\n\n\nCustomising the appearance and font.\n\n\nIn the same ‘Appearance’ submenu as the theme settings, you can find an option to change fonts. Monospace fonts, ones where each character takes up the same width, will appear here automatically if you’ve installed them on your computer. One popular font for coding is Fira Code, which has the special property of converting certain sets of characters into ‘ligatures’, which some people find easier to read. For example, the base pipe will appear as a rightward-pointing arrow rather than its constituent vertical-pipe and greater-than symbol (|&gt;)."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#panes",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#panes",
    "title": "RStudio Tips and Tricks",
    "section": "Panes",
    "text": "Panes\n\nLayout\nThe structural layout of RStudio’s panes can be adjusted. One simple thing you can do is minimise and maximise each pane by clicking the window icons in their upper-right corners. This is useful when you want more screen real-estate for a particular pane.\nYou can move pane loations too. Click the ‘Workspace Panes’ button (a square with four more inside it) at the top of the IDE to see a number of settings. For example, you can select ‘Console on the right’ to move the R console to the upper-right pane, which you may prefer for maximimsing the vertical space in which code is shown. You could also click Pane Layout… in this menu to be taken to Tools &gt; Global Options… &gt; Pane layout, where you can click ‘Add Column’ to insert new script panes that allow you to inspect and write multiple files side-by-side.\n\n\nScript navigation\nThe script pane in particular has a nice feature for navigating through sections of your script or Quarto/R Markdown files. Click the ‘Show Document Outline’ button or use the keyboard shortcut Control + Shift + O to slide open a tray that provides a nice indented list of all the sections and function defintions in your file.\nSection headers are auto-detected in a Quarto or R Markdown document wherever the Markdown header markup has been used: one hashmark (#) for a level 1 header, two for level 2, and so on. To add section headers to an R Script, add at least four hyphens after a commented line that starts with #. Use two or more hashes at the start of the comment to increase the nestedness of that section.\n\n# Header ------------------------------------------------------------------\n\n## Section ----\n\n### Subsection ----\n\nNote that Ctrl + Shift + R will open a dialog box for you to input the name of a section header, which will be inserted and automatically padded to 75 characters to provide a strong visual cue between sections.\nAs well as the document outline, there’s also a reminder in the lower-left of the script pane that gives the name of the section that your cursor is currently in. A symbol is also shown: a hashmark means it’s a headed section and an ‘f’ means it’s a function definition. You can click this to jump to other sections.\n\n\n\nNavigating with headers in the R script pane.\n\n\n\n\nBackground jobs\nPerhaps an under-used pane is ‘Background jobs’. This is where you can run a separate R process that keeps your R console free. Go to Tools &gt; Background Jobs &gt; Start Background Job… to expose this tab if it isn’t already listed alongside the R console.\nWhy might you want to do this? As I write this post, there’s a background process to detect changes to the Quarto document that I’m writing and then update a preview I have running in the browser. You can do something similar for Shiny apps. You can continue to develop your app and test things in the console and the app preview will update on save. You won’t need to keep hitting the ‘Render’ or ‘Run app’ button every time you make a change."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#magic-wand",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#magic-wand",
    "title": "RStudio Tips and Tricks",
    "section": "Magic wand",
    "text": "Magic wand\nThere’s a miscellany of useful tools available when you click the ‘magic wand’ button in the script pane.\n\n\n\nAbracadabra! Casting open the ‘magic wand’ menu.\n\n\nThis includes:\n\n‘Rename in Scope’, which is like find-and-replace but you only change instances with the same ‘scope’, so you could select the variable x, go to Rename in Scope and then you can edit all instances of the variable in the document and change them at the same time (e.g. to rename them)\n‘Reflow Comment’, which you can click after higlighting a comments block to have the comments automatically line-break at the maximum width\n‘Insert Roxygen Skeleton’, which you can click when your cursor is inside the body of a function you’ve written and a {roxygen2} documentation template will be added above your function with the @params argument names pre-filled\n\nAlong with ‘Comment/Uncomment Lines’, ‘Reindent Lines’ and ‘Reformat Lines’, mentioned above in the keyboard shortcuts section."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#wrapping-up",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#wrapping-up",
    "title": "RStudio Tips and Tricks",
    "section": "Wrapping up",
    "text": "Wrapping up\nTime was limited in our discussion. There are so many more tips and tricks that we didn’t get to. Let us know what we missed, or what your favourite shortcuts and settings are."
  },
  {
    "objectID": "blogs/posts/2024-03-21_rstudio-tips/index.html#footnotes",
    "href": "blogs/posts/2024-03-21_rstudio-tips/index.html#footnotes",
    "title": "RStudio Tips and Tricks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor the same reason it’s a good idea to restart R on a frequent basis. You may assume that an object x in your environment was made in a certain way and contains certain information, but does it? What if you overwrote it at some point and forgot? Best to wipe the slate clean and rebuild it from scratch. Jenny Bryan has written an explainer.↩︎\nYou can ‘snap focus’ to the script and console panes with the pre-existing shortcuts Control + 1 and Control + 2. My next most-used pane is the terminal, so I’ve re-mapped the shortcut to Control + 3.↩︎\nThe classic shortcuts of select-all (Control + A), cut (Control + X), copy Control + C, paste (Control + V), undo (Control + Z) and redo (Control + Shift + Z) are all available when editing.↩︎\nNote that you can set the default pipe to the base-R version (|&gt;) by checking the box at Tools &gt; Global Options… &gt; Code &gt; Use native pipe operator↩︎\nProbably ‘M’ for {magrittr}, the name of the package that contains the %&gt;% incarnation of the operator.↩︎"
  },
  {
    "objectID": "blogs/posts/2023-03-24_hotfix-with-git/index.html",
    "href": "blogs/posts/2023-03-24_hotfix-with-git/index.html",
    "title": "Creating a hotfix with git",
    "section": "",
    "text": "I recently discovered a bug in a code-base which needed to be fixed and deployed back to production A.S.A.P., but since the last release the code has moved on significantly. The history looks something a bit like:\nThat is, we have a tag which is the code that is currently in production (which we need to patch), a number of commits after that tag to main (which were separate branches merged via pull requests), and a current development branch.\nI need to somehow: 1. go back to the tagged release, 2. check that code out, 3. patch that code, 4. commit this change, but insert the commit before all of the new commits after the tag\nThere are at least two ways that I know to do this, one would be with an interactive rebase, but I used a slightly longer method, but one I feel is a little less likely to get wrong.\nBelow are the step’s that I took. One thing I should note is this worked well for my particular issue because the change didn’t cause any merge conflicts later on."
  },
  {
    "objectID": "blogs/posts/2023-03-24_hotfix-with-git/index.html#fixing-my-codebase",
    "href": "blogs/posts/2023-03-24_hotfix-with-git/index.html#fixing-my-codebase",
    "title": "Creating a hotfix with git",
    "section": "Fixing my codebase",
    "text": "Fixing my codebase\nFirst, we need to checkout the tag\ngit checkout -b hotfix v0.2.0\nThis creates a new branch called hotfix off of the tag v0.2.0.\nNow that I have the code base checked out at the point I need to fix, I can make the change that is needed, and commit the change\ngit add [FILENAME]\ngit commit -m \"fixes the code\"\n(Obviously, I used the actual file name and gave a better commit message. I Promise 😝)\nNow my code is fixed, I create a new tag for this “release”, as well as push the code to production (this step is omitted here)\ngit tag v0.2.1 -m \"version 0.2.0\"\nAt this point, our history looks something like\n\n\n\n\n\n\n\n\n\nWhat we want to do is break the link between main and v0.2.0, instead attaching tov0.2.1. First though, I want to make sure that if I make a mistake, I’m not making it on the main branch.\ngit checkout main\ngit checkout -b apply-hotfix\nThen we can fix our history using the rebase command\ngit rebase hotfix\nWhat this does is it rolls back to the point where the branch that we are rebasing (apply-hotfix) and the hotfix branch both share a common commit (v0.2.0 tag). It then applies the commits in the hotfix branch, before reapplying the commits from apply-hotfix (a.k.a. the main branch).\nOne thing to note, if you have any merge conflicts created by your fix, then the rebase will stop and ask you to fix the merge conflicts. There is some information in the GitHub doc’s for [resolving merge conflicts after a Git rebase][2].\n[2]: https://docs.github.com/en/get-started/using-git/resolving-merge-conflicts-after-a-git-rebase\nAt this point, we can check that the commit history looks correct\ngit log v0.2.0..HEAD\nIf we are happy, then we can apply this to the main branch. I do this by renaming the apply-hotfix branch as main. First, you have to delete the main branch to allow us to rename the branch.\ngit branch -D main\ngit branch -m main\nWe also need to update the other branches to use the new main branch\ngit checkout branch\ngit rebase main\nNow, we should have a history like"
  },
  {
    "objectID": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html",
    "href": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html",
    "title": "Using GitHub to plan and organise Coffee & Coding",
    "section": "",
    "text": "Coffee & Coding is a fortnightly hour-long session organised by the Data Science team, open to all members of the Strategy Unit with an interest in coding. It’s been well received and is a valued source of professional development and general geekery in the team.\nWe’ve been experimenting with using GitHub as an organisational tool for our team’s work, and are testing the same approach for Coffee & Coding sessions as well. Previously, future Coffee & Coding sessions were haphazardly listed in a Google Doc that was only accessible to members of the Data Science team, and we wanted a more open approach. We also didn’t have a good record of topics that were previously covered.\nYou’ll need a GitHub account to enjoy the full functionality of the planner. If you need help setting this up, get in touch with any member of the Data Science team.\nAny feedback on this new system for organising and planning Coffee & Coding is very welcome! Hope you enjoy using it."
  },
  {
    "objectID": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html#coffee-coding",
    "href": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html#coffee-coding",
    "title": "Using GitHub to plan and organise Coffee & Coding",
    "section": "",
    "text": "Coffee & Coding is a fortnightly hour-long session organised by the Data Science team, open to all members of the Strategy Unit with an interest in coding. It’s been well received and is a valued source of professional development and general geekery in the team.\nWe’ve been experimenting with using GitHub as an organisational tool for our team’s work, and are testing the same approach for Coffee & Coding sessions as well. Previously, future Coffee & Coding sessions were haphazardly listed in a Google Doc that was only accessible to members of the Data Science team, and we wanted a more open approach. We also didn’t have a good record of topics that were previously covered.\nYou’ll need a GitHub account to enjoy the full functionality of the planner. If you need help setting this up, get in touch with any member of the Data Science team.\nAny feedback on this new system for organising and planning Coffee & Coding is very welcome! Hope you enjoy using it."
  },
  {
    "objectID": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html#viewing-upcoming-sessions",
    "href": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html#viewing-upcoming-sessions",
    "title": "Using GitHub to plan and organise Coffee & Coding",
    "section": "Viewing upcoming sessions",
    "text": "Viewing upcoming sessions\nWe have created a fully open GitHub project for tracking Coffee & Coding sessions. Any sessions with scheduled dates can be seen in the “Upcoming sessions” view. Clicking on a session title brings up more information, including a brief overview of the session and the people running it. Users with GitHub accounts can make comments or post emoji reactions.\n\n\n\nViewing upcoming session details"
  },
  {
    "objectID": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html#adding-session-ideas",
    "href": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html#adding-session-ideas",
    "title": "Using GitHub to plan and organise Coffee & Coding",
    "section": "Adding session ideas",
    "text": "Adding session ideas\nTo add a session idea:\n\nCreate a new issue on the data_science repository. Provide a useful title and description for the session.\nGive your new issue the label C&C☕\nIf you would like to run or contribute to the session, assign yourself to it.\nClick “Create” to save your session idea as a GitHub issue. You should then be able to see it listed as a “Potential session” on the planner, and others will be able to view, vote for, and comment on your session idea.\n\n\n\n\nAdding a session idea"
  },
  {
    "objectID": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html#voting-for-session-ideas",
    "href": "blogs/posts/2024-11-12_coffee-coding-github-planner/index.html#voting-for-session-ideas",
    "title": "Using GitHub to plan and organise Coffee & Coding",
    "section": "Voting for session ideas",
    "text": "Voting for session ideas\nWe will use thumbs up (👍) emoji reactions to suggested sessions as a voting system to help us with planning and scheduling.\nIf you see any potential sessions that you are interested in, react to them with a thumbs up emoji. You can see all planned sessions, in order of votes received, listed here.\n\n\n\nVoting for a session idea"
  },
  {
    "objectID": "blogs/posts/2024-12-04-gha-branch-preview/index.html",
    "href": "blogs/posts/2024-12-04-gha-branch-preview/index.html",
    "title": "Deploy previews with GitHub pages",
    "section": "",
    "text": "#| include: FALSE\nrenv::use(lockfile = \"renv.lock\")\nWhen reviewing a pull request (PR) for a Quarto website, it’s good practice to check the rendered output, as well as the code. This is useful for ensuring that the changes look as expected, for example, ensuring that bullet points have rendered correctly, or that images are well sized.\nHowever, it’s a pain for the reviewer to clone the repository and render the Quarto site locally just to check it looks correct. Wouldn’t it be nice if when the PR was created, you automatically got a deployed version of your changes to look at?\nOther development platforms like Netlify and Vercel have offered deploy previews for a while, and although these are free for individual users in public repos they aren’t free for organisations.\nThere has been discussion of GitHub deploy preview for a few years, but there is currently no ETA for this feature. However, there is a popular GitHub marketplace action deploy-pr-preview by rossjrw which does just what we need.\nThis features of this action are:"
  },
  {
    "objectID": "blogs/posts/2024-12-04-gha-branch-preview/index.html#how-to-use-deploy-pr-preview",
    "href": "blogs/posts/2024-12-04-gha-branch-preview/index.html#how-to-use-deploy-pr-preview",
    "title": "Deploy previews with GitHub pages",
    "section": "How to use deploy-pr-preview",
    "text": "How to use deploy-pr-preview\nWe weren’t doing any CI/CD on PRs initially, so first I need to define a new workflow. Workflows are defined in .yaml files in the .github/workflows folder. At the top of the workflow, I need to give it a name and tell it when to trigger. In this case I want it to trigger on any PR.\nname: Quarto Preview\n\non:\n  pull_request:\n    types:\n      - opened\n      - reopened\n      - synchronize\n      - closed\nOnce I’ve defined when it should run, I need to specify what it should run. That tends to involve a number of steps such as\n\nChecking out the repository\nInstalling system dependencies\nInstalling packages (via {renv})\nRendering the Quarto site\n\nWe already have a publish.yml workflow for main which has a number of relevant steps which I’ve borrowed.\nThe file looks a bit like this:\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    defaults:\n      run:\n        working-directory: ./.github/workflows\n    permissions:\n      contents: write\n      pull-requests: write\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n\n      - name: Use cache\n        uses: actions/cache@v4\n        with:\n          key: freeze\n          path: _freeze\n\n      - name: Install System Dependencies\n        run:  bash install_system_deps.sh\n\n      - name: Set up R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          use-public-rspm: true\n      \n      - name: Set up renv\n        uses: r-lib/actions/setup-renv@v2\n\n      - name: Setup Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Render\n        uses: quarto-dev/quarto-actions/render@v2\nOnce the site has rendered, we just need to add a step to deploy the PR.\n      - name: Deploy PR Preview\n        uses: rossjrw/pr-preview-action@v1.4.8\n        with:\n          source-dir: ./_site/\nNow everytime a PR is created, or updated, the GitHub Action bot will spring into action."
  },
  {
    "objectID": "blogs/posts/2024-12-04-gha-branch-preview/index.html#adjusting-the-publish.yml",
    "href": "blogs/posts/2024-12-04-gha-branch-preview/index.html#adjusting-the-publish.yml",
    "title": "Deploy previews with GitHub pages",
    "section": "Adjusting the publish.yml",
    "text": "Adjusting the publish.yml\nThat’s all working and looking good! However, there is one last change we need to make. When someone merges into main, the publish action is triggered, and sometimes in that process the pr-previews can get wiped. This means that your preview link would no longer work if someone merges another branch to main whilst you’re working on your PR.\nWe were using the standard Quarto publish action which as far as I know, does not have an inbuilt configuration to exclude folders from the clean up process.\nInstead, I’ve replaced the Quarto action with another marketplace action, github-pages-deploy-action by JamesIves which has an inbuilt way to exclude certain folders from the clean-up. Since it’s not a specific Quarto publish action, we need to remember to add a Quarto render step first.\n      - name: Publish to GitHub Pages (and render)\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n      - name: Render Quarto\n        uses: quarto-dev/quarto-actions/render@v2\n\n      - name: Deploy 🚀\n        uses: JamesIves/github-pages-deploy-action@v4\n        with:\n          folder: _site/\n          clean-exclude: pr-preview/\n          force: false"
  },
  {
    "objectID": "blogs/posts/2024-12-04-gha-branch-preview/index.html#spring-cleaning",
    "href": "blogs/posts/2024-12-04-gha-branch-preview/index.html#spring-cleaning",
    "title": "Deploy previews with GitHub pages",
    "section": "Spring cleaning",
    "text": "Spring cleaning\nNow that we have a preview workflow (for branches) and a publish workflow (for main), we have a number of duplicated steps. As any good programmer knows, this is not ideal as it goes against the DRY prinicipal and makes code harder to maintain.\nI couldn’t find a quick way for two workflows to share steps1, so as a small improvement, I moved the lengthy system depencies call into it’s own script. I then ran the script using the following snippet. and called it via the following step.\n      - name: Install System Dependencies\n        run:  bash install_system_deps.sh\nwhich I think is a bit nicer to read than the original.\n - name: Install System Dependencies\n        run: |\n          sudo apt update\n          sudo apt install -y cmake\n          sudo apt install -y gdal-bin\n          sudo apt install -y git\n          sudo apt install -y libcurl4-openssl-dev\n          ...\nIn order for the actions to find the script, I also specified the path when I defined the job.\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    defaults:\n      run:\n        working-directory: ./.github/workflows\nNow we’ve got this working for GitHub pages 🎉 we’d also like to start using it for some of our deployments on Posit Connect. But that’s for another day."
  },
  {
    "objectID": "blogs/posts/2024-12-04-gha-branch-preview/index.html#footnotes",
    "href": "blogs/posts/2024-12-04-gha-branch-preview/index.html#footnotes",
    "title": "Deploy previews with GitHub pages",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you know how to do this, do let me know, or make a PR.↩︎"
  },
  {
    "objectID": "blogs/posts/2024-01-10_advent-of-code-and-test-driven-development/index.html",
    "href": "blogs/posts/2024-01-10_advent-of-code-and-test-driven-development/index.html",
    "title": "Advent of Code and Test Driven Development",
    "section": "",
    "text": "Advent of Code is an annual event, where daily coding puzzles are released from 1st – 24th December. We ran one of our fortnightly Coffee & Coding sessions introducing Advent of Code to people who code in the Strategy Unit, as well as the concept of test-driven development as a potential way of approaching the puzzles.\nTest-driven development (TDD) is an approach to coding which involves writing the test for a function BEFORE we write the function. This might seem quite counterintuitive, but it makes it easier to identify bugs 🐛 when they are introduced to our code, and ensures that our functions meet all necessary criteria. From my experience, this takes quite a long time to implement and can be quite tedious, but it is definitely worth it overall, especially as your project develops. Testing is also recommended in the NHS Reproducible Analytical Pipeline (RAP) guidelines.\nAn interesting thing to note about TDD is that we’re always expecting our first test to fail, and indeed failing tests are useful and important! If we wrote tests that just passed all the time, this would not be useful at all for our code.\nThe way that Advent of Code is structured, with test data for each puzzle and an expected test result, makes it very amenable to a test-driven approach. In order to support this, Matt and I created template repositories for a test-driven approach to Advent of Code, in Python and in R.\nOur goal when setting this up was to introduce others in the Strategy Unit to both TDD and Advent of Code. Advent of code can be challenging and I personally struggle to get past the first week, but it encourages creative (and maybe even fun?!) approaches to coding problems. I’m glad that we had the chance to explore some of the puzzles together in Coffee & Coding – it was interesting to see so many different approaches to the same problem, and hopefully it also gave us all the chance to practice writing tests."
  },
  {
    "objectID": "blogs/posts/2025-05-06_NLP-hackathon/index.html",
    "href": "blogs/posts/2025-05-06_NLP-hackathon/index.html",
    "title": "Taking a Hackathon approach to exploring new methods in NLP",
    "section": "",
    "text": "At the Strategy Unit, we’re lucky to have an awesome Evaluation team who often process and categorise large amounts of free text in the work that they do. This can be a time-consuming, labour-intensive process, and presents a good opportunity for AI/machine learning to help reduce some of this burden using Natural Language Processing (NLP). We want to use technology to help augment the existing skills and expertise of our qualitative analysts.\nThe Data Science team has intended for some time to explore ways of helping to automate some Evaluation tasks. However, we lacked the opportunity to do so effectively, given competing demands on our time and capacity. We finally decided to take a Hackathon-like approach which worked really well for us. We’re sharing our methods and findings here, in case they’re helpful to others. You can find our code on this GitHub repo. We approached the problem from a few different angles, each of which lives in a separate branch."
  },
  {
    "objectID": "blogs/posts/2025-05-06_NLP-hackathon/index.html#setup-defining-the-problem",
    "href": "blogs/posts/2025-05-06_NLP-hackathon/index.html#setup-defining-the-problem",
    "title": "Taking a Hackathon approach to exploring new methods in NLP",
    "section": "Setup: Defining the problem",
    "text": "Setup: Defining the problem\nThe key to success when you don’t have much time is to define your problem and objectives well. This helps keep your session focused and realistic. I had a good preparatory meeting with Andriana, our collaborator in the Evaluation team, who provided some examples of the free text data generated from Evaluation studies and their intended uses. We decided to use some data from a recent survey, which needed to be categorised into one or more of six different categories - a multilabel classification problem, with a small dataset of only 460 rows.\nOur mission was to develop a model to help the Evaluation team categorise responses quickly, with a reasonable degree of performance.\n\n\n\nMission impossible"
  },
  {
    "objectID": "blogs/posts/2025-05-06_NLP-hackathon/index.html#the-hackathon-format",
    "href": "blogs/posts/2025-05-06_NLP-hackathon/index.html#the-hackathon-format",
    "title": "Taking a Hackathon approach to exploring new methods in NLP",
    "section": "The Hackathon format",
    "text": "The Hackathon format\nWe set aside one working day for our Hackathon. Having the event in our calendars was useful - we were able to focus uninterrupted on one problem all day, without other meetings and obligations. We kept our schedule pretty loose, but ended up with three meetings; one in the morning, one at lunchtime, and one at the end of the day. We opted to work separately, although we discussed the option of pair programming all day as well.\nThe meeting in the morning was mostly spent talking through the problem, and discussing the approaches we were going to take. This was so that we could avoid duplicating our efforts, and explore different potential solutions.\nOur afternoon meeting was just a quick status update; I hadn’t done much coding by lunchtime because I’d spent hours trying to sort out my virtual environment! There’s quite a lot of setup required for some of these more state-of-the-art packages, and Windows isn’t the ideal operating system…\nWe reconvened in the evening to discuss what we’d achieved. It was great having quite a tight deadline - it made me really focus on the task at hand.\nAs an aside, I used ChatGPT to scaffold a lot of my code, and to help with debugging - it saved me so much time! I was able to use my time thinking through the problem and working out my approach, instead of trawling through Stackoverflow to figure out why my code wasn’t working. For me, the Hackathon would have been much less successful without it.\nBetween the three of us, we tried two different methods out."
  },
  {
    "objectID": "blogs/posts/2025-05-06_NLP-hackathon/index.html#zero-shot-classification-with-fine-tuning",
    "href": "blogs/posts/2025-05-06_NLP-hackathon/index.html#zero-shot-classification-with-fine-tuning",
    "title": "Taking a Hackathon approach to exploring new methods in NLP",
    "section": "Zero shot classification with fine-tuning",
    "text": "Zero shot classification with fine-tuning\nThis method is more like traditional machine learning. It is relatively quick and requires less computing power than the zero shot prompting method. We used the huggingface facebook/bart-large-mnli model model, which could be finetuned as part of a human-in-the-loop training cycle. However, this method is quite slow, taking about 20 minutes to retrain the model even with such a small dataset.\n\n\n\n\n\nflowchart TB\n    A(\"Model labels data\") --&gt; B(\"Humans check and correct data\")\n    B --&gt; n3(\"Model retrains\")\n    n3 --&gt; A\n    nn(\"Give model labels and data\") --&gt; A\n\n    style A stroke:#000000,fill:#276DC2,color:#fff\n    style nn stroke: #000000,fill:#ffde57\n    style B stroke: #000000,fill:#ffde57\n    style n3 stroke: #000000,fill:#276DC2,color:#fff"
  },
  {
    "objectID": "blogs/posts/2025-05-06_NLP-hackathon/index.html#zero-shot-prompting",
    "href": "blogs/posts/2025-05-06_NLP-hackathon/index.html#zero-shot-prompting",
    "title": "Taking a Hackathon approach to exploring new methods in NLP",
    "section": "Zero shot prompting",
    "text": "Zero shot prompting\nThis method utilises Ollama to run complex Large Language Models (LLMs) locally. This method relies on good prompt engineering, meaning that we had to test several different ways of asking the model to complete the task. No retraining was required. We just used our own machines to do this, and a computer with decent hardware can generate just over 80 tokens per second - meaning that it’s reasonably quick. Performance was slightly better, I would say, and it would even be possible to evaluate results automatically, with human oversight, using a method called “LLM eval”."
  },
  {
    "objectID": "blogs/posts/2025-05-06_NLP-hackathon/index.html#next-steps",
    "href": "blogs/posts/2025-05-06_NLP-hackathon/index.html#next-steps",
    "title": "Taking a Hackathon approach to exploring new methods in NLP",
    "section": "Next steps",
    "text": "Next steps\nBoth methods performed reasonably well, and Andriana could see how they would be able to help the Evaluation team in the future. The Evaluation team would need explore how exactly these models might fit into the qualitative analysis process and which types of projects they would be relevant for, as these methods are likely more applicable to projects with ‘simpler’ data. We also spotted some opportunities for quick wins with better cleaning of the data. Having built these simple proofs of concept on our local machines, we now need to think about how we would productionise these approaches in the real world. I also want to spend time thinking about how the Data Science team and the Evaluation team can work more closely together in the future.\nOverall, I would say that the Hackathon format was a great way to explore new methods in a collaborative way, without too much disruption of regular day to day responsibilities."
  },
  {
    "objectID": "style/project_structure.html",
    "href": "style/project_structure.html",
    "title": "Project Structure",
    "section": "",
    "text": "Analytical projects should be self-contained and portable. This means that all the materials required for an analysis should be organised into a single folder that can be shared in its entirety and be re-run by other people, ideally via GitHub.\nWe recommend RStudio Projects as a system for creating standardised project structures that meet these goals. The {usethis} package contains a number of helper functions to help get you started, including usethis::create_project().\n\n\nOne of the most common issues you’ll face when using a project someone else has created, or you created previously, is maintaining the required packages to run the project. Knowing what packages are needed to run a particular project isn’t always obvious, and over time packages can change, rendering code that once worked unusable.\nThe {renv} R package helps solve this problem by:\n\nKeeping track of the packages that are required for a particular project.\nLogging the installed version of all of the packages.\nMaintaining a per-project library of packages, so projects don’t interfere with one another.\n\n\n\n\nIt’s helpful to split discrete analytical tasks into separate script files, which can make it easier to handle the codebase in context and provide an obvious order of operations. For example, 01_read.R, 02_wrangle.R, 03_model.R, etc.\nYou could still forget to re-run one of the numbered files, however, or it may take a long time to re-run all the steps again if you only make one small change to the code. This is where a workflow manager is useful.\nWe recommend the {targets} R package as a workflow manager. You write a series of steps and {targets} automatically recognises all the relationships between functions and objects as a graph. This means {targets} knows the order that things should be run and knows which bits of code need to be re-run if there are upstream changes. It’s a well-documented and supported package.\n\n\n\nIt’s beneficial to convert code into discrete functions where possible. This makes it easier to:\n\nreduce the chance of errors, because you’ll avoid repetitive and mistake-prone copy-pasting of code\nunderstand your scripts, because code can be condensed into a simpler calls that are easier to read\nreuse your code, because functions allow you to consistently call the same code more than once and can be copied into other projects\ndebug, because the source of an error can be more easily traced and your code can be tested more easily\n\nConsider the DRY (Don’t Repeat Yourself) principle when deciding whether or not to convert some code into a function. It may be better to write a function if you’ve used the same piece of code more than once in an analysis, especially if it contains many lines.\nFunction names should be short but descriptive and should contain a verb that describes what the function does. For example, get_geospatial_data() may be better than the generic get_data(), which is certainly better than the uninformative data().\nIn a project, it’s conventional to put your functions in a folder called R in the project’s root directory. You can group functions into separate R scripts with meaningful names to make it easier to organise them (read-data.R, model.R, etc). You can then source() these function scripts into your analytical scripts as required.\n\n\n\n\nIt may be beneficial to gather your functions into a discrete package so that you and others can install and reuse them for other projects.\nThe {usethis} package has a number of shortcuts to help you set up a package. You can begin with usethis::create_package() to generate the basic structure and then usethis::use_r and usethis::use_test() to add scripts and {testthat} tests into the correct folder structure.\nWe recommend you include a number of extra files in your package to make its purpose clear and to encourage collaboration. This includes:\n\na README file to describe the purpose of your package and provide some simple examples, which you can set up with usethis::use_readme_md() or usethis::use_readme_rmd() if it contains R code that you want to execute\na NEWS file with usethis::use_news_md(), which is used to communicate the latest changes to your package\na CODE_OF_CONDUCT file with usethis::use_code_of_conduct to explain to collaborators how they should engage with your project\nvignettes with usethis::use_vignette(), which are short documents that let you mix code with prose to describe how to use the functions in your package\n\nWe recommend semantic versioning as you develop your package. In this system, the version number is composed of three digits (like ‘1.2.3’) that are each incremented as you make major breaking changes, minor changes and patches or bug fixes. The usethis::use_version() function can help you to do this and to automatically update the DESCRIPTION and NEWS file.\nUse {pkgdown} to autogenerate a website from your package’s documentation. This lets people see your documentation rendered nicely on the internet, without the need to install the package. You can serve this site on the web and update it automatically using GitHub Pages and GitHub Actions.",
    "crumbs": [
      "Project Structure"
    ]
  },
  {
    "objectID": "style/project_structure.html#rstudio-projects",
    "href": "style/project_structure.html#rstudio-projects",
    "title": "Project Structure",
    "section": "",
    "text": "Analytical projects should be self-contained and portable. This means that all the materials required for an analysis should be organised into a single folder that can be shared in its entirety and be re-run by other people, ideally via GitHub.\nWe recommend RStudio Projects as a system for creating standardised project structures that meet these goals. The {usethis} package contains a number of helper functions to help get you started, including usethis::create_project().\n\n\nOne of the most common issues you’ll face when using a project someone else has created, or you created previously, is maintaining the required packages to run the project. Knowing what packages are needed to run a particular project isn’t always obvious, and over time packages can change, rendering code that once worked unusable.\nThe {renv} R package helps solve this problem by:\n\nKeeping track of the packages that are required for a particular project.\nLogging the installed version of all of the packages.\nMaintaining a per-project library of packages, so projects don’t interfere with one another.\n\n\n\n\nIt’s helpful to split discrete analytical tasks into separate script files, which can make it easier to handle the codebase in context and provide an obvious order of operations. For example, 01_read.R, 02_wrangle.R, 03_model.R, etc.\nYou could still forget to re-run one of the numbered files, however, or it may take a long time to re-run all the steps again if you only make one small change to the code. This is where a workflow manager is useful.\nWe recommend the {targets} R package as a workflow manager. You write a series of steps and {targets} automatically recognises all the relationships between functions and objects as a graph. This means {targets} knows the order that things should be run and knows which bits of code need to be re-run if there are upstream changes. It’s a well-documented and supported package.\n\n\n\nIt’s beneficial to convert code into discrete functions where possible. This makes it easier to:\n\nreduce the chance of errors, because you’ll avoid repetitive and mistake-prone copy-pasting of code\nunderstand your scripts, because code can be condensed into a simpler calls that are easier to read\nreuse your code, because functions allow you to consistently call the same code more than once and can be copied into other projects\ndebug, because the source of an error can be more easily traced and your code can be tested more easily\n\nConsider the DRY (Don’t Repeat Yourself) principle when deciding whether or not to convert some code into a function. It may be better to write a function if you’ve used the same piece of code more than once in an analysis, especially if it contains many lines.\nFunction names should be short but descriptive and should contain a verb that describes what the function does. For example, get_geospatial_data() may be better than the generic get_data(), which is certainly better than the uninformative data().\nIn a project, it’s conventional to put your functions in a folder called R in the project’s root directory. You can group functions into separate R scripts with meaningful names to make it easier to organise them (read-data.R, model.R, etc). You can then source() these function scripts into your analytical scripts as required.",
    "crumbs": [
      "Project Structure"
    ]
  },
  {
    "objectID": "style/project_structure.html#packages",
    "href": "style/project_structure.html#packages",
    "title": "Project Structure",
    "section": "",
    "text": "It may be beneficial to gather your functions into a discrete package so that you and others can install and reuse them for other projects.\nThe {usethis} package has a number of shortcuts to help you set up a package. You can begin with usethis::create_package() to generate the basic structure and then usethis::use_r and usethis::use_test() to add scripts and {testthat} tests into the correct folder structure.\nWe recommend you include a number of extra files in your package to make its purpose clear and to encourage collaboration. This includes:\n\na README file to describe the purpose of your package and provide some simple examples, which you can set up with usethis::use_readme_md() or usethis::use_readme_rmd() if it contains R code that you want to execute\na NEWS file with usethis::use_news_md(), which is used to communicate the latest changes to your package\na CODE_OF_CONDUCT file with usethis::use_code_of_conduct to explain to collaborators how they should engage with your project\nvignettes with usethis::use_vignette(), which are short documents that let you mix code with prose to describe how to use the functions in your package\n\nWe recommend semantic versioning as you develop your package. In this system, the version number is composed of three digits (like ‘1.2.3’) that are each incremented as you make major breaking changes, minor changes and patches or bug fixes. The usethis::use_version() function can help you to do this and to automatically update the DESCRIPTION and NEWS file.\nUse {pkgdown} to autogenerate a website from your package’s documentation. This lets people see your documentation rendered nicely on the internet, without the need to install the package. You can serve this site on the web and update it automatically using GitHub Pages and GitHub Actions.",
    "crumbs": [
      "Project Structure"
    ]
  },
  {
    "objectID": "style/team.html",
    "href": "style/team.html",
    "title": "Team work",
    "section": "",
    "text": "In advance of the weekly stand-up, all team members share a short summary, in Slack, answering the following questions:\n\n⭐ What went well last week\n🦉 What lessons did you learn\n🎯 Goal(s) for this week\n\nThere is also a Slack bot every Wednesday & Friday prompting a check-in. You don’t have to respond to this, but it’s a nice way to share any recent wins or blockers.\n\n\n\nEvery Monday from 10:00 - 10:50 we have a team meeting.\nWe start with a stand-up from two team members. These are randomly selected by a bot on Slack in advance of the meeting.\nIf it’s your week to talk, you have 5 minutes to chat about anything that matters to you. You could expand on your Slack stand-up, perhaps sharing your wins or learnings in more detail. You might also choose to share a project you are working on in, showing code or a shiny application. Chat about your weekend or an update on how you’re doing is also totally fine!\nWe then discuss any items that team members have added to the rolling agenda.\n\n\n\nYour line manager should arrange (1 : 1) meetings with you once a fortnight.\n\n\n\nWe use MS Teams to communicate about project related work. We have a private slack channel on NHS-R slack which we use for general team chat and more casually. (Slack has much better emojis 🤸🎮🦥)\n\n\n\nOne of the ways we celebrate each other and the successes we have is by presenting each other with a “trophy” 🏆 in the slack channel. This is for good code, problem solving, helping someone - anything that deserves a special mention.\n\n\n\nCore hours vary considerably between team members and there is a lot of flexibility. Team members may send messages out of hours, but that is purely because it works for them, and will not expect anything immediate in response. Flexibility is not only allowed but recommended if it benefits you. Please agree core hours with your line manager and where possible communicate any changes in your core hours to team members via out of office on your calendar and/or Slack (e.g. “I’m out all morning because my car is broken - working later on”).\n\n\n\nThe team is primarily remote. The Strategy Unit has two all company gathering a year, in Birmingham. Currently the Data Science team don’t have any in-person meetings, although we might start doing them occasionally when it’s beneficial. Some of the London-based team coordinate when they work in the office.\n\n\n\nMost of the team’s time is working on a large project which we manage in an agile-like manner. We use GitHub to capture issues, and a project board to manage the sprint.\nTasks outside this project can be managed how you like. If you want to use GitHub, there is a Data Science project planner.\n\n\n\nWe organise a fortnightly coffee and code for analysts in the Strategy Unit. This is planned in a Coffee and Coding ☕🧑‍💻 GitHub project. Materials from previous sessions are shared here as blogs or presentations.",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#slack-check-ins",
    "href": "style/team.html#slack-check-ins",
    "title": "Team work",
    "section": "",
    "text": "In advance of the weekly stand-up, all team members share a short summary, in Slack, answering the following questions:\n\n⭐ What went well last week\n🦉 What lessons did you learn\n🎯 Goal(s) for this week\n\nThere is also a Slack bot every Wednesday & Friday prompting a check-in. You don’t have to respond to this, but it’s a nice way to share any recent wins or blockers.",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#weekly-stand-up",
    "href": "style/team.html#weekly-stand-up",
    "title": "Team work",
    "section": "",
    "text": "Every Monday from 10:00 - 10:50 we have a team meeting.\nWe start with a stand-up from two team members. These are randomly selected by a bot on Slack in advance of the meeting.\nIf it’s your week to talk, you have 5 minutes to chat about anything that matters to you. You could expand on your Slack stand-up, perhaps sharing your wins or learnings in more detail. You might also choose to share a project you are working on in, showing code or a shiny application. Chat about your weekend or an update on how you’re doing is also totally fine!\nWe then discuss any items that team members have added to the rolling agenda.",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#line-management",
    "href": "style/team.html#line-management",
    "title": "Team work",
    "section": "",
    "text": "Your line manager should arrange (1 : 1) meetings with you once a fortnight.",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#communication",
    "href": "style/team.html#communication",
    "title": "Team work",
    "section": "",
    "text": "We use MS Teams to communicate about project related work. We have a private slack channel on NHS-R slack which we use for general team chat and more casually. (Slack has much better emojis 🤸🎮🦥)",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#data-science-trophy",
    "href": "style/team.html#data-science-trophy",
    "title": "Team work",
    "section": "",
    "text": "One of the ways we celebrate each other and the successes we have is by presenting each other with a “trophy” 🏆 in the slack channel. This is for good code, problem solving, helping someone - anything that deserves a special mention.",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#working-times",
    "href": "style/team.html#working-times",
    "title": "Team work",
    "section": "",
    "text": "Core hours vary considerably between team members and there is a lot of flexibility. Team members may send messages out of hours, but that is purely because it works for them, and will not expect anything immediate in response. Flexibility is not only allowed but recommended if it benefits you. Please agree core hours with your line manager and where possible communicate any changes in your core hours to team members via out of office on your calendar and/or Slack (e.g. “I’m out all morning because my car is broken - working later on”).",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#working-location",
    "href": "style/team.html#working-location",
    "title": "Team work",
    "section": "",
    "text": "The team is primarily remote. The Strategy Unit has two all company gathering a year, in Birmingham. Currently the Data Science team don’t have any in-person meetings, although we might start doing them occasionally when it’s beneficial. Some of the London-based team coordinate when they work in the office.",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#task-management",
    "href": "style/team.html#task-management",
    "title": "Team work",
    "section": "",
    "text": "Most of the team’s time is working on a large project which we manage in an agile-like manner. We use GitHub to capture issues, and a project board to manage the sprint.\nTasks outside this project can be managed how you like. If you want to use GitHub, there is a Data Science project planner.",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/team.html#coffee-and-code",
    "href": "style/team.html#coffee-and-code",
    "title": "Team work",
    "section": "",
    "text": "We organise a fortnightly coffee and code for analysts in the Strategy Unit. This is planned in a Coffee and Coding ☕🧑‍💻 GitHub project. Materials from previous sessions are shared here as blogs or presentations.",
    "crumbs": [
      "Team work"
    ]
  },
  {
    "objectID": "style/style_guide.html",
    "href": "style/style_guide.html",
    "title": "Style Guide",
    "section": "",
    "text": "In general, follow the conventions of the tidyverse style guide.\nPrefer packages to be explicitly namespaced with a double colon in production code, like dplyr::mutate(), though this is not essential in exploratory data analysis.\nFavour the base R pipe (|&gt;) over the {magrittr} pipe (%&gt;%).\nAvoid library(tidyverse) in production code because it attaches a lot of packages that might not be used, though you may use it in exploratory data analysis.\nUse {styler} and {lintr} (or Python equivalents such as black) to tidy your code.\nInsert linebreaks in your code at or before column 80.\nPrefer the ‘source’ rather than the ‘visual’ editor for Quarto documents in RStudio (see the instructions for switching and remove the editor: visual key-value pair in the YAML header).\nWhen using {dplyr}, favour one mutate over many if .by grouping isn’t being used. For example, between the two examples below, example B is preferred:\n\nExample A:\ndplyr::starwars |&gt;\n  dplyr::mutate(height_cm = height) |&gt;\n  dplyr::mutate(name_copy = name)\nExample B:\ndplyr::starwars |&gt;\n  dplyr::mutate(\n    height_cm = height,\n    name_copy = name\n  )\n\n\n\nTo avoid excessive whitespace cluttering up Git diffs, set your chosen IDE to use LF line endings.\nFor RStudio, select Tools &gt; Global Options &gt; Code &gt; Saving. Under Serialization, set the Line ending conversion to Posix (LF).\n\n\n\nScreenshot showing how to set line endings correctly in RStudio\n\n\nFor VSCode, follow the instructions here.\nYou should also change your git settings to checkout in LF by running this in your terminal: git config --global core.autocrlf false\nPer repository, you can run git add --renormalize . to fix line endings.\n\n\n\n\nOur preferred date format is ISO: YYYY-MM-DD\nFavour Quarto (.qmd files) over R Markdown (.Rmd) for document production.\nUse Git for all projects and GitHub as the remote home for of all of the project code.\nUse the Reproducible Analytical Pipelines (RAP) approach wherever possible.\nLine breaks in Markdown (.md) files should be at 120 characters or at sentence breaks.\nWhen writing about code, use curly braces to identify a {package} name and use backticks around `functions()` as these render nicely and highlight the words clearly.\nIf you’re not sure about something try the NHS-R Way, the UK Government accessibility guidelines, or the Turing Way. If you’re still not sure, just ask the team.",
    "crumbs": [
      "Style Guide"
    ]
  },
  {
    "objectID": "style/style_guide.html#code-style",
    "href": "style/style_guide.html#code-style",
    "title": "Style Guide",
    "section": "",
    "text": "In general, follow the conventions of the tidyverse style guide.\nPrefer packages to be explicitly namespaced with a double colon in production code, like dplyr::mutate(), though this is not essential in exploratory data analysis.\nFavour the base R pipe (|&gt;) over the {magrittr} pipe (%&gt;%).\nAvoid library(tidyverse) in production code because it attaches a lot of packages that might not be used, though you may use it in exploratory data analysis.\nUse {styler} and {lintr} (or Python equivalents such as black) to tidy your code.\nInsert linebreaks in your code at or before column 80.\nPrefer the ‘source’ rather than the ‘visual’ editor for Quarto documents in RStudio (see the instructions for switching and remove the editor: visual key-value pair in the YAML header).\nWhen using {dplyr}, favour one mutate over many if .by grouping isn’t being used. For example, between the two examples below, example B is preferred:\n\nExample A:\ndplyr::starwars |&gt;\n  dplyr::mutate(height_cm = height) |&gt;\n  dplyr::mutate(name_copy = name)\nExample B:\ndplyr::starwars |&gt;\n  dplyr::mutate(\n    height_cm = height,\n    name_copy = name\n  )",
    "crumbs": [
      "Style Guide"
    ]
  },
  {
    "objectID": "style/style_guide.html#line-endings",
    "href": "style/style_guide.html#line-endings",
    "title": "Style Guide",
    "section": "",
    "text": "To avoid excessive whitespace cluttering up Git diffs, set your chosen IDE to use LF line endings.\nFor RStudio, select Tools &gt; Global Options &gt; Code &gt; Saving. Under Serialization, set the Line ending conversion to Posix (LF).\n\n\n\nScreenshot showing how to set line endings correctly in RStudio\n\n\nFor VSCode, follow the instructions here.\nYou should also change your git settings to checkout in LF by running this in your terminal: git config --global core.autocrlf false\nPer repository, you can run git add --renormalize . to fix line endings.",
    "crumbs": [
      "Style Guide"
    ]
  },
  {
    "objectID": "style/style_guide.html#additional-assorted-notes-on-style",
    "href": "style/style_guide.html#additional-assorted-notes-on-style",
    "title": "Style Guide",
    "section": "",
    "text": "Our preferred date format is ISO: YYYY-MM-DD\nFavour Quarto (.qmd files) over R Markdown (.Rmd) for document production.\nUse Git for all projects and GitHub as the remote home for of all of the project code.\nUse the Reproducible Analytical Pipelines (RAP) approach wherever possible.\nLine breaks in Markdown (.md) files should be at 120 characters or at sentence breaks.\nWhen writing about code, use curly braces to identify a {package} name and use backticks around `functions()` as these render nicely and highlight the words clearly.\nIf you’re not sure about something try the NHS-R Way, the UK Government accessibility guidelines, or the Turing Way. If you’re still not sure, just ask the team.",
    "crumbs": [
      "Style Guide"
    ]
  },
  {
    "objectID": "style/git_and_github.html",
    "href": "style/git_and_github.html",
    "title": "Using Git and GitHub",
    "section": "",
    "text": "Important\n\n\n\nNever push to the main branch! Code must always be reviewed first.\n\n\n\n\n\n\nIf you haven’t already, file an issue that describes what you are doing - whether it be fixing a bug, adding a feature, or something else. Issues help to keep track of both the work to be done, and the work that has been done.\nYou should ensure that the issue is as detailed as possible as it may need to be picked up by someone else. Even if only you intend to work on the issue, over time it can be easy to forget things if there isn’t enough information. They can also help you to remember what changes you have made to your code, and why you had to make these changes, which can be useful if you need to provide change logs when you release your code. You could even attach screenshots or other images that help explain the problem and anything you’ve tried in the past that hasn’t worked out. If you can, try to provide a reproducible example (‘reprex’), perhaps by using the {reprex} R package.\n\n\nIssues are a good record to explain what a PR is for. It’s also a useful area for input from other members of the team.\n\n\n\nTo contribute to someone else’s code, you will first need to clone a copy of their repository onto your computer. You can do this by first obtaining the URL of the repository of interest, by clicking the green ‘Code’ button on the top right of the repository home page.\n\nUsing the command line, Git Bash, the RStudio terminal or another shell script program, navigate to the location you’d like the files stored in, then run:\ngit clone https://github.com/&lt;user or org name&gt;/&lt;repository name&gt;.git\nYou can then use file explorer (if on Windows) to navigate to the R project folder within the files you’ve cloned, and open it to begin working with the code.\n\n\n\nOnce the issue is created and you have decided that you are going to work on it, first assign yourself to the issue in GitHub. This is an indication to others and helps to prevent multiple people from independently working on the same issue.\nOnce you have assigned yourself, you must make a new branch to work on that feature. GitHub offers a button to do this automatically on the issue page, on the right-hand side.\n\n\n\nScreenshot links to specific GitHub help page\n\n\n\n\nIf you used the create branch button on GitHub it will automatically close the issue when merged.\nLocally you can then work on the branch, pushing your code regularly to GitHub so it can be run and inspected when you are not around.\n\n\nAll commits should be atomic, in short,\n\nEach commit does one, and only one simple thing, that can be summed up in a simple sentence.\n\nAll of your commits should be in branches, the only changes that are made to the main branch would be via pull requests (PR) that have been reviewed by at minimum one colleague.\nWhen you think that your changes are ready to be merged, it’s time to create a PR and request a code review.\n\n\n\n\n\nWhen you create a PR, you must do two things:\n\nImmediately make someone an assignee - this is the person who will merge the PR. Typically, this should be the person creating the PR (you)\nSelect a person (or people) to review the PR (if the repository is open and has a CODEOWNERS file, then the users named in that file will be automatically assigned as reviewers)\n\nIf your code is not yet ready to be merged then you should use a draft PR.\n\n\nNote that draft PRs are only available on public GitHub repos.\nWait until the reviewer(s) has completed the review and marked it as ready to merge. At this point, the person who is assigned to the PR can complete the merge.\n\n\nMost merges will be the default Create a merge commit but sometimes you may wish to Squash and merge. As the person requesting the PR, you can select whichever option is wanted from the drop-down in GitHub as part of the PR.\n\n\nBy using this approach of the assignee completing the merge, it ensures that code quality is maintained and prevents code from being merged when it is not yet ready. For example, you may have started a PR thinking your work is complete, and a reviewer checks the code and agrees to merge, however, you may realise that there are still things to work on, or there are issues that need to be addressed.\nThe person who is assigned to the PR should be the only person making commits to the branch and this will prevent merge conflicts. If you wish for someone else to collaborate on the branch, then you should assign the PR to that person. At that point, they can pull your branch down and work on it, but you must stop using that branch locally.\n\n\n\n\n\n\nImportant\n\n\n\nOnly one person should ever work on changes to a branch at any time, and it is important to communicate with colleagues so they know to pull the latest changes in.\n\n\n\n\nIf the PR is later assigned back to you then you must immediately pull changes.\nThere may be times when you cannot be the assignee on a PR and in those situations you should nominate someone else to be the assignee and in charge of the PR, the same rules as in the paragraph above would then apply.\nIf, as a reviewer, you find that no one is assigned to the branch, you should get in contact with the person who created the branch and decide who is going to be the assigned owner of the PR.\n\n\nIn circumstances where the person who created the PR is an outside collaborator and doesn’t have permission to merge, then the reviewer should also be the assignee. In these circumstances, the collaborator will be working from their local fork and will be the only person who can push to the branch. The reviewer, once happy to approve the changes, can merge the PR.\n\n\n\n\n\nWe use semantic versioning.\n\n\n\nEach of the team’s repositories should have two assigned roles: owner and deputy.\nThe owner is likely to be the person who created the repository and/or has good knowledge of its content. The responsibility of the owner is to:\n\nhave overall responsibility for quality\ntriage issues, label them (especially ‘bug’ and ‘must’) and put into milestones\ntag and release the code and, if relevant, deploy it\nbring important issues to sprint by adding to backlog\ndefine how PRs are reviewed (e.g. issue/PR templates, CI Actions) and that they are reviewed\nensure adequate testing is in place\nadd and maintain the CODEOWNERS file\n\nThe deputy should perform these duties in the absence of the owner and provide updates to the owner on their return.\n\n\nThe Data Science team at The Strategy Unit maintain a template repository that contains the minimum set of files and basic structure needed to ensure organisational consistency in repository management. You are welcome to view, clone, copy or reuse the template as you see fit.\n\n\n\n\nSee also our GitHub as a Team Sport presentation.",
    "crumbs": [
      "Using Git and GitHub"
    ]
  },
  {
    "objectID": "style/git_and_github.html#workflow-with-git-and-github",
    "href": "style/git_and_github.html#workflow-with-git-and-github",
    "title": "Using Git and GitHub",
    "section": "",
    "text": "If you haven’t already, file an issue that describes what you are doing - whether it be fixing a bug, adding a feature, or something else. Issues help to keep track of both the work to be done, and the work that has been done.\nYou should ensure that the issue is as detailed as possible as it may need to be picked up by someone else. Even if only you intend to work on the issue, over time it can be easy to forget things if there isn’t enough information. They can also help you to remember what changes you have made to your code, and why you had to make these changes, which can be useful if you need to provide change logs when you release your code. You could even attach screenshots or other images that help explain the problem and anything you’ve tried in the past that hasn’t worked out. If you can, try to provide a reproducible example (‘reprex’), perhaps by using the {reprex} R package.\n\n\nIssues are a good record to explain what a PR is for. It’s also a useful area for input from other members of the team.\n\n\n\nTo contribute to someone else’s code, you will first need to clone a copy of their repository onto your computer. You can do this by first obtaining the URL of the repository of interest, by clicking the green ‘Code’ button on the top right of the repository home page.\n\nUsing the command line, Git Bash, the RStudio terminal or another shell script program, navigate to the location you’d like the files stored in, then run:\ngit clone https://github.com/&lt;user or org name&gt;/&lt;repository name&gt;.git\nYou can then use file explorer (if on Windows) to navigate to the R project folder within the files you’ve cloned, and open it to begin working with the code.\n\n\n\nOnce the issue is created and you have decided that you are going to work on it, first assign yourself to the issue in GitHub. This is an indication to others and helps to prevent multiple people from independently working on the same issue.\nOnce you have assigned yourself, you must make a new branch to work on that feature. GitHub offers a button to do this automatically on the issue page, on the right-hand side.\n\n\n\nScreenshot links to specific GitHub help page\n\n\n\n\nIf you used the create branch button on GitHub it will automatically close the issue when merged.\nLocally you can then work on the branch, pushing your code regularly to GitHub so it can be run and inspected when you are not around.\n\n\nAll commits should be atomic, in short,\n\nEach commit does one, and only one simple thing, that can be summed up in a simple sentence.\n\nAll of your commits should be in branches, the only changes that are made to the main branch would be via pull requests (PR) that have been reviewed by at minimum one colleague.\nWhen you think that your changes are ready to be merged, it’s time to create a PR and request a code review.",
    "crumbs": [
      "Using Git and GitHub"
    ]
  },
  {
    "objectID": "style/git_and_github.html#creating-and-reviewing-prs",
    "href": "style/git_and_github.html#creating-and-reviewing-prs",
    "title": "Using Git and GitHub",
    "section": "",
    "text": "When you create a PR, you must do two things:\n\nImmediately make someone an assignee - this is the person who will merge the PR. Typically, this should be the person creating the PR (you)\nSelect a person (or people) to review the PR (if the repository is open and has a CODEOWNERS file, then the users named in that file will be automatically assigned as reviewers)\n\nIf your code is not yet ready to be merged then you should use a draft PR.\n\n\nNote that draft PRs are only available on public GitHub repos.\nWait until the reviewer(s) has completed the review and marked it as ready to merge. At this point, the person who is assigned to the PR can complete the merge.\n\n\nMost merges will be the default Create a merge commit but sometimes you may wish to Squash and merge. As the person requesting the PR, you can select whichever option is wanted from the drop-down in GitHub as part of the PR.\n\n\nBy using this approach of the assignee completing the merge, it ensures that code quality is maintained and prevents code from being merged when it is not yet ready. For example, you may have started a PR thinking your work is complete, and a reviewer checks the code and agrees to merge, however, you may realise that there are still things to work on, or there are issues that need to be addressed.\nThe person who is assigned to the PR should be the only person making commits to the branch and this will prevent merge conflicts. If you wish for someone else to collaborate on the branch, then you should assign the PR to that person. At that point, they can pull your branch down and work on it, but you must stop using that branch locally.\n\n\n\n\n\n\nImportant\n\n\n\nOnly one person should ever work on changes to a branch at any time, and it is important to communicate with colleagues so they know to pull the latest changes in.\n\n\n\n\nIf the PR is later assigned back to you then you must immediately pull changes.\nThere may be times when you cannot be the assignee on a PR and in those situations you should nominate someone else to be the assignee and in charge of the PR, the same rules as in the paragraph above would then apply.\nIf, as a reviewer, you find that no one is assigned to the branch, you should get in contact with the person who created the branch and decide who is going to be the assigned owner of the PR.\n\n\nIn circumstances where the person who created the PR is an outside collaborator and doesn’t have permission to merge, then the reviewer should also be the assignee. In these circumstances, the collaborator will be working from their local fork and will be the only person who can push to the branch. The reviewer, once happy to approve the changes, can merge the PR.",
    "crumbs": [
      "Using Git and GitHub"
    ]
  },
  {
    "objectID": "style/git_and_github.html#preparing-a-package-release-of-your-code",
    "href": "style/git_and_github.html#preparing-a-package-release-of-your-code",
    "title": "Using Git and GitHub",
    "section": "",
    "text": "We use semantic versioning.",
    "crumbs": [
      "Using Git and GitHub"
    ]
  },
  {
    "objectID": "style/git_and_github.html#repository-organisation",
    "href": "style/git_and_github.html#repository-organisation",
    "title": "Using Git and GitHub",
    "section": "",
    "text": "Each of the team’s repositories should have two assigned roles: owner and deputy.\nThe owner is likely to be the person who created the repository and/or has good knowledge of its content. The responsibility of the owner is to:\n\nhave overall responsibility for quality\ntriage issues, label them (especially ‘bug’ and ‘must’) and put into milestones\ntag and release the code and, if relevant, deploy it\nbring important issues to sprint by adding to backlog\ndefine how PRs are reviewed (e.g. issue/PR templates, CI Actions) and that they are reviewed\nensure adequate testing is in place\nadd and maintain the CODEOWNERS file\n\nThe deputy should perform these duties in the absence of the owner and provide updates to the owner on their return.\n\n\nThe Data Science team at The Strategy Unit maintain a template repository that contains the minimum set of files and basic structure needed to ensure organisational consistency in repository management. You are welcome to view, clone, copy or reuse the template as you see fit.",
    "crumbs": [
      "Using Git and GitHub"
    ]
  },
  {
    "objectID": "style/git_and_github.html#further-reading",
    "href": "style/git_and_github.html#further-reading",
    "title": "Using Git and GitHub",
    "section": "",
    "text": "See also our GitHub as a Team Sport presentation.",
    "crumbs": [
      "Using Git and GitHub"
    ]
  },
  {
    "objectID": "style/data_storage.html",
    "href": "style/data_storage.html",
    "title": "Data Storage",
    "section": "",
    "text": "All projects should be commited to version control, with a repository created in the Strategy Unit’s GitHub organisation.\nIdeally, any data that is used within the project should be part of a pipeline, whether in {targets} or on Databricks.\nSee the Storing Data Safely blog post for details of how to use Azure and Sharepoint for data storage.\nThere are a number of considerations about whether to add the data to version control or not. At a high level:\n\nis the data OK to release publicly?\nis the data in a text-based (non-binary) format, such as .csv, .json (rather than say .xlsx)?\nis the data relatively small in size?\n\n\n\nIf data is grabbed from a website, or via an API, create code to download the file/data. Consider whether this is likely to be a stable way of getting the data (does the data change over time? do you suspect that the location of the resource may disappear? is it quick to retrieve the data?). If so, then it doesn’t make much sense to commit the data to version control as it can always be quickly regenerated.\n\n\n\nLarge files tend not to work particularly well with version control. Specifically, files larger than 100MB will be blocked by GitHub, and files larger than 50MB will generate a warning. But you may even want to class any file over a few MB as large.\nAlternatives for storing large files:\n\nif the file is something that is generated (and reproducible) from other sources, then do not bother tracking the file\nif the file is something that you want tracking with version control, look at git LFS\nif the file needs to be shared publicly, but LFS is not suitable, the file could be stored in Azure blob storage\nif the file needs to be shared privately, also consider Azure blob storage (using something like SAS tokens)\nif the file needs to only be shared within the Strategy Unit then store in SharePoint (i.e. within a teams channel)\n\nUse of network drives should be deprecated and avoided at all costs due to issues of lack of versioning of files and the performance bottleneck that is created by using a network share. If a network share is truly the only way of storing the data for sharing with colleagues, then look at using ways of syncing the file to local storage to avoid performance bottlenecks, such as robocopy.",
    "crumbs": [
      "Data Storage"
    ]
  },
  {
    "objectID": "style/data_storage.html#data-from-websites",
    "href": "style/data_storage.html#data-from-websites",
    "title": "Data Storage",
    "section": "",
    "text": "If data is grabbed from a website, or via an API, create code to download the file/data. Consider whether this is likely to be a stable way of getting the data (does the data change over time? do you suspect that the location of the resource may disappear? is it quick to retrieve the data?). If so, then it doesn’t make much sense to commit the data to version control as it can always be quickly regenerated.",
    "crumbs": [
      "Data Storage"
    ]
  },
  {
    "objectID": "style/data_storage.html#filesize",
    "href": "style/data_storage.html#filesize",
    "title": "Data Storage",
    "section": "",
    "text": "Large files tend not to work particularly well with version control. Specifically, files larger than 100MB will be blocked by GitHub, and files larger than 50MB will generate a warning. But you may even want to class any file over a few MB as large.\nAlternatives for storing large files:\n\nif the file is something that is generated (and reproducible) from other sources, then do not bother tracking the file\nif the file is something that you want tracking with version control, look at git LFS\nif the file needs to be shared publicly, but LFS is not suitable, the file could be stored in Azure blob storage\nif the file needs to be shared privately, also consider Azure blob storage (using something like SAS tokens)\nif the file needs to only be shared within the Strategy Unit then store in SharePoint (i.e. within a teams channel)\n\nUse of network drives should be deprecated and avoided at all costs due to issues of lack of versioning of files and the performance bottleneck that is created by using a network share. If a network share is truly the only way of storing the data for sharing with colleagues, then look at using ways of syncing the file to local storage to avoid performance bottlenecks, such as robocopy.",
    "crumbs": [
      "Data Storage"
    ]
  },
  {
    "objectID": "blogs/posts/2024-08-08_map-and-nest/index.html",
    "href": "blogs/posts/2024-08-08_map-and-nest/index.html",
    "title": "Map and Nest",
    "section": "",
    "text": "I want to share a framework that I like using occasionally for data analysis. It’s the nest-and-map and it’s helped me countless times when I’m working with related datasets. By combining {purrr} mapping with {tidyr} nesting, I can keep my analysis steps linked, allowing me to easily track from a summary or plot, back to the original data.\nThe main funtions we’ll need are"
  },
  {
    "objectID": "blogs/posts/2024-08-08_map-and-nest/index.html#example-on-nhs-workforce-statistics",
    "href": "blogs/posts/2024-08-08_map-and-nest/index.html#example-on-nhs-workforce-statistics",
    "title": "Map and Nest",
    "section": "Example on NHS workforce statistics",
    "text": "Example on NHS workforce statistics\nThe NHS workforce statistics are official statistics published monthly for England.\n\nstaff_group &lt;- readRDS(file = \"workforce_staff_group.rds\")\n\nI want to perform an analysis for each of the 42 integrated care systems (ICS). The {tidyr} nest() function creates a list-column, where each cell contains a mini dataframe for each grouping.\nLet’s group by ICS, and call the nested data column raw_data.\n\ngroup_by_ics &lt;- staff_group |&gt;\n    tidyr::nest(raw_data = -ics_name)\n\nThe new column is a list-column, with each cell containing an entire tibble of data relating to that individual ICS.\n\n#' echo: false\nhead(group_by_ics)\n\n# A tibble: 6 × 2\n  ics_name             raw_data         \n  &lt;chr&gt;                &lt;list&gt;           \n1 South East London    &lt;tibble [8 × 6]&gt; \n2 North East London    &lt;tibble [7 × 6]&gt; \n3 North Central London &lt;tibble [12 × 6]&gt;\n4 North West London    &lt;tibble [10 × 6]&gt;\n5 South West London    &lt;tibble [8 × 6]&gt; \n6 Devon                &lt;tibble [7 × 6]&gt; \n\n\nWe can grab these mini datasets in the usual way and explore them interactively.\n\ngroup_by_ics$raw_data[[1]]\n\n# A tibble: 8 × 6\n  organisation_name           total hchs_doctors nurses_health_visitors midwives\n  &lt;chr&gt;                       &lt;dbl&gt;        &lt;dbl&gt;                  &lt;dbl&gt;    &lt;dbl&gt;\n1 Total                       58394         7108                  14939      926\n2 Guy's and St Thomas' NHS F… 21361         3003                   6196      281\n3 King's College Hospital NH… 13158         2443                   4202      375\n4 Lewisham and Greenwich NHS…  6617          979                   2103      271\n5 London Ambulance Service N…  7050            4                     44        0\n6 NHS South East London ICB     617            9                     43        0\n7 Oxleas NHS Foundation Trust  4094          200                   1196        0\n8 South London and Maudsley …  5496          471                   1155        0\n# ℹ 1 more variable: ambulance_staff &lt;dbl&gt;\n\n\nNext, let’s apply some simple processing, say converting absolute numbers into percentages, to each of the ICSs in turn.\nWe use mutate() to create a new list-column staff_percent and map() to apply the processing function to each cell in turn. 1\n\n\nSee function definition for convert_percent()\n\n\n#' Convert percent\n#' @param raw_staff Tibble containing organisation_name, total and a number of staff categories\n#' @return Tibble like raw_staff but with staff categories represented as percentages rather than absolute numbers\nconvert_percent &lt;- function(staff){\n    staff |&gt;\n    dplyr::mutate(dplyr::across(.cols = -c(organisation_name, total),\n                  .fns =  \\(x)x/total)) |&gt;\n    dplyr::rename(\"Doctors\" = \"hchs_doctors\",\n                  \"Nurses\" = \"nurses_health_visitors\",\n                  \"Ambulance staff\" = \"ambulance_staff\",\n                  \"Midwives\" = \"midwives\")\n}\n\n\n\nprocessed_staff &lt;-\ngroup_by_ics |&gt;\n    dplyr::mutate(\n        staff_percent = purrr::map(raw_data, convert_percent)\n    )\n\nWhere I think this map-and-nest process really comes into its own is creating plots. Often, I find myself wanting to create a couple of different plots for each grouping, and then optionally save the plots with sensible names. Particularly in the analysis stage, I like having these plots in the same row as the raw data, so I can quickly compare and validate.\nI’ve created two functions, plot_barchart() and plot_waffle() which take the data and create charts.\n\n\nSee definition for plot_barchart() & plot_waffle()\n\n\n#' Plot barchart\n#' Makes a bar chart of staff perentages by organisation\n#' @param df tibble of staff data in percent format\nplot_barchart &lt;- function(df) {\n  df |&gt;\n    dplyr::filter(organisation_name != \"Total\") |&gt;\n    dplyr::select(-total) |&gt;\n    tidyr::pivot_longer(cols = -c(organisation_name), names_to = \"job\", values_to = \"percent\") |&gt;\n    ggplot2::ggplot(ggplot2::aes(x = percent, y = organisation_name, fill = job)) +\n    ggplot2::geom_col(position = \"dodge\") + \n    ggplot2::scale_x_continuous(labels = scales::percent_format(scale = 100)) +\n    ggplot2::labs(x = \"\", y = \"\") +\n    StrategyUnitTheme::scale_fill_su() + \n    ggplot2::theme_minimal() + \n    ggplot2::theme(legend.title = ggplot2::element_blank())\n}\n\n#' Plot waffle\n#' Makes a waffle chart to visualise staff breakdown at an ICS level\n#' @param raw_staff count data of staff\n#' @param title Title for the graphic\nplot_waffle &lt;- function(raw_staff, title) {\nwaffle_data &lt;-\nraw_staff |&gt;\n    dplyr::filter(organisation_name == \"Total\") |&gt;\n    dplyr::select(-total, -organisation_name) |&gt;\n    tidyr::pivot_longer(cols = dplyr::everything(), names_to = \"names\", values_to = \"vals\") |&gt;\n    dplyr::mutate(vals = round(vals / 100))\n\nggplot2::ggplot(waffle_data, ggplot2::aes(fill = names, values = vals)) +\n  waffle::geom_waffle(n_rows = 8, size = 0.33, colour = \"white\") +\n  ggplot2::coord_equal() +\n  ggplot2::theme_void() + \n  ggplot2::theme(legend.title = ggplot2::element_blank()) +\n  ggplot2::ggtitle(title)\n}\n\n\nAgain, using mutate() I can create a new column called barchart and I can map() the function plot_barchart(), applying it to each row at a time.\n\ngraphs &lt;-\nprocessed_staff |&gt;\n    dplyr::mutate(\n        barchart =  purrr::map(staff_percent, plot_barchart)\n    ) \n\nThe resulting column barchart is again a list-column, but this time instead of containing a tibble, it holds a ggplot object. A whole ggplot in a single cell. 2\nIf we want to pass two arguments to our function, we can replace map() with map2(). Here we’re using map2() to pass the ics_name column to use as a title in our waffle plot. 3\n\ngraphs &lt;-\nprocessed_staff |&gt;\n    dplyr::mutate(\n        waffle =  purrr::map2(raw_data, ics_name, \n            \\(data, title) plot_waffle(data, title)\n        )\n    ) \n\n\n\n\nAn example bar chart plot"
  },
  {
    "objectID": "blogs/posts/2024-08-08_map-and-nest/index.html#putting-it-all-together",
    "href": "blogs/posts/2024-08-08_map-and-nest/index.html#putting-it-all-together",
    "title": "Map and Nest",
    "section": "Putting it all together",
    "text": "Putting it all together\nAll of these mutate() steps can actually be called in one step. Here’s the full workflow again in full after a little refactor. I’ve also used pivot_longer() to move the two plotting columns into a single plot column. This will make it easier for me to generate nice filenames, and save the plots.\n\nresults &lt;-\nstaff_group |&gt;\n    tidyr::nest(raw_data = -ics_name) |&gt;\n    dplyr::mutate(\n        staff_percent = purrr::map(raw_data, convert_percent),\n        barchart =  purrr::map(staff_percent, plot_barchart),\n        waffle =  purrr::map2(raw_data, ics_name, \\(data, title) plot_waffle(data, title)) \n    )     |&gt;\n    tidyr::pivot_longer(cols = c(barchart, waffle), names_to = \"plot_type\", values_to = \"plot\") |&gt;\n    dplyr::mutate(filename = glue::glue(\"{snakecase::to_snake_case(ics_name)}_{plot_type}.png\"))\n\nThe walk() family of functions in {purrr} are used when the function you’re applying does not return an object, but is being used for it’s side-effect, for example reading or writing files.\nHere we call walk2(), passing in both the filename column and the plots column are arguments to save all the plots.\n\npurrr::walk2(\n  results$filename,\n  results$plot,\n  \\(filename, plot) ggplot2::ggsave(file.path(\"plots\", filename), plot, width = 10, height = 6)\n)\n\nBy keeping everything together in one nested structure, I personally find it much easier to keep track of my analyses. If you’re doing a more complex or permenant analysis, you might want to consider setting up a more formal data processing pipeline, and following RAP principals."
  },
  {
    "objectID": "blogs/posts/2024-08-08_map-and-nest/index.html#footnotes",
    "href": "blogs/posts/2024-08-08_map-and-nest/index.html#footnotes",
    "title": "Map and Nest",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn this example, we actually didn’t need to nest first. We could have performed the mutate() step on the full dataset.↩︎\nThis totally blew my mind the first time I saw it 🤯.↩︎\nWe’re mapping the relationship between the two inputs and the plot_waffle() with an anonymous function. This shorthand syntax for anonymous functions came in R v 4.1.0. For compatibility with older versions of R, you’ll need the ~ operator. For the different ways you can specify functions in {purrr} see the help file.↩︎"
  },
  {
    "objectID": "blogs/posts/2024-02-28_sankey_plot/index.html",
    "href": "blogs/posts/2024-02-28_sankey_plot/index.html",
    "title": "Visualising participant recruitment in R using Sankey plots",
    "section": "",
    "text": "Sankey diagrams are great tools to visualise flows through a system. They show connections between the steps of a process where the width of the arrows is proportional to the flow.\nI’m working on an evaluation of a risk screening process for people aged between 55-74 years and a history of smoking. In this Targeted Lung Health Check (TLHC) programme1 eligible people are invited to attend a free lung check where those assessed at high risk of lung cancer are then offered low-dose CT screening scans.\n1 Please visit the NHS England site for for more background.We used Sankey diagrams to visualise how people have engaged with the programme, from recruitment, attendance at appointments, their outcome from risk assessment, attendance at CT scans and will eventually be extended to cover the impact of the screening on early detection of those diagnosed with lung cancer.\nThis blog post is about the technical process of preparing record-level data for visualisation in a Sankey plot using R and customising it to enhance look and feel. Here is how the finished product will look:"
  },
  {
    "objectID": "blogs/posts/2024-02-28_sankey_plot/index.html#get-the-data",
    "href": "blogs/posts/2024-02-28_sankey_plot/index.html#get-the-data",
    "title": "Visualising participant recruitment in R using Sankey plots",
    "section": "Get the data",
    "text": "Get the data\nIn this example we will work with a simplified set of data focused on invitations.\nThe invites table holds details of when people were sent a letter or message inviting them to take part, how many times they were invited and how the person responded.\nThe people eligible for the programme are identified up-front and are represented by a unique ID with one row per person. Let’s assume each person receives at least one invitation to take part, they can have one of three outcomes:\n\nThey accept the invitation and agree to take part,\nThey decline the invitation,\nThey do not respond to the invitation.\n\nIf the person doesn’t respond to the first invitation they may be sent a second invitation and could be offered a third invitation if they didn’t respond to the second.\nHere is the specification for our simplified invites table:\n\nInvites specification\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nParticipant ID\nInteger\nA unique identifier for each person.\n\n\nInvite date 1\nDate\nThe date the person was first invited to participate.\nEvery person will have a date in this field.\n\n\nInvite date 2\nDate\nThe date a second invitation was sent.\n\n\nInvite date 3\nDate\nThe date a third invitation was sent.\n\n\nInvite outcome\nText\nThe outcome from the invite, one of either ‘Accepted’, ‘Declined’ or ‘No response’.\n\n\n\nEveryone receives at least one invite. Assuming a third of these respond (to either accept or decline) then two-thirds receive a follow-up invite. Of these, we assume half respond, meaning the remaining participants receive a third invite.\nHere we generate 100 rows of example data to populate our table.\n\n\nCode\n# set a randomisation seed for reproducibility\nset.seed(seed = 1234)\n\n# define some parameters\nstart_date &lt;- as.Date(\"2019-01-01\")\nend_date &lt;- as.Date(\"2021-01-01\")\nrows &lt;- 100\n\ndf_invites_1 &lt;- tibble(\n  # create a unique id for each participant\n  participant_id = 1:rows,\n\n  # create a random initial invite date between our start and end dates\n  invite_1_date = sample(\n    seq(start_date, end_date, by = \"day\"),\n    size = rows, replace = T\n  ),\n\n  # create a random outcome for this participant\n  invite_outcome = sample(\n    x = c(\"Accepted\", \"Declined\", \"No response\"),\n    size = rows, replace = T\n  )\n)\n\n# take a sample of participants and allocate them a second invite date\ndf_invites_2 &lt;- df_invites_1 |&gt;\n  # sample two thirds of participants to get a second invite\n  slice_sample(prop = 2 / 3) |&gt;\n  # allocate a date between 10 and 30 days following the first\n  mutate(\n    invite_2_date = invite_1_date + sample(10:30, size = n(), replace = T)\n  ) |&gt;\n  # keep just id and second date\n  select(participant_id, invite_2_date)\n\n\n# take a sample of those with a second invite and allocate them a third invite date\ndf_invites_3 &lt;- df_invites_2 |&gt;\n  # sample half of these to get a third invite\n  slice_sample(prop = 1 / 2) |&gt;\n  # allocate a date between 10 to 30 days following the second\n  mutate(\n    invite_3_date = invite_2_date + sample(10:30, size = n(), replace = T)\n  ) |&gt;\n  # keep just id and second date\n  select(participant_id, invite_3_date)\n\n# combine the 2nd and 3rd invites with the first table\ndf_invites &lt;- df_invites_1 |&gt;\n  left_join(\n    y = df_invites_2,\n    by = \"participant_id\"\n  ) |&gt;\n  left_join(\n    y = df_invites_3,\n    by = \"participant_id\"\n  ) |&gt;\n  # move the outcome field after the third invite\n  relocate(invite_outcome, .after = invite_3_date)\n\n# housekeeping\nrm(df_invites_1, df_invites_2, df_invites_3, start_date, end_date, rows)\n\n# view our data\ndf_invites |&gt;\n  reactable(defaultPageSize = 5)\n\n\n\n\nGenerated invite table"
  },
  {
    "objectID": "blogs/posts/2024-02-28_sankey_plot/index.html#determine-milestone-outcomes",
    "href": "blogs/posts/2024-02-28_sankey_plot/index.html#determine-milestone-outcomes",
    "title": "Visualising participant recruitment in R using Sankey plots",
    "section": "Determine milestone outcomes",
    "text": "Determine milestone outcomes\nThe next step is to take our source table and convert the data into a series of milestones (and associated outcomes) that represents how our invited participants moved through the pathway.\nIn our example we have five milestones to represent in our Sankey plot:\n\nOur eligible population (everyone in our invites table),\nThe result from the first invitation,\nThe result from the second invitation,\nThe result from the third invitation,\nThe overall invite outcome.\n\nAside from the eligible population, where everyone starts with the same value, participants will have one of several outcomes at each milestone. This step is about naming these milestones and the outcomes.\nIt is important that each milestone-outcome has unique values. An outcome of ‘No response’ can be recorded against the first, second and third invite, and we wish to see these outcomes separately represented on the Sankey (rather than just one ‘No response’), so each outcome must be made unique. In this example we prefix the outcome from each invite with the number of the invite, e.g. ‘Invite 1 No response’.\nThe reason for this will become clearer when we come to plot the Sankey, but for now we produce these milestone-outcomes from our invites table.\n\n\nCode\ndf_milestones &lt;- df_invites |&gt;\n  mutate(\n    # everyone starts in the eligible population\n    start_population = \"Eligible population\",\n\n    # work out what happened following the first invite\n    invite_1_outcome = case_when(\n      # if a second invite was sent we assume there was no outcome from the first\n      !is.na(invite_2_date) ~ \"Invitation 1 No response\",\n      # otherwise the overall outcome resulted from the first invite\n      .default = glue(\"Invitation 1 {invite_outcome}\")\n    ),\n\n    # work out what happened following the second invite\n    invite_2_outcome = case_when(\n      # if a third invite was sent we assume there was no outcome from the second\n      !is.na(invite_3_date) ~ \"Invitation 2 No response\",\n      # if a second invite was sent but no third then\n      !is.na(invite_2_date) ~ glue(\"Invitation 2 {invite_outcome}\"),\n      # default to NA if neither of the above are true\n      .default = NA\n    ),\n\n    # work out what happened following the third invite\n    invite_3_outcome = case_when(\n      # if a third invite was sent then the outcome is the overall outcome\n      !is.na(invite_3_date) ~ glue(\"Invitation 3 {invite_outcome}\"),\n      # otherwise mark as NA\n      .default = NA\n    )\n  ) |&gt;\n  # exclude the dates as they are no longer needed\n  select(-contains(\"_date\")) |&gt;\n  # move the overall invite outcome to the end\n  relocate(invite_outcome, .after = invite_3_outcome)\n\n# view our data\ndf_milestones |&gt;\n  reactable(defaultPageSize = 5)\n\n\n\n\nMilestone-outcomes for participants"
  },
  {
    "objectID": "blogs/posts/2024-02-28_sankey_plot/index.html#calculate-flows",
    "href": "blogs/posts/2024-02-28_sankey_plot/index.html#calculate-flows",
    "title": "Visualising participant recruitment in R using Sankey plots",
    "section": "Calculate flows",
    "text": "Calculate flows\nNext we take pairs of milestone-outcomes and calculate the number of participants that moved between them.\nHere we utilise the power of dplyr::summarise with an argument .by to group by our data before counting the number of unique participants who move between our start and end groups.\nFor invites 2 and 3 we perform two sets of summaries:\n\nThe first where the values in the to and from fields contain details.\nThe second to capture cases where the to destination is NULL. This is because the participant responded at the previous invite so there was no subsequent invite. In these cases we flow the participant to the overall invite outcome.2\n\n2 If you are thinking there is a lot of repetition here, you’re right. In practice I abstracted both steps to a function and passed in the parameters for the from and to variables and simplified my workflow a little, however, I’m showing it in plain form here for simplification.\n\nCode\ndf_flows &lt;- bind_rows(\n\n  # flow from population to invite 1\n  df_milestones |&gt;\n    filter(!is.na(start_population) & !is.na(invite_1_outcome)) |&gt;\n    rename(from = start_population, to = invite_1_outcome) |&gt;\n    summarise(\n      flow = n_distinct(participant_id, na.rm = T),\n      .by = c(from, to)\n    ),\n\n  # flow from invite 1 to invite 2 (where not NA)\n  df_milestones |&gt;\n    filter(!is.na(invite_1_outcome) & !is.na(invite_2_outcome)) |&gt;\n    rename(from = invite_1_outcome, to = invite_2_outcome) |&gt;\n    summarise(\n      flow = n_distinct(participant_id, na.rm = T),\n      .by = c(from, to)\n    ),\n\n  # flow from invite 1 to overall invite outcome (where invite 2 is NA)\n  df_milestones |&gt;\n    filter(!is.na(invite_1_outcome) & is.na(invite_2_outcome)) |&gt;\n    rename(from = invite_1_outcome, to = invite_outcome) |&gt;\n    summarise(\n      flow = n_distinct(participant_id, na.rm = T),\n      .by = c(from, to)\n    ),\n\n  # flow from invite 2 to invite 3 (where not NA)\n  df_milestones |&gt;\n    filter(!is.na(invite_2_outcome) & !is.na(invite_3_outcome)) |&gt;\n    rename(from = invite_2_outcome, to = invite_3_outcome) |&gt;\n    summarise(\n      flow = n_distinct(participant_id, na.rm = T),\n      .by = c(from, to)\n    ),\n\n  # flow from invite 2 to overall invite outcome (where invite 3 is NA)\n  df_milestones |&gt;\n    filter(!is.na(invite_2_outcome) & is.na(invite_3_outcome)) |&gt;\n    rename(from = invite_2_outcome, to = invite_outcome) |&gt;\n    summarise(\n      flow = n_distinct(participant_id, na.rm = T),\n      .by = c(from, to)\n    ),\n\n  # final flow - invite 3 to overall outcome (where both are not NA)\n  df_milestones |&gt;\n    filter(!is.na(invite_3_outcome) & !is.na(invite_outcome)) |&gt;\n    rename(from = invite_3_outcome, to = invite_outcome) |&gt;\n    summarise(\n      flow = n_distinct(participant_id, na.rm = T),\n      .by = c(from, to)\n    )\n)\n\n# view our data\ndf_flows |&gt;\n  reactable(defaultPageSize = 5)\n\n\n\n\nFlows of participants between milestones"
  },
  {
    "objectID": "blogs/posts/2024-02-28_sankey_plot/index.html#preparing-for-plotly",
    "href": "blogs/posts/2024-02-28_sankey_plot/index.html#preparing-for-plotly",
    "title": "Visualising participant recruitment in R using Sankey plots",
    "section": "Preparing for plotly",
    "text": "Preparing for plotly\nPlotly expects to be fed two sets of data:\n\nNodes - these are the milestones we have in our from and to fields,\nEdges - these are the flows that occur between nodes, the flow in our table.\n\nIt is possible to extract this data by hand but I found using the tidygraph package was much easier and more convenient.\n\ndf_sankey &lt;- df_flows |&gt;\n  # convert our flows data to a tidy graph object\n  as_tbl_graph()\n\nThe tidygraph package splits our data into nodes and edges. We can selectively work on each by ‘activating’ them - here is the nodes list:\n\ndf_sankey |&gt;\n  activate(what = \"nodes\") |&gt;\n  as_tibble() |&gt;\n  reactable(defaultPageSize = 5)\n\n\n\n\n\nYou can see each unique node name listed. The row numbers for these nodes are used as reference IDs in the edges object:\n\ndf_sankey |&gt;\n  activate(what = \"edges\") |&gt;\n  as_tibble() |&gt;\n  reactable(defaultPageSize = 5)\n\n\n\n\n\nWe now have enough information to generate our Sankey.\nFirst we extract our nodes and edges to separate data frames then convert the ID values to be zero-based (starts at 0) as this is what plotly is expecting. To do this is as simple as subtracting 1 from the value of the IDs.\nFinally we pass these two dataframes to plotly’s node and link function inputs to generate the plot.\n\n\nCode\n# extract the nodes to a dataframe\nnodes &lt;- df_sankey |&gt;\n  activate(nodes) |&gt;\n  data.frame() |&gt;\n  mutate(\n    id = row_number() - 1\n  )\n\n# extract the edges to a dataframe\nedges &lt;- df_sankey |&gt;\n  activate(edges) |&gt;\n  data.frame() |&gt;\n  mutate(\n    from = from - 1,\n    to = to - 1\n  )\n\n# plot our sankey\nplot_ly(\n  # setup\n  type = \"sankey\",\n  orientation = \"h\",\n  arrangement = \"snap\",\n\n  # use our node data\n  node = list(\n    label = nodes$name\n  ),\n\n  # use our link data\n  link = list(\n    source = edges$from,\n    target = edges$to,\n    value = edges$flow\n  )\n)\n\n\n\n\nOur first sankey\n\n\nNot bad!\nWe can see the structure of our Sankey now. Can you see the relative proportions of participants who did or didn’t respond to our first invite? Marvel at how those who responded to the first invite flow into our final outcome. How about those who didn’t respond to the first invitation go on to receive a second invite?\nPlotly’s charts are interactive. Try hovering your cursor over the nodes and edges to highlight them and a pop-up box will appear giving you additional details. You can reorder the vertical position of the nodes by dragging them above or below an adjacent node.\nThis looks functional."
  },
  {
    "objectID": "blogs/posts/2024-02-28_sankey_plot/index.html#styling-our-sankey",
    "href": "blogs/posts/2024-02-28_sankey_plot/index.html#styling-our-sankey",
    "title": "Visualising participant recruitment in R using Sankey plots",
    "section": "Styling our Sankey",
    "text": "Styling our Sankey\nNow we have the foundations of our Sankey I’d like to move on to its presentation. Specifically I’d like to:\n\nuse colour coding to clearly group those who accept or decline the invite,\nimprove the readability of the node titles,\nadd additional information to the pop-up boxes when you hover over nodes and edges, and\ncontrol the positioning of the nodes in the plot.\n\nAs our nodes and edges objects are dataframes it is straightforward to add this styling information directly to them.\nFor the nodes object we define colours based on the name of each node and manually position them in the plot\n\n\nCode\n# get the eligible population as a single value\n# NB, will be used to work out % amounts in each node and edge\ntemp_eligible_pop &lt;- df_flows |&gt;\n  filter(from == \"Eligible population\") |&gt;\n  summarise(total = sum(flow, na.rm = T)) |&gt;\n  pull(total)\n\n# style our nodes object\nnodes &lt;- nodes |&gt;\n  mutate(\n    # colour ----\n    # add colour definitions, green for accepted, red for declined\n    colour = case_when(\n      str_detect(name, \"Accepted\") ~ \"#44bd32\",\n      str_detect(name, \"Declined\") ~ \"#c23616\",\n      str_detect(name, \"No response\") ~ \"#7f8fa6\",\n      str_detect(name, \"Eligible population\") ~ \"#7f8fa6\"\n    ),\n\n    # add a semi-transparent colour for the edges based on node colours\n    colour_fade = col2hcl(colour = colour, alpha = 0.3),\n\n    # positioning ----\n    # NB, I found that to position nodes you need to supply both\n    # horizontal and vertical positions\n    # NNB, it was a bit of trial and error to get the these positions just\n    # right\n\n    # horizontal positions (0 = left, 1 = right)\n    x = case_when(\n      str_detect(name, \"Eligible population\") ~ 1,\n      str_detect(name, \"Invitation 1\") ~ 2,\n      str_detect(name, \"Invitation 2\") ~ 3,\n      str_detect(name, \"Invitation 3\") ~ 4,\n      .default = 5\n    ) |&gt; rescale(to = c(0.001, 0.9)),\n\n    # vertical position (1 = bottom, 0 = top)\n    y = case_when(\n      str_detect(name, \"Eligible population\") ~ 5,\n      # invite 1\n      str_detect(name, \"Invitation 1 Accepted\") ~ 1,\n      str_detect(name, \"Invitation 1 No response\") ~ 5,\n      str_detect(name, \"Invitation 1 Declined\") ~ 8.5,\n      # invite 2\n      str_detect(name, \"Invitation 2 Accepted\") ~ 2,\n      str_detect(name, \"Invitation 2 No response\") ~ 5,\n      str_detect(name, \"Invitation 2 Declined\") ~ 7.8,\n      # invite 3\n      str_detect(name, \"Invitation 3 Accepted\") ~ 2.7,\n      str_detect(name, \"Invitation 3 No response\") ~ 5.8,\n      str_detect(name, \"Invitation 3 Declined\") ~ 7.2,\n      # final outcomes\n      str_detect(name, \"Accepted\") ~ 1,\n      str_detect(name, \"No response\") ~ 5,\n      str_detect(name, \"Declined\") ~ 8,\n      .default = 5\n    ) |&gt; rescale(to = c(0.001, 0.999))\n  ) |&gt;\n  # add in a custom field to show the percentage flow\n  left_join(\n    y = df_flows |&gt;\n      group_by(to) |&gt;\n      summarise(\n        flow = sum(flow, na.rm = T),\n        flow_perc = percent(flow / temp_eligible_pop, accuracy = 0.1),\n      ) |&gt;\n      select(name = to, flow_perc),\n    by = \"name\"\n  )\n\n# view our nodes data\nnodes |&gt;\n  reactable(defaultPageSize = 5)\n\n\n\n\nStyling the nodes dataframe\n\n\nNext we move to styling the edges, which is a much simpler prospect:\n\n\nCode\nedges &lt;- edges |&gt;\n  mutate(\n    # add a label for each flow to tell us how many people are in each\n    label = number(flow, big.mark = \",\"),\n    # add a percentage flow figure\n    flow_perc = percent(flow / temp_eligible_pop, accuracy = 0.1)\n  ) |&gt;\n  # add the faded colour from our nodes object to match the destinations\n  left_join(\n    y = nodes |&gt; select(to = id, colour_fade),\n    by = \"to\"\n  )\n\n# view our edges data\nedges |&gt;\n  reactable(defaultPageSize = 5)\n\n\n\n\nStyling the edges dataframe\n\n\nWe now have stylised node and edge tables ready and can bring it all together. Note the use of customdata and hovertemplate help to bring in additional information and styling to the pop-up boxes that appear when you hover over each flow and node.\n\n\nCode\n# plot our stylised sankey\nplot_ly(\n  # setup\n  type = \"sankey\",\n  orientation = \"h\",\n  arrangement = \"snap\",\n\n  # use our node data\n  node = list(\n    label = nodes$name,\n    color = nodes$colour,\n    x = nodes$x,\n    y = nodes$y,\n    customdata = nodes$flow_perc,\n    hovertemplate = \"%{label}&lt;br /&gt;&lt;b&gt;%{value}&lt;/b&gt; participants&lt;br /&gt;&lt;b&gt;%{customdata}&lt;/b&gt; of eligible population\"\n  ),\n\n  # use our edge data\n  link = list(\n    source = edges$from,\n    target = edges$to,\n    value = edges$flow,\n    label = edges$label,\n    color = edges$colour_fade,\n    customdata = edges$flow_perc,\n    hovertemplate = \"%{source.label} → %{target.label}&lt;br /&gt;&lt;b&gt;%{value}&lt;/b&gt; participants&lt;br /&gt;&lt;b&gt;%{customdata}&lt;/b&gt; of eligible population\"\n  )\n) |&gt;\n  layout(\n    font = list(\n      family = \"Arial, Helvetica, sans-serif\",\n      size = 12\n    ),\n    # make the background transparent (also removes the text shadow)\n    paper_bgcolor = \"rgba(0,0,0,0)\"\n  ) |&gt;\n  config(responsive = T)\n\n\n\n\nA stylish Sankey"
  },
  {
    "objectID": "blogs/posts/2024-05-13_one-year-coffee-code/index.html",
    "href": "blogs/posts/2024-05-13_one-year-coffee-code/index.html",
    "title": "One year of coffee & coding",
    "section": "",
    "text": "The data science team have been running coffee & coding sessions for just over a year now. When I joined that Strategy Unit, I was really pleased to see these sessions running as I think making time to discuss and share technical knowledge is highly valuable, especially as an organisation grows.\nCoffee and coding sessions run every two weeks and usually take the form of a short presentation, followed by a discussion. Although we have had a variety of different sessions including live coding demos and show and tell for projects.\nWe figured it would be a good idea to do a quick survey of attendees to make sure that the sessions were beneficial and see if there were any suggestions for future sessions. We had 11 responses, all of which were really positive, with 90% agreeing that the sessions are interesting, and over 80% saying that they learn new things. Respondents said that the sessions were well varied across the technical spectrum and that they “almost always learn something useful”.\nThe two main themes of the results were that sessions were inclusive and sparked collaboration. ✨\n\nI like that everyone can contribute\n\n\nIt’s great seeing what else people are doing\n\n\nI get more ideas for future projects\n\nSome of the main suggestions included more content for newer programmers and encouraging the wider analytical team to share real project examples.\nSo with that, why not consider presenting? The sessions are informal and everyone is welcome to contribute. If you’ve got something to share, please let a member of the data science team know.\nAs a reminder, materials for our previous sessions are available under Presentations."
  },
  {
    "objectID": "blogs/posts/2025-04-30_using-github-apps-for-authentication/index.html",
    "href": "blogs/posts/2025-04-30_using-github-apps-for-authentication/index.html",
    "title": "Using GitHub Apps for Authentication",
    "section": "",
    "text": "Recently, we’ve been working on building an internal dashboard to monitor the repositories in our GitHub organisation. The intention is to perform various checks, such as ensuring each repo has a CODEOWNERS file.\nGitHub has a REST API that can do all of the things we need, but we hit a bit of a snag early on. We want this dashboard to update itself on our Posit Connect server—but authenticating with the GitHub API requires a Personal Access Token (PAT).\nPATs are useful, but they’re managed by a user (not an organisation), and ideally should be short-lived and expire regularly.\nWhat we really need is a more robust way of authenticating with GitHub."
  },
  {
    "objectID": "blogs/posts/2025-04-30_using-github-apps-for-authentication/index.html#github-apps",
    "href": "blogs/posts/2025-04-30_using-github-apps-for-authentication/index.html#github-apps",
    "title": "Using GitHub Apps for Authentication",
    "section": "GitHub Apps",
    "text": "GitHub Apps\n\nA GitHub App is a type of integration that you can build to interact with and extend the functionality of GitHub. You can build a GitHub App to provide flexibility and reduce friction in your processes, without needing to sign in a user or create a service account.\nCommon use cases for GitHub Apps include:\n\nAutomating tasks or background processes\n\n…\n\nAbout GitHub Apps.\nSounds ideal, right? And it turns out it’s pretty easy to create your own app too! Well, there are a few steps, and a bit of boilerplate code to write, but I’ll get to that later.\nIf you explore that link, you’ll find all the details needed to create your own app—but I’ll quickly note the steps I took below.\n\nCreating the App\n\nGo to your organisation’s Settings page on GitHub.\nAt the bottom of the left-hand navigation, find Developer settings and choose GitHub Apps.\nClick the New GitHub App button.\nGive it a name (I named ours “Strategy Unit GitHub Dashboard”).\nFor the Homepage URL, set it to where the app will be deployed.\nSkip down to Webhook and uncheck the Active checkbox.\nGrant the app only the minimum permissions required. In my case, I gave repository metadata read access—additional permissions can be granted later if needed.\nLeave Where can this GitHub App be Installed? set to Only on this account.\nClick Create GitHub App.\nOn the newly created app page, a small menu should appear on the left with Install App near the bottom. Use that to install the app into your organisation.\nBack on the app’s settings page, note the App ID near the top.\nAt the bottom of the settings page, click Generate a private key—this will download a private key for later use.\n\n\n\nUsing the App\nWe can now use the app to authenticate with the GitHub API. But to perform requests—like listing repositories—we still need a token.\nWait, I thought we were trying to avoid using PATs?\nWell… yes. But we’ll use the GitHub App to generate a PAT for us!\nLet me outline the workflow and show how to generate the token using R and the {httr2} package.\nIf you haven’t used {httr2} before, the final code example includes extra comments explaining what’s going on.\n\n1. Generate a JWT\nFirst, we need to create a JSON Web Token (JWT) issued by our app (using the App ID and private key from earlier):\n\n\nshow code for get_github_jwt()\n#' Get GitHub JWT for an Application\n#'\n#' @param key Path to the private key file or a string containing the private\n#'     key. Defaults to the environment variable `GITHUB_APP_PRIVATE_KEY`.\n#' @param app_id GitHub App ID. Defaults to the environment variable\n#'     `GITHUB_APP_ID`.\n#' @param expiry_time Expiry time for the JWT in seconds. Defaults to 30s.\n#'\n#' @return A JSON Web Token (JWT) for the GitHub App.\nget_github_jwt &lt;- function(\n    key = Sys.getenv(\"GITHUB_APP_PRIVATE_KEY\"),\n    app_id = Sys.getenv(\"GITHUB_APP_ID\"),\n    expiry_time = 30) {\n  private_key &lt;- openssl::read_key(key)\n\n  now &lt;- as.numeric(Sys.time())\n  claim &lt;- httr2::jwt_claim(\n    iat = now,\n    exp = now + expiry_time,\n    iss = app_id\n  )\n\n  httr2::jwt_encode_sig(claim, key = private_key)\n}\n\n\n\n\n2. Get the Installation ID for the App\nNext, we need the App’s installation ID.\nYou could find it manually via your organisation’s settings page under Installed Apps, but that’s cumbersome. Instead, we’ll use the API and our JWT to fetch it. Since we created the app and restricted installation to our org only, we can assume there’s just one installation.\n\n\nshow code for get_github_app_installation_id()\n#' Get GitHub PAT from Installation Access Token\n#'\n#' @param jwt JSON Web Token (JWT) for the GitHub App. Defaults to the output of\n#'     `get_github_jwt()`.\n#' @param installation_id GitHub Installation ID. Defaults to the environment\n#'     variable\n#' @param github_api_ep The base URL for the GitHub API. Defaults to\n#'     \"https://api.github.com/\".\n#'\n#' @return A personal access token (PAT) with permissions granted to the app.\nget_github_app_installation_id &lt;- function(\n    jwt = get_github_jwt(),\n    github_api_ep = \"https://api.github.com/\") {\n  resp &lt;- httr2::request(github_api_ep) |&gt;\n    httr2::req_url_path_append(\n      \"app\",\n      \"installations\"\n    ) |&gt;\n    httr2::req_method(\"GET\") |&gt;\n    httr2::req_auth_bearer_token(get_github_jwt()) |&gt;\n    httr2::req_headers(\n      Accept = \"application/vnd.github+json\"\n    ) |&gt;\n    httr2::req_perform()\n\n  httr2::resp_check_status(resp)\n\n  httr2::resp_body_json(resp)[[1]][[\"id\"]]\n}\n\n\n\n\n3. Generate a PAT\nWe’re now ready to generate the token we’ll use for API requests.\n\n\nshow code for get_github_iat_pat()\n#' Get GitHub PAT from Installation Access Token\n#'\n#' @param jwt JSON Web Token (JWT) for the GitHub App. Defaults to the output of\n#'     `get_github_jwt()`.\n#' @param installation_id GitHub Installation ID. Defaults to the output of\n#'     get_github_app_installation_id()`.\n#' @param github_api_ep The base URL for the GitHub API. Defaults to\n#'     \"https://api.github.com/\".\n#'\n#' @return A personal access token (PAT) with permissions granted to the app.\nget_github_iat_pat &lt;- function(\n    jwt = get_github_jwt(),\n    installation_id = get_github_app_installation_id(),\n    github_api_ep = \"https://api.github.com/\") {\n  resp &lt;- httr2::request(github_api_ep) |&gt;\n    httr2::req_url_path_append(\n      \"app\",\n      \"installations\",\n      installation_id,\n      \"access_tokens\"\n    ) |&gt;\n    httr2::req_auth_bearer_token(jwt) |&gt;\n    httr2::req_headers(\n      Accept = \"application/vnd.github+json\"\n    ) |&gt;\n    httr2::req_method(\"POST\") |&gt;\n    httr2::req_perform()\n\n  httr2::resp_check_status(resp)\n\n  httr2::resp_body_json(resp) |&gt;\n    purrr::pluck(\"token\")\n}"
  },
  {
    "objectID": "blogs/posts/2025-04-30_using-github-apps-for-authentication/index.html#putting-it-all-together",
    "href": "blogs/posts/2025-04-30_using-github-apps-for-authentication/index.html#putting-it-all-together",
    "title": "Using GitHub Apps for Authentication",
    "section": "Putting it all together",
    "text": "Putting it all together\nNow that we can generate a token using our app, we can write a function to query the list of repositories.\nWe need to keep in mind that the API returns a maximum of 100 items per page. Fortunately, {httr2} makes it easy to iterate through paginated responses.\n\n\nshow code for get_repos()\n#' Get GitHub Repositories for an organisation\n#'\n#' @param org The name of the GitHub organisation.\n#' @param pat Personal Access Token (PAT) for authentication. Defaults to the\n#'     output of `get_github_iat_pat()`.\n#' @param github_api_ep The base URL for the GitHub API. Defaults to\n#'     \"https://api.github.com/\".\nget_repos &lt;- function(\n    org,\n    pat = get_github_iat_pat(),\n    github_api_ep = \"https://api.github.com/\") {\n  req &lt;- httr2::request(github_api_ep) |&gt;\n    # build the url up, this should create something like\n    # https://api.github.com/orgs/YOUR_ORG/repos\n    httr2::req_url_path_append(\n      \"orgs\",\n      org,\n      \"repos\"\n    ) |&gt;\n    # add the correct requeste header for authentication using our PAT\n    httr2::req_auth_bearer_token(pat) |&gt;\n    # additional headers GitHub expects to be passed to their API\n    httr2::req_headers(\n      Accept = \"application/vnd.github+json\",\n      \"X-GitHub-Api-Version\" = \"2022-11-28\"\n    ) |&gt;\n    # append url query parameters, this should look something like\n    # https://api.github.com/orgs/YOUR_ORG/repos?per_page=100&page=1&sort=created\n    httr2::req_url_query(\n      per_page = 100, # anything between 1 and 100 max, as per the docs\n      page = 1,\n      sort = \"created\"\n    )\n\n  # because the API will only return a maximum of 100 items at a time, we need\n  # to query multiple times for each page of results.\n  # {httr2} makes this super easy, as the GitHub api returns page links in the\n  # Link header as per RFC8288  (https://datatracker.ietf.org/doc/html/rfc8288)\n  resps &lt;- httr2::req_perform_iterative(\n    req,\n    next_req = httr2::iterate_with_link_url(rel = \"next\")\n  )\n\n  # ensure that we got a non-error response for each request\n  purrr::walk(resps, httr2::resp_check_status)\n\n  # get the data from each response, iterate over them and just extract the\n  # \"name\" field that is returned for each item\n  resps |&gt;\n    httr2::resps_data(httr2::resp_body_json) |&gt;\n    purrr::map_chr(\"name\")\n}\n\n\nFinally, run the function like this:\n\n# replace the below as required\nSys.setenv(\"GITHUB_APP_ID\" = \"[app_id]\")\nSys.setenv(\"GITHUB_APP_PRIVATE_KEY\" = \"path-to-your.private-key.pem\")\n\nget_repos(\"Your Organisation\")"
  },
  {
    "objectID": "blogs/posts/2024-01-17_nearest_neighbour/index.html",
    "href": "blogs/posts/2024-01-17_nearest_neighbour/index.html",
    "title": "Nearest neighbour imputation",
    "section": "",
    "text": "Recently I have been gathering data by GP practice, from a variety of different sources. The ultimate purpose of my project is to be able to report at an ICB/sub-ICB level1. The various datasets cover different timescales and consequently changes in GP practices over time have left me with mismatching datasets.\n1 An ICB (Integrated Care Board) is a statutory NHS organisation responsible for planning health services for their local populationsMy approach has been to take as the basis of my project a recent GP List. Later in my project I want to perform calculations at a GP practice level based on an underlying health need and the data for this need is a CHD prevalence value from a dataset that is around 8 years old, and for which there is no update or alternative. From my recent list of 6454 practices, when I match to the need dataset, I am left with 151 practices without a value for need. If I remove these practices from the analysis then this could impact the analysis by sub-ICB since often a group of practices in the same area could be subject to changes, mergers and reorganisation.\nHere’s the packages and some demo objects to work with to create an example for two practices:\n\n\nCode\n# Packages\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidygeocoder)\nlibrary(leaflet)\nlibrary(viridisLite)\nlibrary(gt)\n\n# Create some data with two practices with no need data\n# and a selection of practices locally with need data\npractices &lt;- tribble(\n  ~practice_code, ~postcode, ~has_orig_need, ~value,\n  \"P1\", \"CV1 4FS\", 0, NA,\n  \"P2\", \"CV1 3GB\", 1, 7.3,\n  \"P3\", \"CV11 5TW\", 1, 6.9,\n  \"P4\", \"CV6 3HZ\", 1, 7.1,\n  \"P5\", \"CV6 1HS\", 1, 7.7,\n  \"P6\", \"CV6 5DF\", 1, 8.2,\n  \"P7\", \"CV6 3FA\", 1, 7.9,\n  \"P8\", \"CV1 2DL\", 1, 7.5,\n  \"P9\", \"CV1 4JH\", 1, 7.7,\n  \"P10\", \"CV10 0GQ\", 1, 7.5,\n  \"P11\", \"CV10 0JH\", 1, 7.8,\n  \"P12\", \"CV11 5QT\", 0, NA,\n  \"P13\", \"CV11 6AB\", 1, 7.6,\n  \"P14\", \"CV6 4DD\", 1, 7.9\n)\n\n# get domain of numeric data\n(domain &lt;- range(practices$has_orig_need))\n\n# make a colour palette\npal &lt;- colorNumeric(palette = viridis(2), domain = domain)\n\n\nTo provide a suitable estimate of need for the newer practices without values, all the practices in the dataset were geocoded2 using the geocode function from the {tidygeocoder} package.\n2 Geocoding is the process of converting addresses (often the postcode) into geographic coordinates (such as latitude and longitude) that can be plotted on a map.\npractices &lt;- practices |&gt;\n  mutate(id = row_number()) |&gt;\n  geocode(postalcode = postcode) |&gt;\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326)\n\n\n\nCode\npractices |&gt;\n  gt()\n\n\n\n\n\n\n\n\npractice_code\npostcode\nhas_orig_need\nvalue\nid\ngeometry\n\n\n\n\nP1\nCV1 4FS\n0\nNA\n1\nc(-1.50672203333333, 52.4140662333333)\n\n\nP2\nCV1 3GB\n1\n7.3\n2\nc(-1.51888, 52.4034199)\n\n\nP3\nCV11 5TW\n1\n6.9\n3\nc(-1.46746, 52.519)\n\n\nP4\nCV6 3HZ\n1\n7.1\n4\nc(-1.52231, 52.42367)\n\n\nP5\nCV6 1HS\n1\n7.7\n5\nc(-1.52542, 52.41989)\n\n\nP6\nCV6 5DF\n1\n8.2\n6\nc(-1.498344825, 52.4250186)\n\n\nP7\nCV6 3FA\n1\n7.9\n7\nc(-1.51787, 52.43135)\n\n\nP8\nCV1 2DL\n1\n7.5\n8\nc(-1.49105, 52.40582)\n\n\nP9\nCV1 4JH\n1\n7.7\n9\nc(-1.5069566, 52.4191685)\n\n\nP10\nCV10 0GQ\n1\n7.5\n10\nc(-1.52197, 52.54074)\n\n\nP11\nCV10 0JH\n1\n7.8\n11\nc(-1.5163199, 52.53723)\n\n\nP12\nCV11 5QT\n0\nNA\n12\nc(-1.46927, 52.51899)\n\n\nP13\nCV11 6AB\n1\n7.6\n13\nc(-1.45822, 52.52682)\n\n\nP14\nCV6 4DD\n1\n7.9\n14\nc(-1.50832, 52.44104)\n\n\n\n\n\n\n\nThis map shows the practices, purple are the practices with no need data and yellow are practices with need data available.\n\n\nCode\n# make map to display practices\nleaflet(practices) |&gt;\n  addTiles() |&gt;\n  addCircleMarkers(color = ~ pal(has_orig_need))\n\n\n\n\n\n\nThe data was split into those with, and without, a value for need. Using st_join from the {sf} package to join those without, and those with, a value for need, using the geometry to find all those within 1500m (1.5km).\n\nno_need &lt;- practices |&gt;\n  filter(has_orig_need == 0)\n\nwith_need &lt;- practices |&gt;\n  filter(has_orig_need == 1)\n\n\nneighbours &lt;- no_need |&gt;\n  select(no_need_postcode = postcode, no_need_prac_code = practice_code) |&gt;\n  st_join(with_need, st_is_within_distance, 1500) |&gt;\n  st_drop_geometry() |&gt;\n  select(id, no_need_postcode, no_need_prac_code) |&gt;\n  inner_join(x = with_need, by = join_by(\"id\"))\n\n\n\nCode\nleaflet(neighbours) |&gt;\n  addTiles() |&gt;\n  addCircleMarkers(color = \"purple\") |&gt;\n  addMarkers(-1.50686326666667, 52.4141089666667, popup = \"Practice with no data\") |&gt;\n  addCircles(-1.50686326666667, 52.4141089666667, radius = 1500) |&gt;\n  addMarkers(-1.46927, 52.51899, popup = \"Practice with no data\") |&gt;\n  addCircles(-1.46927, 52.51899, radius = 1500)\n\n\n\n\n\n\nThe data for the “neighbours” was grouped by the practice code of those without need data and a mean value was calculated for each practice to generate an estimated value.\n\nneighbours_estimate &lt;- neighbours |&gt;\n  group_by(no_need_prac_code) |&gt;\n  summarise(need_est = mean(value)) |&gt;\n  st_drop_geometry(select(no_need_prac_code, need_est))\n\nThe original data was joined back to the “neighbours”.\n\npractices_with_neighbours_estimate &lt;- practices |&gt;\n  left_join(neighbours_estimate, join_by(practice_code == no_need_prac_code)) |&gt;\n  st_drop_geometry(select(practice_code, need_est))\n\n\n\nCode\npractices_with_neighbours_estimate |&gt;\n  select(-has_orig_need, -id) |&gt;\n  gt()\n\n\n\n\n\n\n\n\npractice_code\npostcode\nvalue\nneed_est\n\n\n\n\nP1\nCV1 4FS\nNA\n7.68\n\n\nP2\nCV1 3GB\n7.3\nNA\n\n\nP3\nCV11 5TW\n6.9\nNA\n\n\nP4\nCV6 3HZ\n7.1\nNA\n\n\nP5\nCV6 1HS\n7.7\nNA\n\n\nP6\nCV6 5DF\n8.2\nNA\n\n\nP7\nCV6 3FA\n7.9\nNA\n\n\nP8\nCV1 2DL\n7.5\nNA\n\n\nP9\nCV1 4JH\n7.7\nNA\n\n\nP10\nCV10 0GQ\n7.5\nNA\n\n\nP11\nCV10 0JH\n7.8\nNA\n\n\nP12\nCV11 5QT\nNA\n7.25\n\n\nP13\nCV11 6AB\n7.6\nNA\n\n\nP14\nCV6 4DD\n7.9\nNA\n\n\n\n\n\n\n\nFinally, an updated data frame was created of the need data using the actual need for the practice where available, otherwise using estimated need.\n\npractices_with_neighbours_estimate &lt;- practices_with_neighbours_estimate |&gt;\n  mutate(need_to_use = case_when(value &gt;= 0 ~ value,\n    .default = need_est\n  )) |&gt;\n  select(practice_code, need_to_use)\n\n\n\n\n\n\n\n\n\npractice_code\nneed_to_use\n\n\n\n\nP1\n7.68\n\n\nP2\n7.30\n\n\nP3\n6.90\n\n\nP4\n7.10\n\n\nP5\n7.70\n\n\nP6\n8.20\n\n\nP7\n7.90\n\n\nP8\n7.50\n\n\nP9\n7.70\n\n\nP10\n7.50\n\n\nP11\n7.80\n\n\nP12\n7.25\n\n\nP13\n7.60\n\n\nP14\n7.90\n\n\n\n\n\n\n\nFor my project, this method has successfully generated a prevalence for 125 of the 151 practices without a need value, leaving just 26 practices without a need. This is using a 1.5 km radius. In each use case there will be a decision to make regarding a more accurate estimate (smaller radius) and therefore fewer matches versus a less accurate estimate (using a larger radius) and therefore more matches.\nThis approach could be replicated for other similar uses/purposes. A topical example from an SU project is the need to assign population prevalence for hypertension and compare it to current QOF3 data. Again, the prevalence data is a few years old so we have to move the historical data to fit with current practices and this leaves missing data that can be estimated using this method.\n\n\n3 QOF (Quality and Outcomes Framework) is a voluntary annual reward and incentive programme for all GP practices in England, detailing practice achievement results."
  },
  {
    "objectID": "blogs/posts/2023-04-26_reinstalling-r-packages/index.html",
    "href": "blogs/posts/2023-04-26_reinstalling-r-packages/index.html",
    "title": "Reinstalling R Packages",
    "section": "",
    "text": "R 4.3.0 was released last week. Anytime you update R you will probably find yourself in the position where no packages are installed. This is by design - the packages that you have installed may need to be updated and recompiled to work under new versions of R.\nYou may find yourself wanting to have all of the packages that you previously used, so one approach that some people take is to copy the previous library folder to the new versions folder. This isn’t a good idea and could potentially break your R install.\nAnother approach would be to export the list of packages in R before updating and then using that list after you have updated R. This can cause issues though if you install from places other than CRAN, e.g. bioconductor, or from GitHub.\nSome of these approaches are discussed on the RStudio Community Forum. But I prefer an approach of having a “spring clean”, instead only installing the packages that I know that I need.\nI maintain a list of the packages that I used as a gist. Using this, I can then simply run this script on any new R install. In fact, if you click the “raw” button on the gist, and copy that url, you can simply run\nsource(\"https://gist.githubusercontent.com/tomjemmett/c105d3e0fbea7558088f68c65e68e1ed/raw/a1db4b5fa0d24562d16d3f57fe8c25fb0d8aa53e/setup.R\")\nGenerally, sourcing a url is a bad idea - the reason for this is if it’s not a link that you control, then someone could update the contents and run arbritary code on your machine. In this case, I’m happy to run this as it’s my own gist, but you should be mindful if running it yourself!\nIf you look at the script I first install a number of packages from CRAN, then I install packages that only exist on GitHub."
  },
  {
    "objectID": "blogs/posts/2025-05-16 data-science-as-a-product/index.html",
    "href": "blogs/posts/2025-05-16 data-science-as-a-product/index.html",
    "title": "Data science as a product",
    "section": "",
    "text": "Data Science\n\n\nData science in healthcare is booming. Every day new advances emerge that promise to deepen understanding, improve screening or optimise treatments. But as this panoply of models grows, so does the scrapheap of potentially useful ones that have never helped a single patient. The vast majority of promising new models, even when published in prestigious journals and cited extensively, are never fully translated into clinical practice.\nThere are many reasons for this, and many potential solutions. Viewing a model through the lens of a ‘product’, produced and optimised for ‘customers’, is one potential solution.\nAt The Strategy Unit, our acute demand forecasting model for the New Hospitals Programme has been in use for nearly 2 years. The suite of apps, simulation models, bespoke functions and outputs that make up this ‘model’ has been constantly evolving since its inception.\nThis is a fast-paced environment in which to do data science, since the team are continuously improving the model alongside its active deployment. The NHP model is not ‘new’, and has already had considerable national impact, but there are a whole host of potential applications which we are yet to explore.\nIt is all too easy to get your head down and work away at the next issue in the backlog without realising that doing something else, something you may not even have thought of, could be preferable.\nTo avoid this problem, we have created a ‘product team’ for the NHP model.\nBorrowing from the realm of software development and Scrum, our team has a Product Owner, a Product Manager and a Customer Engagement Manager, all overseen by a project director. The team is in its infancy, but almost immediately the need for it was obvious. The overarching goal of the team is to ensure that the model (and all its component parts) meet the needs of current and future ‘customers’, and prioritising work to ensure the medium and long-term ambitions for the product are met.\nHow we do this requires, first, a brief explanation of the roles of the product team members:\n*Product Owner**: As product owner, it is my responsibility to ensure that the work the data science team does is moving us forward towards our shared goals. This means that I need to have a clear view of the work backlog, how each element relates to the vision for the product and understand the competing priorities coming from our stakeholders. I distil those into plans of action that the data science team can fulfil.\n*Product Manager**: This person’s priority is to have a clear understanding of where we are now. Who are the current users, are their needs being met, what niche does the model fit into currently, who are the main competitors, what are the potential opportunities for growth and development. What are we good at, what could/should we improve? The product manager does not need to be a data scientist but needs to think holistically and be comfortable talking to coders, planners, customers and everyone in between. They have a key role to play in helping prioritise the data science team’s work and contextualising new requests.\nCustomer Engagement: How do we know if the model is fulfilling the needs of our existing customers? What are the pain points? How could we improve the offer? Gathering and assimilating this information is essential if we are to keep the model friction-free and fit-for-purpose. This role is very time consuming and iterative but is crucial to the model’s success and eventual impact.\nOur product team works closely together.\nWe hold regular meetings and end up having ad hoc chats most days.\nWe create our own tools for planning, prioritisation and information dissemination, using whatever software works for us. Currently this means a combination of GitHub projects, Excel workbooks, Quarto documents and Canva. We’ve created a product vision and goal, adopted a prioritisation framework for assessing proposed new functionality, we’ve created Gantt charts and a bespoke product team backlog, and gathered hours of user feedback, all within the first few months of the team’s inception. All this user feedback is fed into the data science team’s work backlog, ensuring that it will be done as determined by our prioritisation exercises. Our current users know that their opinions matter, that we are continually improving a tool that is as intuitive, functional and impactful as they need it to be to do their jobs. Where the product team identifies new use cases for the model, the work to understand the implications of this are captured, examined, and measured.\nThrough conversation and research, we help the team to make decisions around whether a risky new path may be worth the work.\nAlthough data scientists are excellent at building useful tools, conceptualisation of the needs of new users is a different challenge entirely.\nWe want our data scientists to do data science because they love it and excel at it – other tasks should sit with other teams.\nEvery iteration of data science work (we operate in ‘sprints’, meaning we release code every 4 weeks) moves us one increment closer to the ultimate vision for the product, and this journey is transparent, building the trust of stakeholders. The product team helps us sharpen the point of the spear, helping us clarify what we should do and why, and it does so without burdening the data science team with any extra workload. This frees them up to do what they do best.\nIts early days for this way of working, but I believe we’ve already seen benefits.\nThe more our workload grows, the more important it is to have team members whose job it is to help steer the ship where we all want it to go. To realise opportunities and help us make the most of our work."
  },
  {
    "objectID": "blogs/posts/2025-07-28_imputing-data/index.html",
    "href": "blogs/posts/2025-07-28_imputing-data/index.html",
    "title": "Imputing missing data",
    "section": "",
    "text": "In the world of data analysis, missing values can feel like puzzle pieces that just won’t fit, leaving analysts frustrated and insights incomplete. But what if I told you that imputing these missing values could be the key to unlocking a treasure trove of insights? By skilfully filling in the gaps, we can enhance the integrity of our datasets and elevate the quality of our analyses.\nThis blog explores the importance of addressing missing data and how an effective imputation strategy can transform incomplete datasets into powerful tools for decision-making.\nRecently, I encountered a challenge while working with a dataset that had approximately 3% missing values. The goal was to match sites that received an intervention with control sites using the {MatchIt} package. However, the package excluded records with missing data, which would have resulted in the loss of around 40 intervention sites.\nFurther investigation revealed the missingness was not at random, which meant that proceeding with a complete-case analysis would have introduced biases into the resulting analysis.\nTo address these issues, I turned to the Multiple Imputation by Chained Equations (MICE) algorithm, implemented in the {mice} package. The MICE algorithm offered a solution by imputing likely values for the missing data which resulted in reduced bias in the resulting analysis and increased accuracy of the matched sites."
  },
  {
    "objectID": "blogs/posts/2025-07-28_imputing-data/index.html#whats-the-problem-with-missing-data",
    "href": "blogs/posts/2025-07-28_imputing-data/index.html#whats-the-problem-with-missing-data",
    "title": "Imputing missing data",
    "section": "What’s the problem with missing data?",
    "text": "What’s the problem with missing data?\nWhen datasets contain gaps, whether due to non-response in surveys, data entry errors, or system malfunctions, the integrity of the analysis is compromised.\nIn my case, missing data presented two main challenges. First, the missingness prevented some of my intervention sites from being matched with control sites, which reduced the overall dataset size and excluded important information from my analysis. Second, the type of missingness in my data meant that excluding incomplete records introduced biases into the dataset.\nUnderstanding the type of missingness in your dataset is essential for choosing the right approach to handle it.\n\nTypes of missingness\nMissing data can be categorised into three types:\n\nMissing Completely at Random (MCAR), where the missingness is entirely independent of both observed and unobserved data\nMissing at Random (MAR), where the missingness is related to observed data but not the missing values themselves\nMissing Not at Random (MNAR), where the missingness is related to the unobserved data.\n\nThese types are summarised in the below table along with strategies for handling them.\n\nTypes of missing data in data analysis: theoretical background, Medium\n\n\nMissing Completely at Random (MCAR)\nMissing at Random (MAR)\nMissing Not at Random (MNAR)\n\n\n\n\nThe missingness is completely unrelated to the data (no pattern).\nAnalyses remain unbiased as missingness does not systematically affect the data.\nRemove rows with missing data, or simple imputation using, e.g. mean.\nThe gaps are related to observed variable/s (conditional).\nBiases are introduced if missingness is not accounted for.\nCan be addressed using imputation methods using observed data to predict missing values.\nThe gaps are related to observed variable/s and also to themselves.\nNeed to collect a sample of missing data to be sure.\nAnalysis is biased.\nSophisticated techniques can possibly fix.\n\n\n\nUsing techniques described in the Epidemiologists Handbook, I found that the missingness in my data was non-random, specifically either MAR or MNAR. Since I could not collect any of the missing values (as I was working with a nationally-produced dataset), I decided to treat my data as Missing at Random (MAR), which meant that biases could be introduced if I failed to account for the missingness in the data."
  },
  {
    "objectID": "blogs/posts/2025-07-28_imputing-data/index.html#handling-missingness",
    "href": "blogs/posts/2025-07-28_imputing-data/index.html#handling-missingness",
    "title": "Imputing missing data",
    "section": "Handling missingness",
    "text": "Handling missingness\nHandling missingness can be approached through various techniques, each with its own strengths and weaknesses.\nOne common method is to delete rows containing missing data (i.e. complete-case analysis). This method is simple to do and is often a default method for statistical packages, however, it is only appropriate for situations where the data is missing completely at random (MCAR).\nAnother approach is imputation, where missing values are filled in based on observed data. Simple methods include the mean or median imputation, which replaces missing values with the average of the observed values. While straightforward, these methods can reduce variability in the dataset. More sophisticated techniques, such as regression imputation, use regression models to predict and fill in missing values based on other variables, providing potentially more accurate estimates, but introducing bias if the model is not well-specified.\nFor more advanced handling of missing data, multiple imputation creates several different imputed datasets and combines the results to account for uncertainty in the missing data. This approach offers more robust estimates and better reflects the variability in the data.\n\n\n\nTechniques for handling missing data"
  },
  {
    "objectID": "blogs/posts/2025-07-28_imputing-data/index.html#applied-example",
    "href": "blogs/posts/2025-07-28_imputing-data/index.html#applied-example",
    "title": "Imputing missing data",
    "section": "Applied example",
    "text": "Applied example\nNow that we’ve covered the theory of missingness, its impact on analyses, and ways to deal with it, let’s put these techniques into practice using a fictional dataset.\n\nWe’ll create a sample dataset that simulates authentic relationships between age, gender and salary. This will provide the foundation for our experiment.\nNext, we’ll intentionally introduce non-random missingness into the dataset to create a realistic scenario with incomplete data.\nWe’ll then fill in the missing data using four different imputation techniques.\nFinally, we’ll compare the results from each imputation technique with the original dataset to evaluate their performance and see how well they work with this example dataset.\n\n\nCreate a complete dataset\nFirst, lets create an example dataset consisting of UK salaries by age and gender in 2025. This data is loosely based on figures reported in a Forbes article and based on Office for National Statistics (ONS) data.\nOur data will contain details for 1,000 people’s salary along with their age and gender:\n\nAge will be independently generated using a poisson distribution using an average of 40 years,\nGender will be independently generated using a random sample from a list of either Male or Female.\nSalary will be dependent on both age and gender such that there is an ‘n’ shaped distribution between age with the highest salary for those aged 40 to 49 years, and introduce a 7% gender pay gap between men and women.\n\n\n\nCode\n# decide the number of rows to create\nrows &lt;- 1000\n\n# Ensure reproducibility\nset.seed(123)\n\n# Generate the data\ndf_complete &lt;-\n  tibble::tibble(\n    # age and gender are independent\n    age = rpois(n = rows, lambda = 40),\n    gender = sample(x = c('Male', 'Female'), size = rows, replace = TRUE),\n\n    # generate salary based on age group\n    salary_age = dplyr::case_when(\n      age %in% 18:21 ~ rnorm(n = rows, mean = 24000, sd = 5000),\n      age %in% 22:29 ~ rnorm(n = rows, mean = 33000, sd = 6000),\n      age %in% 30:39 ~ rnorm(n = rows, mean = 40000, sd = 7000),\n      age %in% 40:49 ~ rnorm(n = rows, mean = 43000, sd = 8000),\n      age %in% 50:59 ~ rnorm(n = rows, mean = 41000, sd = 7000),\n      age &gt; 59       ~ rnorm(n = rows, mean = 36000, sd = 5000)\n    ),\n\n    # gender pay gap of 7% across all age ranges\n    salary = dplyr::case_match(\n      gender,\n      'Female' ~ salary_age * 0.93,\n      .default = salary_age\n    ) |&gt;\n      round(digits = 0)\n  ) |&gt;\n  dplyr::select(-salary_age)\n\n# see a sample\ndf_complete |&gt; \n  dplyr::slice_head(n = 10) |&gt; \n  gt::gt() |&gt; \n  gt::opt_stylize(style = 5, add_row_striping = FALSE)\n\n\n\n\n\n\n\n\nage\ngender\nsalary\n\n\n\n\n36\nMale\n34410\n\n\n47\nMale\n41889\n\n\n29\nMale\n31711\n\n\n40\nFemale\n18838\n\n\n50\nMale\n30264\n\n\n42\nFemale\n33879\n\n\n31\nMale\n29416\n\n\n29\nFemale\n17698\n\n\n47\nFemale\n44293\n\n\n42\nFemale\n50074\n\n\n\n\n\n\n\nHere we see a sample of the 1,000 available in our Complete dataset. This data has built-in relationships between salary and age, and also between salary and gender.\nWe can visualise the data for age and salary as density distributions. Below are two function definitions, one for generating a density plot for a given variable and another for comparing age and salary distributions between our complete dataset and another dataset.\n\n\nCode\n#' Function to create a density plot\n#' \n#' @param df Tibble of data\n#' @param x_val Variable to plot on the x axis\n#' @param x_label String - Name of the variable in the x axis\n#' @param summary String - Summary stats (mean and sd) for `x_val`\n#' @param complete Boolean (default = FALSE) - Is this complete data?\nplot_density &lt;- function(df, x_val, x_label, summary, complete = FALSE) {\n  \n  # define our fill colour:\n  # reference (complete) data is coloured orange,\n  # comparison data is coloured blue\n  if (complete) {\n    fill_colour &lt;- adjustcolor(\n      col = \"#f9bf07\",\n      alpha.f = 0.2\n    )\n  } else {\n    fill_colour &lt;- adjustcolor(\n      col = \"#5881c1\",\n      alpha.f = 0.2\n    )\n  }\n  \n  # produce the plot\n  p &lt;- \n    df |&gt; \n    ggplot2::ggplot(ggplot2::aes(x = {{x_val}})) +\n    ggplot2::geom_density(\n      fill = fill_colour,\n      outline.type = \"upper\"\n    ) +\n    ggplot2::labs(title = x_label) +\n    ggplot2::annotate(\n      geom = 'text',\n      label = summary,\n      size = 14/ggplot2::.pt,\n      x = mean(df |&gt; dplyr::pull({{x_val}}), na.rm = TRUE),\n      y = 0,\n      hjust = 0.5,\n      vjust = 0\n    ) +\n    ggplot2::theme_minimal(base_size = 18) +\n    ggplot2::theme(\n      # titles\n      plot.title = ggplot2::element_text(size = 22),\n      # axes\n      axis.ticks = ggplot2::element_blank(),\n      axis.line = ggplot2::element_blank(),\n      axis.title = ggplot2::element_blank(),\n      axis.text.y = ggplot2::element_blank(),\n      axis.text.x = ggplot2::element_text(size = 18),\n      # grid lines\n      panel.grid = ggplot2::element_blank(),\n      panel.border = ggplot2::element_blank(),\n      # background\n      plot.background = ggplot2::element_blank()\n    )\n  \n  # add thousands suffix to salary outputs\n  if (x_label == \"Salary\") {\n    p &lt;-\n      p +\n      ggplot2::scale_x_continuous(\n        labels = scales::label_number(suffix = 'k', scale = 1e-3)\n      )\n  }\n  \n  # return the plot\n  return(p)\n}\n\n\n#' Function to compare density distributions\n#'\n#' @param df_complete Tibble - the 'complete' dataset\n#' @param df Tibble - the comparison dataset containing missing or imputed values\n#'\n#' @returns ggplot2 object\ncompare_distributions &lt;- function(df_complete, df = NA) {\n  \n  # summary stats for age and salary\n  complete_summary_age = glue::glue(\n    \"Mean: {round(mean(df_complete$age, na.rm = TRUE), digits = 2)}\\n\",\n    \"SD: {round(sd(df_complete$age, na.rm = TRUE), digits = 2)}\"\n  )\n  \n  complete_summary_salary = glue::glue(\n    \"Mean: {round(mean(df_complete$salary, na.rm = TRUE)/1000, digits = 2)}k\\n\",\n    \"SD: {round(sd(df_complete$salary, na.rm = TRUE)/1000, digits = 2)}k\"\n  )\n  \n   # get the 'complete' density plots\n  complete_age &lt;-\n    plot_density(\n      df = df_complete,\n      x_val = age,\n      x_label = \"Age\",\n      summary = complete_summary_age,\n      complete = TRUE\n    )\n  \n  complete_salary &lt;-\n    plot_density(\n      df = df_complete,\n      x_val = salary,\n      x_label = \"Salary\",\n      summary = complete_summary_salary,\n      complete = TRUE\n    )\n  \n  # check whether a comparison dataset has been provided\n  if (!missing(df)) {\n    \n    # summary stats for age and salary\n    input_summary_age = glue::glue(\n      \"Mean: {round(mean(df$age, na.rm = TRUE), digits = 2)}\\n\",\n      \"SD: {round(sd(df$age, na.rm = TRUE), digits = 2)}\"\n    )\n  \n    input_summary_salary = glue::glue(\n      \"Mean: {round(mean(df$salary, na.rm = TRUE)/1000, digits = 2)}k\\n\",\n      \"SD: {round(sd(df$salary, na.rm = TRUE)/1000, digits = 2)}k\"\n    )\n    \n    # get the comparison density plots\n    comparison_age &lt;-\n      plot_density(\n        df = df,\n        x_val = age,\n        x_label = \"Age\",\n        summary = input_summary_age,\n        complete = FALSE\n      )\n    \n    comparison_salary &lt;-\n      plot_density(\n        df = df,\n        x_val = salary,\n        x_label = \"Salary\",\n        summary = input_summary_salary,\n        complete = FALSE\n      )\n    \n    # combine the charts to compare\n    plots &lt;- \n      patchwork::wrap_plots(\n        complete_age, comparison_age,\n        complete_salary, comparison_salary,\n        nrow = 2\n      )\n  } else {\n    # there is only the complete dataset included, so show that\n    plots &lt;-\n      patchwork::wrap_plots(\n        complete_age, complete_salary,\n        ncol = 1\n      )\n  }\n  \n  # return the plots\n  return(plots)\n  \n}\n\n\nWe’ll see examples comparing datasets later, but for now lets examine how age and salary are distributed among our pristine complete dataset.\n\n\nCode\ncompare_distributions(df_complete = df_complete)\n\n\n\n\n\n\n\n\n\nThe average (mean) age of people in our dataset is 39.91 years with a standard deviation (SD) of 6.24 years. This indicates that most individuals are clustered around this average age, but there is some variability, with ages ranging from 18 to 65 years.\nThe average (mean) salary is £39,400 with a SD of £7,410. There is a notable spread in salaries, influenced by factors such as age and gender.\nWhile we have established these averages, it’s important to note that neither the age nor salary distribution is perfectly smooth. Several factors contribute to the irregularities observed in the density plots:\nRandomness in data generation: The use of a Poisson distribution for age generation introduces inherent randomness. This randomness can lead to fluctuations in the density curve, resulting in peaks and troughs that may not represent a perfectly normal distribution.\nRelationships between salary and gender: The salary calculations were influenced by both age and gender, creating an ‘n’ shaped distribution where salaries peak for individuals aged 40 to 49 years. The 7% gender pay gap further complicates the distribution, as it introduces additional variability based on gender.\nThese plots represent the complete dataset, showcasing all the complexities we introduced during the data generation process. In many ways, they illustrate an idealised version of data - one that is fully complete, with no missing values and clear relationships between variables. This is often referred to as the “unknowable truth” of the data, as it reflects a perfect scenario that analysts rarely experience in practice.\n\n\nIntroducing missingness in our dataset\nWe will now introduce missingness into our complete dataset, creating a new dataset that reflects a more realistic situation. Specifically, we will implement a Missing At Random (MAR) mechanism, where the missingness is conditionally related to other observed variables.\nWe will remove 100 values (10% of the dataset) from each variable based on the following criteria:\nAge missingness: we will randomly select 100 individuals whose salary is above £45,000 and set their age to missing.\nSalary missingness: we will randomly select 100 individuals whose age is below 40 years and set their salary to missing.\nGender missingness: we will randomly select 100 individuals to have their gender value set to missing, independent of other variables.\n\n\nCode\n# take a copy of the complete data\ndf_missing &lt;- df_complete\nseed &lt;- 100\n\n# age - where salary &gt; 45000\nset.seed(seed) # for reproducibility\ndf_missing$age[\n  sample(which(df_complete$salary &gt; 45000), \n         size = 100, \n         replace = FALSE)] &lt;- NA\n  \n# salary - 90 where age &gt; 45, 10 from rest\nset.seed(seed) # for reproducibility\ndf_missing$salary[\n  sample(which(df_complete$age &lt; 40), \n         size = 100, \n         replace = FALSE)] &lt;- NA\n\n# gender - 100 randomly\nset.seed(seed) # for reproducibility\ndf_missing$gender[\n  sample(1:rows, \n         size = 100, \n         replace = FALSE)] &lt;- NA\n\n# see a sample\nmissing_cell_style &lt;- gt::cell_fill(color = \"#ec6555\")\ndf_missing |&gt; \n  dplyr::slice_head(n = 10) |&gt; \n  gt::gt() |&gt; \n  gt::opt_stylize(style = 5, add_row_striping = FALSE) |&gt; \n  # highlight cells with missing values\n  gt::tab_style(\n    style = missing_cell_style,\n    locations = gt::cells_body(columns = age, rows = is.na(age))\n  ) |&gt; \n  gt::tab_style(\n    style = missing_cell_style,\n    locations = gt::cells_body(columns = gender, rows = is.na(gender))\n  ) |&gt; \n  gt::tab_style(\n    style = missing_cell_style,\n    locations = gt::cells_body(columns = salary, rows = is.na(salary))\n  )\n\n\n\n\n\n\n\n\nage\ngender\nsalary\n\n\n\n\n36\nMale\n34410\n\n\n47\nMale\n41889\n\n\n29\nMale\n31711\n\n\n40\nFemale\n18838\n\n\n50\nMale\n30264\n\n\n42\nFemale\n33879\n\n\n31\nNA\n29416\n\n\n29\nFemale\nNA\n\n\n47\nFemale\n44293\n\n\n42\nFemale\n50074\n\n\n\n\n\n\n\nHere we can see a sample from our dataset with missingness introduced. Two missing values are shown here, highlighted in red.\nWe will now explore techniques handling this missingness.\n\n\nTechnique 1 - complete-case analysis\nThe complete-case analysis approach, also known as listwise deletion, involves removing any cases that have missing values. This means only observations with complete data for age, gender and salary are used in the analysis.\n\n\nCode\n# remove records containing missing values\ndf_removal &lt;-\n  df_missing |&gt; \n  na.omit()\n\n# see the result\ncompare_distributions(df_complete = df_complete, df = df_removal)\n\n\n\n\n\n\n\n\n\nHere we see the original dataset (left, yellow) compared with the newly created dataset (right, blue) produced by omitting records where they contain one or more missing values.\nThis approach has a significant impact on the shape of the distributions, especially for salary. In the dataset with missing values, there is a noticeable gap or ‘chunk’ missing from the right-hand side of the salary distribution. Additionally, the salary standard deviation decreases, indicating there is less variability in the imputed dataset than we know existed in the original complete data.\n\n\nTechnique 2 - substituting averages\nThis technique involves replacing missing values with the mean of the observed values for age and salary.\n\n\nCode\ndf_average &lt;- \n  df_missing |&gt; \n  dplyr::mutate(\n    age = dplyr::case_match(\n      age,\n      NA ~ mean(age, na.rm = TRUE),\n      .default = age\n    ) |&gt; \n      round(digits = 0),\n    \n    salary = dplyr::case_match(\n      salary,\n      NA ~ mean(salary, na.rm = TRUE),\n      .default = salary\n    ) |&gt; \n      round(digits = 0)\n  )\n\n# see the result\ncompare_distributions(df_complete = df_complete, df = df_average)\n\n\n\n\n\n\n\n\n\nThis technique produces unusual distributions, characterised by noticeable spikes in the central region in the newly created data, (right, blue), that result from substituting all missing values with the average. While this method does not significantly alter the mean values, it does lead to much smaller standard deviations for both age and salary. This reduction indicates there is considerably less variability in the imputed data compared to the original dataset.\n\n\nTechnique 3 - linear regression\nIn this technique the missing values are imputed based on a linear regression using the relationships with the other variable. Specifically, salary is imputed based on its relationship with age, and age is imputed based on its relationship with salary.\n\n\nCode\n# model age and salary\nmodel_age &lt;- glm(\n  data = df_missing |&gt; na.omit(),\n  formula = age ~ salary,\n  na.action = \"na.exclude\"\n)\n\nmodel_salary &lt;- glm(\n  data = df_missing |&gt; na.omit(),\n  formula = salary ~ age,\n  na.action = \"na.exclude\"\n)\n\n# fill in details using regression\ndf_regression &lt;-\n  df_missing |&gt; \n  dplyr::mutate(\n    age_pred = predict(\n      object = model_age,\n      newdata = data.frame(\n        gender = df_average$gender, \n        salary = df_average$salary)\n    ) |&gt; \n      round(digits = 0),\n    \n    salary_pred = predict(\n      object = model_salary,\n      newdata = data.frame(\n        gender = df_average$gender, \n        age = df_average$age)\n    ) |&gt; \n      round(digits = 0),\n    \n    age = dplyr::coalesce(age, age_pred),\n    salary = dplyr::coalesce(salary, salary_pred)\n  ) |&gt; \n  dplyr::select(-c(age_pred, salary_pred))\n\n# see the result\ncompare_distributions(df_complete = df_complete, df = df_regression)\n\n\n\n\n\n\n\n\n\nThis technique also produces unusual distributions, characterised by noticeable spikes in the new dataset (right, blue). Both the imputed age and imputed salary exhibit much smaller standard deviations compared to the original dataset, indicating reduced variability in the imputed data.\n\n\nTechnique 4 - Multivariate Imputation by Chained Equations (MICE)\nThe MICE algorithm works by iteratively imputing missing values using a series of regression models. It initialises missing values with a starting estimate, then for each variable with missing values, creates a regression model using the observed values and other variables. The regression model is used to predict the missing values, and this process is repeated for each variable with missing values, updating the estimates at each iteration. This iterative process is repeated multiple times, creating multiple imputed datasets, which are then combined to obtain a single, final estimate.\n\n\nCode\n# let {mice} suggest an imputation method for each variable\ninit &lt;- mice::mice(df_missing, maxit = 0)\n\n# calculate the imputed values\nimputed &lt;- mice::mice(\n  data = df_missing,          # data to be used\n  m = 10,                     # number of multiple imputations (dflt = 5)\n  method = init$method,       # matching method - will go with default\n  seed = 123,                 # for reproducibility\n  maxit = 10,                 # 10 iterations (default = 5)\n  printFlag = FALSE           # don't print history to the console\n)\n\n# complete the data\ndf_mice &lt;- mice::complete(imputed)\n\n# see the result\ncompare_distributions(df_complete = df_complete, df = df_mice)\n\n\n\n\n\n\n\n\n\nHere we see the original dataset (left, yellow) compared with the newly created dataset (right, blue) produced by multiple imputation using the MICE algorithm.\nWhile this approach is not without its imperfections, it provides significantly better estimates for the missing data compared to previous techniques. As a result, the density distributions of the imputed dataset resemble those of the original dataset, indicating a more accurate representation of the underlying data.\n\n\nTechnique review\nWe have explored four techniques for handling missing data: complete-case analysis, mean imputation, linear regression and multiple imputation using the MICE algorithm. Now, let’s consolidate our findings to see how these techniques compare in terms of their effectiveness.\nIn the tables below, we compile the mean and standard deviation (SD) values for age and salary from each technique, along with the absolute differences from the original dataset’s mean and standard deviations. This comparison highlights the performance of each method in imputing missing data.\n\nMICE performs best at imputing age\nThe MICE algorithm performed the best at imputing missing age data, providing mean and standard deviation values that are the closest overall match to the original dataset. While it is not perfect, it significantly improves the accuracy of the imputed values.\nBoth the mean imputation and linear regression techniques performed reasonably well in estimating the average value of age. However, they resulted in significantly different standard deviations compared to the original dataset. This discrepancy indicates that the resulting distributions were altered, as observed in our earlier plot comparisons.\n\n\nCode\n# list the data showing different imputation approaches\ndata &lt;- \n  tibble::tribble(\n    ~df, ~df_name,\n    df_complete, \"Original\",\n    df_removal, \"Complete-case\",\n    df_average, \"Average\",\n    df_regression, \"Regression\",\n    df_mice, \"MICE\"\n  )\n\n# summarise the results to a single df\ndf_summary_test &lt;-\n  purrr::map2_dfr(\n    .x = data$df,\n    .y = data$df_name,\n    .f = \\(.x, .y) {\n      .x |&gt; \n        dplyr::summarise(\n          set = .y,\n          age_mean = mean(age),\n          age_sd = sd(age),\n          salary_mean = mean(salary),\n          salary_sd = sd(salary)\n        )\n    }\n  ) |&gt; \n  # split age and salary to separate rows\n  tidyr::pivot_longer(\n    cols = -set,\n    names_to = c(\"measure\", \".value\"),\n    names_pattern = \"(.*)_(.*)\"\n  ) |&gt; \n  # convert salary to thousands to put on the same scale as age\n  dplyr::mutate(\n    mean = dplyr::case_when(\n      measure == \"salary\" ~ mean * 1e-3,\n      .default = mean\n    ),\n    sd = dplyr::case_when(\n      measure == \"salary\" ~ sd * 1e-3,\n      .default = sd\n    )\n  )\n\n# define a function to display results in a formatted table\n#' Produce a summary table of results for a given measure\n#'\n#' @param df Tibble containing summary results from each imputation approach\n#' @param .measure String identifying the measure to summarise (either 'age' or 'salary')\n#'\n#' @returns gt table\nimputation_summary_table &lt;- function(df, .measure = c(\"age\", \"salary\")) {\n  \n  # get the original data as reference\n  df_original &lt;-\n    df |&gt; \n    dplyr::filter(measure == .measure, set == \"Original\") |&gt; \n    dplyr::select(mean, sd)\n  \n  # filter the data for the specified measure and work out differences\n  df_summary &lt;-\n    df |&gt; \n    dplyr::filter(measure == .measure) |&gt; \n    dplyr::mutate(\n      # add the 'original' mean and sd values to each row\n      mean_original = df_original$mean,\n      sd_original = df_original$sd,\n      \n      # work out  differences between the imputation and original\n      mean_difference = dplyr::case_when(\n        set == \"Original\" ~ NA, # leave original blank\n        .default = abs(mean - mean_original)\n      ),\n      sd_difference = dplyr::case_when(\n        set == \"Original\" ~ NA, # leave original blank\n        .default = abs(sd - sd_original)\n      ),\n      \n      # determine which approach gives the overall lowest difference\n      overall_difference = mean_difference + sd_difference,\n      trophy = dplyr::case_when(\n        overall_difference == min(overall_difference, na.rm = TRUE) ~ \"🏆\"\n      )\n    )\n  \n  # display as a formatted table\n  tab &lt;- \n    df_summary |&gt; \n    gt::gt() |&gt; \n    gt::tab_options(quarto.disable_processing = TRUE) |&gt; \n    gt::fmt_number(decimals = 2) |&gt; \n    gt::cols_hide(columns = c(\n      \"measure\", \n      \"mean_original\", \n      \"sd_original\"\n    )) |&gt; \n    gt::sub_missing(missing_text = \"\") |&gt; \n    gt::data_color(\n      columns = c(\"mean_difference\", \"sd_difference\", \"overall_difference\"),\n      palette = c(\"#5881c1\", \"#dff9fb\"),\n      na_color = \"white\"\n    ) |&gt; \n    gt::cols_label(\n      set = \"Set\",\n      mean = \"Mean\",\n      sd = \"SD\",\n      mean_difference = \"Mean\",\n      sd_difference = \"SD\",\n      overall_difference = \"Overall\",\n      trophy = \"Trophy\"\n    ) |&gt; \n    gt::tab_spanner(\n      columns = c(mean_difference, sd_difference, overall_difference),\n      label = \"Difference from 'Original'\"\n    ) |&gt; \n    gt::cols_align(columns = trophy, align = \"center\")\n  \n  return(tab)\n}\n\n# summarise the approaches for 'age'\nimputation_summary_table(df = df_summary_test, .measure = \"age\")\n\n\n\n\n\n  \n    \n      Set\n      Mean\n      SD\n      \n        Difference from 'Original'\n      \n      Trophy\n    \n    \n      Mean\n      SD\n      Overall\n    \n  \n  \n    Original\n39.91\n6.24\n\n\n\n\n    Complete-case\n40.20\n6.36\n0.30\n0.12\n0.42\n\n    Average\n39.76\n5.95\n0.15\n0.29\n0.43\n\n    Regression\n39.93\n5.99\n0.02\n0.25\n0.27\n\n    MICE\n39.86\n6.31\n0.05\n0.08\n0.13\n🏆\n  \n  \n  \n\n\n\n\n\n\nMICE performs best at imputing salary\nThe MICE algorithm also performed best at imputing missing salary data, providing mean and standard deviation values that are the closest overall match to the original dataset.\n\n\nCode\n# summarise the approaches for 'salary'\nimputation_summary_table(df = df_summary_test, .measure = \"salary\")\n\n\n\n\n\n  \n    \n      Set\n      Mean\n      SD\n      \n        Difference from 'Original'\n      \n      Trophy\n    \n    \n      Mean\n      SD\n      Overall\n    \n  \n  \n    Original\n39.40\n7.41\n\n\n\n\n    Complete-case\n38.42\n7.06\n0.98\n0.35\n1.33\n\n    Average\n39.54\n7.06\n0.14\n0.35\n0.49\n\n    Regression\n39.33\n7.09\n0.07\n0.32\n0.39\n\n    MICE\n39.48\n7.38\n0.09\n0.03\n0.12\n🏆\n  \n  \n  \n\n\n\n\nIt is interesting that complete-case analysis performed poorly on this dataset, leading to distorted average values for both age and salary. This is concerning, given that complete-case analysis is often the default method used by many statistical packages to handle missing data. This highlights the importance of reviewing and potentially adjusting for missing data to ensure accurate results."
  },
  {
    "objectID": "blogs/posts/2025-07-28_imputing-data/index.html#summary",
    "href": "blogs/posts/2025-07-28_imputing-data/index.html#summary",
    "title": "Imputing missing data",
    "section": "Summary",
    "text": "Summary\nMissing data can significantly impact the integrity of your findings, leading to skewed results and misleading conclusions. Understanding how to manage missing values is essential for drawing accurate insights.\nWe identified three types of missingness to consider:\nMissing Completely At Random (MCAR): missingness is random and unrelated to observed or unobserved data.\nMissing At Random (MAR): missingness is related to observed data, but not the missing data itself.\nMissing Not At Random (MNAR): missingness is related to the unobserved data itself.\nTo explore techniques for handling missingness we created a synthetic dataset of 1,000 individuals with age, gender and salary, using authentic relationships. We introduced MAR missingness, simulating everyday scenarios.\nWe used four methods for handling missing data:\n\nComplete-case analysis\nMean imputation\nLinear regression\nMultiple imputation using the MICE algorithm\n\nThe MICE algorithm emerged as the most accurate, closely mirroring the original dataset’s characteristics. Mean imputation and linear regression provided decent average estimates but distorted variability, leading to altered distributions.\nThis example serves as a powerful reminder of the importance of choosing the right approach for missing data. So, the next time you encounter missing values, remember: how you handle them can shape the story your data tells."
  },
  {
    "objectID": "blogs/posts/2024-05-22_storing-data-safely/index.html",
    "href": "blogs/posts/2024-05-22_storing-data-safely/index.html",
    "title": "Storing data safely",
    "section": "",
    "text": "Note\n\n\n\nUPDATED: Please see the Addendum to this blog, added 2025-04-03 regarding accessing data from SharePoint"
  },
  {
    "objectID": "blogs/posts/2024-05-22_storing-data-safely/index.html#coffee-coding",
    "href": "blogs/posts/2024-05-22_storing-data-safely/index.html#coffee-coding",
    "title": "Storing data safely",
    "section": "Coffee & Coding",
    "text": "Coffee & Coding\nIn a recent Coffee & Coding session we chatted about storing data safely for use in Reproducible Analytical Pipelines (RAP), and the slides from the presentation are now available. We discussed the use of Posit Connect Pins and Azure Storage.\nIn order to avoid duplication, this blog post will not cover the pros and cons of each approach, and will instead focus on documenting the code that was used in our live demonstrations. I would recommend that you look through the slides before using the code in this blogpost and have them alongside, as they provide lots of useful context!"
  },
  {
    "objectID": "blogs/posts/2024-05-22_storing-data-safely/index.html#posit-connect-pins",
    "href": "blogs/posts/2024-05-22_storing-data-safely/index.html#posit-connect-pins",
    "title": "Storing data safely",
    "section": "Posit Connect Pins",
    "text": "Posit Connect Pins\n\n# A brief intro to using {pins} to store, version, share and protect a dataset\n# on Posit Connect. Documentation: https://pins.rstudio.com/\n\n\n# Setup -------------------------------------------------------------------\n\n\ninstall.packages(c(\"pins\", \"dplyr\")) # if not yet installed\n\nsuppressPackageStartupMessages({\n  library(pins)\n  library(dplyr) # for wrangling and the 'starwars' demo dataset\n})\n\nboard &lt;- board_connect() # will error if you haven't authenticated before\n# Error in `check_auth()`: ! auth = `auto` has failed to find a way to authenticate:\n# • `server` and `key` not provided for `auth = 'manual'`\n# • Can't find CONNECT_SERVER and CONNECT_API_KEY envvars for `auth = 'envvar'`\n# • rsconnect package not installed for `auth = 'rsconnect'`\n# Run `rlang::last_trace()` to see where the error occurred.\n\n# To authenticate\n# In RStudio: Tools &gt; Global Options &gt; Publishing &gt; Connect... &gt; Posit Connect\n# public URL of the Strategy Unit Posit Connect Server: connect.strategyunitwm.nhs.uk\n# Your browser will open to the Posit Connect web page and you're prompted to\n# for your password. Enter it and you'll be authenticated.\n\n# Once authenticated\nboard &lt;- board_connect()\n# Connecting to Posit Connect 2024.03.0 at\n# &lt;https://connect.strategyunitwm.nhs.uk&gt;\n\nboard |&gt; pin_list() # see all the pins on that board\n\n\n# Create a pin ------------------------------------------------------------\n\n\n# Write a dataset to the board as a pin\nboard |&gt; pin_write(\n  x = starwars,\n  name = \"starwars_demo\"\n)\n# Guessing `type = 'rds'`\n# Writing to pin 'matt.dray/starwars_demo'\n\nboard |&gt; pin_exists(\"starwars_demo\")\n# ! Use a fully specified name including user name: \"matt.dray/starwars_demo\",\n# not \"starwars_demo\".\n# [1] TRUE\n\npin_name &lt;- \"matt.dray/starwars_demo\"\n\nboard |&gt; pin_exists(pin_name) # logical, TRUE/FALSE\nboard |&gt; pin_meta(pin_name) # metadata, see also 'metadata' arg in pin_write()\nboard |&gt; pin_browse(pin_name) # view the pin in the browser\n\n\n# Permissions -------------------------------------------------------------\n\n\n# You can let people see and edit a pin. Log into Posit Connect and select the\n# pin under 'Content'. In the 'Settings' panel on the right-hand side, adjust\n# the 'sharing' options in the 'Access' tab.\n\n\n# Overwrite and version ---------------------------------------------------\n\n\nstarwars_droids &lt;- starwars |&gt;\n  filter(species == \"Droid\") # beep boop\n\nboard |&gt; pin_write(\n  starwars_droids,\n  pin_name,\n  type = \"rds\"\n)\n# Writing to pin 'matt.dray/starwars_demo'\n\nboard |&gt; pin_versions(pin_name) # see version history\nboard |&gt; pin_versions_prune(pin_name, n = 1) # remove history\nboard |&gt; pin_versions(pin_name)\n\n# What if you try to overwrite the data but it hasn't changed?\nboard |&gt; pin_write(\n  starwars_droids,\n  pin_name,\n  type = \"rds\"\n)\n# ! The hash of pin \"matt.dray/starwars_demo\" has not changed.\n# • Your pin will not be stored.\n\n\n# Use the pin -------------------------------------------------------------\n\n\n# You can read a pin to your local machine, or access it from a Quarto file\n# or Shiny app hosted on Connect, for example. If the output and the pin are\n# both on Connect, no authentication is required; the board is defaulted to\n# the Posit Connect instance where they're both hosted.\n\nboard |&gt;\n  pin_read(pin_name) |&gt; # like you would use e.g. read_csv\n  with(data = _, plot(mass, height)) # wow!\n\n\n# Delete pin --------------------------------------------------------------\n\n\nboard |&gt; pin_exists(pin_name) # logical, good function for error handling\nboard |&gt; pin_delete(pin_name)\nboard |&gt; pin_exists(pin_name)"
  },
  {
    "objectID": "blogs/posts/2024-05-22_storing-data-safely/index.html#azure-storage-in-r",
    "href": "blogs/posts/2024-05-22_storing-data-safely/index.html#azure-storage-in-r",
    "title": "Storing data safely",
    "section": "Azure Storage in R",
    "text": "Azure Storage in R\nYou will need an .Renviron file with the four environment variables listed below for the code to work. This .Renviron file should be ignored by git. You can share the contents of .Renviron files with other team members via Teams, email, or Sharepoint.\nBelow is a sample .Renviron file\nAZ_STORAGE_EP=https://STORAGEACCOUNT.blob.core.windows.net/\nAZ_STORAGE_CONTAINER=container-name\nAZ_TENANT_ID=long-sequence-of-numbers-and-letters\nAZ_APP_ID=another-long-sequence-of-numbers-and-letters\n\ninstall.packages(c(\"AzureAuth\", \"AzureStor\", \"arrow\")) # if not yet installed\n\n# Load all environment variables\nep_uri &lt;- Sys.getenv(\"AZ_STORAGE_EP\")\napp_id &lt;- Sys.getenv(\"AZ_APP_ID\")\ncontainer_name &lt;- Sys.getenv(\"AZ_STORAGE_CONTAINER\")\ntenant &lt;- Sys.getenv(\"AZ_TENANT_ID\")\n\n# Authenticate\ntoken &lt;- AzureAuth::get_azure_token(\n  \"https://storage.azure.com\",\n  tenant = tenant,\n  app = app_id,\n  auth_type = \"device_code\",\n)\n\n# If you have not authenticated before, you will be taken to an external page to\n# authenticate!Use your mlcsu.nhs.uk account.\n\n# Connect to container\nendpoint &lt;- AzureStor::blob_endpoint(ep_uri, token = token)\ncontainer &lt;- AzureStor::storage_container(endpoint, container_name)\n\n# List files in container\nblob_list &lt;- AzureStor::list_blobs(container)\n\n# If you get a 403 error when trying to interact with the container, you may\n# have to clear your Azure token and re-authenticate using a different browser.\n# Use AzureAuth::clean_token_directory() to clear your token, then repeat the\n# AzureAuth::get_azure_token() step above.\n\n# Upload specific file to container\nAzureStor::storage_upload(container, \"data/ronald.jpeg\", \"newdir/ronald.jpeg\")\n\n# Upload contents of a local directory to container\nAzureStor::storage_multiupload(container, \"data/*\", \"newdir\")\n\n# Check files have uploaded\nblob_list &lt;- AzureStor::list_blobs(container)\n\n# Load file directly from Azure container\ndf_from_azure &lt;- AzureStor::storage_read_csv(\n  container,\n  \"newdir/cats.csv\",\n  show_col_types = FALSE\n)\n\n# Load file directly from Azure container (by temporarily downloading file\n# and storing it in memory)\nparquet_in_memory &lt;- AzureStor::storage_download(\n  container,\n  src = \"newdir/cats.parquet\", dest = NULL\n)\nparq_df &lt;- arrow::read_parquet(parquet_in_memory)\n\n# Delete from Azure container (!!!)\nfor (blobfile in blob_list$name) {\n  AzureStor::delete_storage_file(container, blobfile)\n}"
  },
  {
    "objectID": "blogs/posts/2024-05-22_storing-data-safely/index.html#azure-storage-in-python",
    "href": "blogs/posts/2024-05-22_storing-data-safely/index.html#azure-storage-in-python",
    "title": "Storing data safely",
    "section": "Azure Storage in Python",
    "text": "Azure Storage in Python\nThis will use the same environment variables as the R version, just stored in a .env file instead.\nWe didn’t cover this in the presentation, so it’s not in the slides, but the code should be self-explanatory.\n\n\nimport os\nimport io\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom azure.identity import DefaultAzureCredential\nfrom azure.storage.blob import ContainerClient\n\n\n# Load all environment variables\nload_dotenv()\naccount_url = os.getenv('AZ_STORAGE_EP')\ncontainer_name = os.getenv('AZ_STORAGE_CONTAINER')\n\n\n# Authenticate\ndefault_credential = DefaultAzureCredential()\n\nFor the first time, you might need to authenticate via the Azure CLI\nDownload it from https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-windows?tabs=azure-cli\nInstall then run az login in your terminal. Once you have logged in with your browser try the DefaultAzureCredential() again!\n\n# Connect to container\ncontainer_client = ContainerClient(account_url, container_name, default_credential)\n\n\n# List files in container - should be empty\nblob_list = container_client.list_blob_names()\nfor blob in blob_list:\n    if blob.startswith('newdir'):\n        print(blob)\n\nnewdir/cats.parquet\nnewdir/ronald.jpeg\n\n\n\n# Upload file to container\nwith open(file='data/cats.csv', mode=\"rb\") as data:\n    blob_client = container_client.upload_blob(name='newdir/cats.csv', \n                                               data=data, \n                                               overwrite=True)\n\n\n# # Check files have uploaded - List files in container again\nblob_list = container_client.list_blobs()\nfor blob in blob_list:\n    if blob['name'].startswith('newdir'):\n        print(blob['name'])\n\nnewdir/cats.csv\nnewdir/cats.parquet\nnewdir/ronald.jpeg\n\n\n\n# Download file from Azure container to temporary filepath\n\n# Connect to blob\nblob_client = container_client.get_blob_client('newdir/cats.csv')\n\n# Write to local file from blob\ntemp_filepath = os.path.join('temp_data', 'cats.csv')\nwith open(file=temp_filepath, mode=\"wb\") as sample_blob:\n    download_stream = blob_client.download_blob()\n    sample_blob.write(download_stream.readall())\ncat_data = pd.read_csv(temp_filepath)\ncat_data.head()\n\n\n\n\n\n\n\n\nName\nPhysical_characteristics\nBehaviour\n\n\n\n\n0\nRonald\nWhite and ginger\nLazy and greedy but undoubtedly cutest and best\n\n\n1\nKaspie\nSmall calico\nSweet and very shy but adventurous\n\n\n2\nHennimore\nPale orange\nUnhinged and always in a state of panic\n\n\n3\nThug cat\nBlack and white - very large\nLocal bully\n\n\n4\nSon of Stripey\nGrey tabby\nVery vocal\n\n\n\n\n\n\n\n\n# Load directly from Azure - no local copy\n\ndownload_stream = blob_client.download_blob()\nstream_object = io.BytesIO(download_stream.readall())\ncat_data = pd.read_csv(stream_object)\ncat_data\n\n\n\n\n\n\n\n\nName\nPhysical_characteristics\nBehaviour\n\n\n\n\n0\nRonald\nWhite and ginger\nLazy and greedy but undoubtedly cutest and best\n\n\n1\nKaspie\nSmall calico\nSweet and very shy but adventurous\n\n\n2\nHennimore\nPale orange\nUnhinged and always in a state of panic\n\n\n3\nThug cat\nBlack and white - very large\nLocal bully\n\n\n4\nSon of Stripey\nGrey tabby\nVery vocal\n\n\n\n\n\n\n\n\n# !!!!!!!!! Delete from Azure container !!!!!!!!!\nblob_client = container_client.get_blob_client('newdir/cats.csv')\nblob_client.delete_blob()\n\n\nblob_list = container_client.list_blobs()\nfor blob in blob_list:\n    if blob['name'].startswith('newdir'):\n        print(blob['name'])\n\nnewdir/cats.parquet\nnewdir/ronald.jpeg"
  },
  {
    "objectID": "blogs/posts/2024-05-22_storing-data-safely/index.html#addendum-accessing-data-from-sharepoint",
    "href": "blogs/posts/2024-05-22_storing-data-safely/index.html#addendum-accessing-data-from-sharepoint",
    "title": "Storing data safely",
    "section": "ADDENDUM: Accessing data from SharePoint",
    "text": "ADDENDUM: Accessing data from SharePoint\nSharePoint is a Microsoft product, which is a content/knowledge management tool. Many teams across the NHS use SharePoint for all sorts of file types that need to be preserved or shared within and between teams, but also need to be kept secure.\nAccessing SharePoint requires user authentication, which you’ll be prompted for in the browser when you try to access SharePoint from R. Note that you must ‘follow’ a SharePoint site before you can fetch its content.\nTo access data on SharePoint, follow these steps:\n\nNavigate to the SharePoint page that has the file of interest in it, using your browser.\nClick the small star in the top right corner of the window, labelled ‘Follow’.\n\n\n\nOpen (or create) your project’s .Renviron file, used for storing environmental variables. You can do this by running usethis::edit_r_environ() from the R console.\nSave two new environment variables to the .Renviron file:\n\nSP_SITE_NAME: the name of the site (i.e. the name at the top of the browser screen)\nSP_FILE_PATH: the full file path from below the Documents/ folder to the file itself, including the file type extension.\n\nSave and close the .Renviron file then either restart your R session (CTRL + Shift + F10 on Windows machines) or run readRenviron(\".Renviron\") to make the new variables available in your session.\nUse the code below to read in an Excel file (xlsx) into R memory, or adapt it to read other file types.\n\n\n# Read in the securely saved path and site variables\nsharepoint_site &lt;- Sys.getenv(\"SP_SITE_NAME\")\ntemplate_path &lt;- Sys.getenv(\"SP_FILE_PATH\")\n\n# access the dataset and save it into a temporary file\nsite &lt;- Microsoft365R::get_sharepoint_site(sharepoint_site)\ndrv &lt;- site$get_drive()\ntmp_file &lt;- tempfile(fileext = \".xlsx\")\ndrv$download_file(template_path, dest = tmp_file)\n\ndata &lt;- readxl::read_xlsx(tmp_file)\n\n#tidy up\nunlink(tmp_file)"
  },
  {
    "objectID": "blogs/posts/2024-11-29-mapping-my-r-learning/index.html",
    "href": "blogs/posts/2024-11-29-mapping-my-r-learning/index.html",
    "title": "Mapping my R journey so far: ten things that I have done along the way",
    "section": "",
    "text": "This blog post follows up from a talk I gave last year at coffee and coding about my experiences of learning how to code using Rstudio. Here I build on that talk to share some more reflections and advice for others who are starting out on their R learning journey.\nI have tried to learn R a few times over several years, with mixed success. When I first tried learning it a few years ago, I only managed to learn some basics. The second time, I was going through a crisis of confidence about my ability, and so when I had difficulties with learning R, I thought it was more evidence to show that I couldn’t do it. I tried again, and got to the stage of making a plot with some of the data that was included with Rstudio. Soon after that I got swept up in the demands of everyday life, and gradually my work moved away from the world of quantitative data into qualitative research, and I had fewer opportunities to use R. Still, in the back of my mind I had this strange feeling of both wanting to avoid R, but also wondering what it would have been like if I had persisted with learning it.\nA couple of years later, when I started my current job, I heard about the NHS-R community, and felt encouraged to learn R again. I tried to join my colleagues who were participating in Advent of Code. But I couldn’t understand a lot of what was going on, and when I tried to participate in some of the exercises, I immediately hit some hurdles with the basics, which was discouraging.\nIt seemed important to try and change my approach, so that learning R didn’t seem so daunting. I came across the aRtsy package and was amazed by the colourful and intricate artwork that it could produce. But better still, all of the code was open-source. I experimented with the code, making very small changes to see what kind of images it would create.\nI also discovered colour palettes such as those in the wesanderson package, and tried experimenting with those along with the generative art functions. I soon found that my fear of R was quickly replaced by a geeky fascination with all of the beautiful artwork that could be created with only a few lines of code. It felt like a low-stakes situation, because the worst that could happen was that the code wouldn’t work. Suddenly, the process of coding felt less intimidating, and it had opened up a wealth of possibilities1.\nThe great thing about R is that it is free and open source. I believe this lends itself well to a culture of shared learning. When I joined the SU’s Coffee and Coding sessions and NHS-R community’s Coffee and Code, I felt like a child asking very silly questions, but to my surprise, all of the people I have met have been keen to answer my questions. I learned to recognise and value the people in those communities who would encourage me and fellow learners by making time to answer our questions and help us learn.\nThis meant learning some of the key words and phrases, and getting exposure to the language in various ways: reading learning materials, watching tutorials, and spending time with people who were using it, and writing my own code. This had an incremental effect and over time, the more information I absorbed, the more familiar I became with the terminology.\nIn my day job, I was working on a qualitative case study and wanted to illustrate my findings using geospatial and population density data in the form of a choropleth map. Unfortunately this was one of the most challenging tasks I could have chosen as an R novice, but luckily, I had kind mentors who both believed I could achieve the task and were also on hand to help me learn the skills I needed. So I set myself the goal of trying to learn how to create a choropleth map by the end of the year. This involved breaking the task down into steps, and learning skills which I could build on along the way. I celebrated my small wins, even the tiny ones, until I achieved the goals I set for myself.\nThis involved watching tutorials on YouTube, working through books (such as R for Data Science and R for non-programmers, trying out online coding courses, using search engines and forums, and asking my colleagues and mentors for advice about what resources I should look at as well as what to avoid.\nAlthough learning resources were plentiful, I faced some common barriers when trying to use them. Often tutorials were not always written in a way that I could reproduce the code or access the data they cited, or were written in very technical language, which meant that I had to go away and learn some key concepts to be able to understand them properly. Therefore an important part of the learning journey for me has been to gradually build up a vocabulary of words and concepts in Rstudio. This has enabled me to better understand what key concepts I need to learn, and to understand the content of any training materials or tutorials. I realised that chipping away at it, spending an hour here and there, several times a week, was the best approach for me specifically, with some bigger blocks of time set aside occasionally for more difficult tasks where I could just spend a couple of hours trying out different things or understanding the problem in more depth.\nWhen I became more confident with trying out some packages and functions in R, I decided to find opportunities to apply my learning to real data. I practiced using the inbuilt datasets in Rstudio, the palmerpenguins dataset, and the datasets that were referred to in the books and learning resources I was using. For creating my choropleth maps, I then used data from the UK Census as well as geographical data about local authority geographical boundaries. Applying my learning to real data was an essential step in learning some of the key data wrangling skills.\nOver time I understood that failure is part of the learning journey, and a helpful tool for the learning process itself. If I could figure out what didn’t work, that often gave me information about what had gone wrong. This was useful as it either pointed me towards what I needed to fix, or gave me the words and concepts I could look into to help me solve the problem. Sometimes the process of trying to learn different functions accidentally produced hilariously terrible results2\nAs well as providing some humour to contrast with the often frustrating process of learning to code, these failures also helped me to get unstuck. More often than not, they were a catalyst for problem-solving as they provided useful information about what specific aspect of the code had gone wrong, which would give me a clue about what I needed to look into to fix the problem.\nOne of my worries about trying to learn R was that learning new things took more time, now I was years older than the last time I tried. But I was fairly confident that there must have been other people out there who had successfully learned how to code when they were my age or older. This led to a fascinating rabbit hole of learning about people who had successfully learned to code later in their life and the hidden history of women in coding. I bookmarked these stories so that I could revisit them on the days where I was having a difficult time understanding a particular concept or getting my code to work.\nThroughout my R learning journey, I have found that coding has been a useful conduit for my creativity, and similarly, my creative projects outside of work have been a catalyst for learning some key concepts related to coding3.\nI realised this a few months ago when my friend got me a beginner’s embroidery kit, and as I followed the pattern and learned how to create the different types of embroidery stitch, I reflected that just like with the embroidery pattern I was working on, I needed to structure the coding for the map in layers. This led me to approach the process like I would for an art project4 to identify what I needed to do to adequately visualise both types of data that I wanted to include in the map.\nAs I write this, it has been over a year since I re-started my R learning journey in earnest. Early on in the journey, I remember feeling overwhelmed by the kindness and helpfulness of the community. I decided to channel these feelings into learning as best I could, so that I could then pass the learning on. I was reminded of this when I attended the most recent RPYSOC conference where I once again experienced the warm sense of collaboration and community in NHS-R and NHS.pycom. Therefore my aim for 2025 and beyond is to continue my R learning journey (and become more familiar with GitHub), so that I can give back to the wonderful communities that helped me to find my way."
  },
  {
    "objectID": "blogs/posts/2024-11-29-mapping-my-r-learning/index.html#footnotes",
    "href": "blogs/posts/2024-11-29-mapping-my-r-learning/index.html#footnotes",
    "title": "Mapping my R journey so far: ten things that I have done along the way",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf this topic is of interest, I would recommend getting involved in the Tidy Tuesday community activity and also having a look at Nicola Rennie’s data visualisations.↩︎\nThis amused me greatly, as a fan of the Terrible Maps social media pages.↩︎\nThis has also worked the other way around, with my R learning journey helping me with learning new crafts. I have recently begun learning sewing and dressmaking. I have quickly found that the learning journey is just as intimidating, meticulous and complicated as it was for learning R. I have also unintentionally chosen a very complicated project for a beginner, which has resulted in a very steep learning curve and lots of failures and mistakes along the way. Throughout the process, I have applied some of the same principles as I did for learning coding. For example, one of the key parts of my journey of learning sewing and dressmaking has been the process of embracing and learning from failure. This has been essential both in terms of knowing what not to do next time, but also to learn how to fix mistakes, ideally early on in a practice situation (e.g. when creating a mock-up). Luckily there is a large community of supportive fellow learners and patient mentors, who are keen to help with fixing mistakes and to pass on their knowledge to new learners. I’m pleased to say, with a lot of help (and many failures) along the way, I did eventually manage to produce three choropleth maps and submitted them with the report late last year.↩︎\nThroughout the journey I have realised that thinking about the problem like an artist has been very helpful, because it allows me to use a similarly iterative approach. I wanted my choropleth maps to show both the population density and the underlying terrain when superimposed. To do this, I used the colorbrewer2 tool to test out different colour palettes, and changed the opacity and terrain to identify which colours would clearly to show the population data and the terrain underneath. The tool let me test this on an example map and showed me the hexadecimal colour codes for the colours in the palettes. Once I had found some combinations that would likely work for my particular map, I then iteratively adjusted the aesthetics in my R code until I found a combination that worked for my data.  ↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The Data Science team at the Strategy Unit comprises the following team members:\n\nChris Beeley\nClaire Welsh\nFrancis Barton\nMatt Dray\nOzayr Mohammed\nRhian Davies\nTom Jemmett\nYiWen Hon\nEirini Komninou\nNatasha Stephenson\n\nThe team has a wealth of experience in deploying models and other products to the cloud for use by a wide range of users across health and care. This is particularly demonstrated in our work with the New Hospital Programme, where we built and deployed a sophisticated probabilistic demand and capacity model and supported the use of its outputs across the decision-making stages that lead to the construction of a new hospital. The data science team possesses expertise across the breadth of data science activity — for example, statistics, machine learning, natural language processing, and real-time evidence mapping. We also have significant experience in sharing our methods and code as open-source, as well as in training others to use these tools and understand foundational data science concepts and practices. Our experience in developing scalable data science solutions, and open-sourcing them for the benefit of users across health and care, enables us to contribute meaningfully at every stage of a project’s life cycle — from design through to deployment and adoption.\nCurrent and previous projects of note include:\n\nWork supporting the New Hospitals Programme, including building a model for predicting the demand and capacity requirements of hospitals in the future, and a tool for mapping the evidence on this topic.\nThe Patient Experience Qualitative Data Categorisation project\nWork supporting the wider analytical community, through events/communities such as NHS-R and HACA."
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#packages-we-are-using-today",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#packages-we-are-using-today",
    "title": "Coffee and Coding",
    "section": "Packages we are using today",
    "text": "Packages we are using today\n\nlibrary(tidyverse)\n\nlibrary(sf)\n\nlibrary(tidygeocoder)\nlibrary(PostcodesioR)\n\nlibrary(osrm)\n\nlibrary(leaflet)"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#getting-boundary-data",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#getting-boundary-data",
    "title": "Coffee and Coding",
    "section": "Getting boundary data",
    "text": "Getting boundary data\nWe can use the ONS’s Geoportal we can grab boundary data to generate maps\n\n\n\nicb_url &lt;- paste0(\n  \"https://services1.arcgis.com\",\n  \"/ESMARspQHYMw9BZ9/arcgis\",\n  \"/rest/services\",\n  \"/Integrated_Care_Boards_April_2023_EN_BGC\",\n  \"/FeatureServer/0/query\",\n  \"?outFields=*&where=1%3D1&f=geojson\"\n)\nicb_boundaries &lt;- read_sf(icb_url)\n\nicb_boundaries |&gt;\n  ggplot() +\n  geom_sf() +\n  theme_void()"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-is-the-icb_boundaries-data",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-is-the-icb_boundaries-data",
    "title": "Coffee and Coding",
    "section": "What is the icb_boundaries data?",
    "text": "What is the icb_boundaries data?\n\nicb_boundaries |&gt;\n  select(ICB23CD, ICB23NM)\n\nSimple feature collection with 42 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -6.418667 ymin: 49.86479 xmax: 1.763706 ymax: 55.81112\nGeodetic CRS:  WGS 84\n# A tibble: 42 × 3\n   ICB23CD   ICB23NM                                                    geometry\n   &lt;chr&gt;     &lt;chr&gt;                                            &lt;MULTIPOLYGON [°]&gt;\n 1 E54000008 NHS Cheshire and Merseyside Integrated C… (((-3.083264 53.2559, -3…\n 2 E54000010 NHS Staffordshire and Stoke-on-Trent Int… (((-1.950489 53.21188, -…\n 3 E54000011 NHS Shropshire, Telford and Wrekin Integ… (((-2.380794 52.99841, -…\n 4 E54000013 NHS Lincolnshire Integrated Care Board    (((0.2687853 52.81584, 0…\n 5 E54000015 NHS Leicester, Leicestershire and Rutlan… (((-0.7875237 52.97762, …\n 6 E54000018 NHS Coventry and Warwickshire Integrated… (((-1.577608 52.67858, -…\n 7 E54000019 NHS Herefordshire and Worcestershire Int… (((-2.272042 52.43972, -…\n 8 E54000022 NHS Norfolk and Waveney Integrated Care … (((1.666741 52.31366, 1.…\n 9 E54000023 NHS Suffolk and North East Essex Integra… (((0.8997023 51.7732, 0.…\n10 E54000024 NHS Bedfordshire, Luton and Milton Keyne… (((-0.4577115 52.32009, …\n# ℹ 32 more rows"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-dataframes",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-dataframes",
    "title": "Coffee and Coding",
    "section": "Working with geospatial dataframes",
    "text": "Working with geospatial dataframes\nWe can simply join sf data frames and “regular” data frames together\n\n\n\nicb_metrics &lt;- icb_boundaries |&gt;\n  st_drop_geometry() |&gt;\n  select(ICB23CD) |&gt;\n  mutate(admissions = rpois(n(), 1000000))\n\nicb_boundaries |&gt;\n  inner_join(icb_metrics, by = \"ICB23CD\") |&gt;\n  ggplot() +\n  geom_sf(aes(fill = admissions)) +\n  scale_fill_viridis_c() +\n  theme_void()"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-data-frames",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-data-frames",
    "title": "Coffee and Coding",
    "section": "Working with geospatial data frames",
    "text": "Working with geospatial data frames\nWe can manipulate sf objects like other data frames\n\n\n\nlondon_icbs &lt;- icb_boundaries |&gt;\n  filter(ICB23NM |&gt; stringr::str_detect(\"London\"))\n\nggplot() +\n  geom_sf(data = london_icbs) +\n  geom_sf(data = st_centroid(london_icbs)) +\n  theme_void()"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-data-frames-1",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#working-with-geospatial-data-frames-1",
    "title": "Coffee and Coding",
    "section": "Working with geospatial data frames",
    "text": "Working with geospatial data frames\nSummarising the data will combine the geometries.\n\nlondon_icbs |&gt;\n  summarise(area = sum(Shape__Area)) |&gt;\n  # and use geospatial functions to create calculations using the geometry\n  mutate(new_area = st_area(geometry), .before = \"geometry\")\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -0.5102803 ymin: 51.28676 xmax: 0.3340241 ymax: 51.69188\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 3\n         area    new_area                                               geometry\n*       &lt;dbl&gt;       [m^2]                                     &lt;MULTIPOLYGON [°]&gt;\n1 1573336388. 1567995610. (((-0.3314819 51.43935, -0.3306676 51.43889, -0.33118…\n\n\n Why the difference in area?\n\n We are using a simplified geometry, so calculating the area will be slightly inaccurate. The original area was calculated on the non-simplified geometries."
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#creating-our-own-geospatial-data",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#creating-our-own-geospatial-data",
    "title": "Coffee and Coding",
    "section": "Creating our own geospatial data",
    "text": "Creating our own geospatial data\n\nlocation_raw &lt;- postcode_lookup(\"B2 4BJ\")\nglimpse(location_raw)\n\nRows: 1\nColumns: 40\n$ postcode                             &lt;chr&gt; \"B2 4BJ\"\n$ quality                              &lt;int&gt; 1\n$ eastings                             &lt;int&gt; 406866\n$ northings                            &lt;int&gt; 286775\n$ country                              &lt;chr&gt; \"England\"\n$ nhs_ha                               &lt;chr&gt; \"West Midlands\"\n$ longitude                            &lt;dbl&gt; -1.90033\n$ latitude                             &lt;dbl&gt; 52.47887\n$ european_electoral_region            &lt;chr&gt; \"West Midlands\"\n$ primary_care_trust                   &lt;chr&gt; \"Heart of Birmingham Teaching\"\n$ region                               &lt;chr&gt; \"West Midlands\"\n$ lsoa                                 &lt;chr&gt; \"Birmingham 138A\"\n$ msoa                                 &lt;chr&gt; \"Birmingham 138\"\n$ incode                               &lt;chr&gt; \"4BJ\"\n$ outcode                              &lt;chr&gt; \"B2\"\n$ parliamentary_constituency           &lt;chr&gt; \"Birmingham Ladywood\"\n$ parliamentary_constituency_2024      &lt;chr&gt; \"Birmingham Ladywood\"\n$ admin_district                       &lt;chr&gt; \"Birmingham\"\n$ parish                               &lt;chr&gt; \"Birmingham, unparished area\"\n$ admin_county                         &lt;lgl&gt; NA\n$ date_of_introduction                 &lt;chr&gt; \"198001\"\n$ admin_ward                           &lt;chr&gt; \"Ladywood\"\n$ ced                                  &lt;lgl&gt; NA\n$ ccg                                  &lt;chr&gt; \"NHS Birmingham and Solihull\"\n$ nuts                                 &lt;chr&gt; \"Birmingham\"\n$ pfa                                  &lt;chr&gt; \"West Midlands\"\n$ admin_district_code                  &lt;chr&gt; \"E08000025\"\n$ admin_county_code                    &lt;chr&gt; \"E99999999\"\n$ admin_ward_code                      &lt;chr&gt; \"E05011151\"\n$ parish_code                          &lt;chr&gt; \"E43000250\"\n$ parliamentary_constituency_code      &lt;chr&gt; \"E14001096\"\n$ parliamentary_constituency_2024_code &lt;chr&gt; \"E14001096\"\n$ ccg_code                             &lt;chr&gt; \"E38000258\"\n$ ccg_id_code                          &lt;chr&gt; \"15E\"\n$ ced_code                             &lt;chr&gt; \"E99999999\"\n$ nuts_code                            &lt;chr&gt; \"TLG31\"\n$ lsoa_code                            &lt;chr&gt; \"E01033620\"\n$ msoa_code                            &lt;chr&gt; \"E02006899\"\n$ lau2_code                            &lt;chr&gt; \"E08000025\"\n$ pfa_code                             &lt;chr&gt; \"E23000014\"\n\n\n\n\n\nlocation &lt;- location_raw |&gt;\n  st_as_sf(coords = c(\"eastings\", \"northings\"), crs = 27700) |&gt;\n  select(postcode, ccg) |&gt;\n  st_transform(crs = 4326)\n\nlocation\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -1.900335 ymin: 52.47886 xmax: -1.900335 ymax: 52.47886\nGeodetic CRS:  WGS 84\n  postcode                         ccg                   geometry\n1   B2 4BJ NHS Birmingham and Solihull POINT (-1.900335 52.47886)"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#creating-a-geospatial-data-frame-for-all-nhs-trusts",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#creating-a-geospatial-data-frame-for-all-nhs-trusts",
    "title": "Coffee and Coding",
    "section": "Creating a geospatial data frame for all NHS Trusts",
    "text": "Creating a geospatial data frame for all NHS Trusts\n\n\n\n# using the NHSRtools package\n# remotes::install_github(\"NHS-R-Community/NHSRtools\")\ntrusts &lt;- ods_get_trusts() |&gt;\n  filter(status == \"Active\") |&gt;\n  select(name, org_id, post_code) |&gt;\n  geocode(postalcode = \"post_code\") |&gt;\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326)\n\n\ntrusts |&gt;\n  leaflet() |&gt;\n  addProviderTiles(\"Stamen.TonerLite\") |&gt;\n  addMarkers(popup = ~name)"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-are-the-nearest-trusts-to-our-location",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-are-the-nearest-trusts-to-our-location",
    "title": "Coffee and Coding",
    "section": "What are the nearest trusts to our location?",
    "text": "What are the nearest trusts to our location?\n\nnearest_trusts &lt;- trusts |&gt;\n  mutate(\n    distance = st_distance(geometry, location)[, 1]\n  ) |&gt;\n  arrange(distance) |&gt;\n  head(5)\n\nnearest_trusts\n\nSimple feature collection with 5 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -1.9384 ymin: 52.4533 xmax: -1.886282 ymax: 52.48764\nGeodetic CRS:  WGS 84\n# A tibble: 5 × 5\n  name                       org_id post_code             geometry distance\n  &lt;chr&gt;                      &lt;chr&gt;  &lt;chr&gt;              &lt;POINT [°]&gt;      [m]\n1 BIRMINGHAM WOMEN'S AND CH… RQ3    B4 6NH     (-1.894241 52.4849)     789.\n2 BIRMINGHAM AND SOLIHULL M… RXT    B1 3RB    (-1.917663 52.48416)    1313.\n3 BIRMINGHAM COMMUNITY HEAL… RYW    B7 4BN    (-1.886282 52.48754)    1356.\n4 SANDWELL AND WEST BIRMING… RXK    B18 7QH   (-1.930203 52.48764)    2246.\n5 UNIVERSITY HOSPITALS BIRM… RRK    B15 2GW      (-1.9384 52.4533)    3838."
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#lets-find-driving-routes-to-these-trusts",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#lets-find-driving-routes-to-these-trusts",
    "title": "Coffee and Coding",
    "section": "Let’s find driving routes to these trusts",
    "text": "Let’s find driving routes to these trusts\n\nroutes &lt;- nearest_trusts |&gt;\n  mutate(\n    route = map(geometry, ~ osrmRoute(location, st_coordinates(.x)))\n  ) |&gt;\n  st_drop_geometry() |&gt;\n  rename(straight_line_distance = distance) |&gt;\n  unnest(route) |&gt;\n  st_as_sf()\n\nroutes\n\nSimple feature collection with 5 features and 8 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -1.93846 ymin: 52.45316 xmax: -1.88527 ymax: 52.49279\nGeodetic CRS:  WGS 84\n# A tibble: 5 × 9\n  name     org_id post_code straight_line_distance src   dst   duration distance\n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;                        [m] &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 BIRMING… RQ3    B4 6NH                      789. 1     dst       5.81     3.09\n2 BIRMING… RXT    B1 3RB                     1313. 1     dst       6.87     4.14\n3 BIRMING… RYW    B7 4BN                     1356. 1     dst       7.63     4.29\n4 SANDWEL… RXK    B18 7QH                    2246. 1     dst       8.81     4.95\n5 UNIVERS… RRK    B15 2GW                    3838. 1     dst      10.6      4.85\n# ℹ 1 more variable: geometry &lt;LINESTRING [°]&gt;"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#lets-show-the-routes",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#lets-show-the-routes",
    "title": "Coffee and Coding",
    "section": "Let’s show the routes",
    "text": "Let’s show the routes\n\nleaflet(routes) |&gt;\n  addTiles() |&gt;\n  addMarkers(data = location) |&gt;\n  addPolylines(color = \"black\", weight = 3, opacity = 1) |&gt;\n  addCircleMarkers(data = nearest_trusts, radius = 4, opacity = 1, fillOpacity = 1)"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#we-can-use-osrm-to-calculate-isochrones",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#we-can-use-osrm-to-calculate-isochrones",
    "title": "Coffee and Coding",
    "section": "We can use {osrm} to calculate isochrones",
    "text": "We can use {osrm} to calculate isochrones\n\n\n\niso &lt;- osrmIsochrone(location, breaks = seq(0, 60, 15), res = 10)\n\nisochrone_ids &lt;- unique(iso$id)\n\npal &lt;- colorFactor(\n  viridisLite::viridis(length(isochrone_ids)),\n  isochrone_ids\n)\n\nleaflet(location) |&gt;\n  addProviderTiles(\"Stamen.TonerLite\") |&gt;\n  addMarkers() |&gt;\n  addPolygons(\n    data = iso,\n    fillColor = ~ pal(id),\n    color = \"#000000\",\n    weight = 1\n  )"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones",
    "title": "Coffee and Coding",
    "section": "What trusts are in the isochrones?",
    "text": "What trusts are in the isochrones?\nThe summarise() function will “union” the geometry\n\nsummarise(iso)\n\nSimple feature collection with 1 feature and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -2.913575 ymin: 51.98094 xmax: -0.8505441 ymax: 53.10806\nGeodetic CRS:  WGS 84\n                        geometry\n1 POLYGON ((-1.541014 52.9691..."
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones-1",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones-1",
    "title": "Coffee and Coding",
    "section": "What trusts are in the isochrones?",
    "text": "What trusts are in the isochrones?\nWe can use this with a geo-filter to find the trusts in the isochrone\n\n# also works\ntrusts_in_iso &lt;- trusts |&gt;\n  st_filter(\n    summarise(iso),\n    .predicate = st_within\n  )\n\ntrusts_in_iso\n\nSimple feature collection with 31 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -2.793386 ymin: 52.19205 xmax: -1.10302 ymax: 53.01015\nGeodetic CRS:  WGS 84\n# A tibble: 31 × 4\n   name                               org_id post_code             geometry\n * &lt;chr&gt;                              &lt;chr&gt;  &lt;chr&gt;              &lt;POINT [°]&gt;\n 1 BIRMINGHAM AND SOLIHULL MENTAL HE… RXT    B1 3RB    (-1.917663 52.48416)\n 2 BIRMINGHAM COMMUNITY HEALTHCARE N… RYW    B7 4BN    (-1.886282 52.48754)\n 3 BIRMINGHAM WOMEN'S AND CHILDREN'S… RQ3    B4 6NH     (-1.894241 52.4849)\n 4 BIRMINGHAM WOMEN'S NHS FOUNDATION… RLU    B15 2TG   (-1.942861 52.45325)\n 5 BURTON HOSPITALS NHS FOUNDATION T… RJF    DE13 0RB  (-1.656667 52.81774)\n 6 COVENTRY AND WARWICKSHIRE PARTNER… RYG    CV6 6NY    (-1.48692 52.45659)\n 7 DERBYSHIRE HEALTHCARE NHS FOUNDAT… RXM    DE22 3LZ  (-1.512896 52.91831)\n 8 DUDLEY INTEGRATED HEALTH AND CARE… RYK    DY5 1RU    (-2.11786 52.48176)\n 9 GEORGE ELIOT HOSPITAL NHS TRUST    RLT    CV10 7DJ   (-1.47844 52.51258)\n10 HEART OF ENGLAND NHS FOUNDATION T… RR1    B9 5ST     (-1.828759 52.4781)\n# ℹ 21 more rows"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones-2",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#what-trusts-are-in-the-isochrones-2",
    "title": "Coffee and Coding",
    "section": "What trusts are in the isochrones?",
    "text": "What trusts are in the isochrones?\n\n\n\nleaflet(trusts_in_iso) |&gt;\n  addProviderTiles(\"Stamen.TonerLite\") |&gt;\n  addMarkers() |&gt;\n  addPolygons(\n    data = iso,\n    fillColor = ~pal(id),\n    color = \"#000000\",\n    weight = 1\n  )"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#doing-the-same-but-within-a-radius",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#doing-the-same-but-within-a-radius",
    "title": "Coffee and Coding",
    "section": "Doing the same but within a radius",
    "text": "Doing the same but within a radius\n\n\n\nr &lt;- 25000\n\ntrusts_in_radius &lt;- trusts |&gt;\n  st_filter(\n    location,\n    .predicate = st_is_within_distance,\n    dist = r\n  )\n\n# transforming gives us a pretty smooth circle\nradius &lt;- location |&gt;\n  st_transform(crs = 27700) |&gt;\n  st_buffer(dist = r) |&gt;\n  st_transform(crs = 4326)\n\nleaflet(trusts_in_radius) |&gt;\n  addProviderTiles(\"Stamen.TonerLite\") |&gt;\n  addMarkers() |&gt;\n  addPolygons(\n    data = radius,\n    color = \"#000000\",\n    weight = 1\n  )"
  },
  {
    "objectID": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#further-reading",
    "href": "presentations/2023-08-24_coffee-and-coding_geospatial/index.html#further-reading",
    "title": "Coffee and Coding",
    "section": "Further reading",
    "text": "Further reading\n\nGeocomputation with R\nr-spatial\n{sf} documentation\nLeaflet documentation\nTidy Geospatial Networks in R"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#what-is-ai",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#what-is-ai",
    "title": "What is AI?",
    "section": "What is AI?",
    "text": "What is AI?\n\nThis is quite obviously a very large question\nIn a meeting of 5 data scientists there are 6 opinions\nI’m going to start overinclusive\nThen I’m going to give a quite narrow “Wot I think”\nConcepts and vocabulary to reason about (and use, procure, produce…) AI"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#start-with-the-dictionary",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#start-with-the-dictionary",
    "title": "What is AI?",
    "section": "Start with the dictionary",
    "text": "Start with the dictionary\n\nMerriam Webster: Software designed to imitate intelligent aspects of human behavio[u]r\n\n\nWikipedia: [AI] is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#modern-ai",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#modern-ai",
    "title": "What is AI?",
    "section": "Modern AI",
    "text": "Modern AI\n\nWeb search engines\nRecommendation systems\nInteracting via human speech\nAutonomous vehicles\nGenerative AI\nStrategy games (e.g. chess, go)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#early-history-of-ai",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#early-history-of-ai",
    "title": "What is AI?",
    "section": "Early history of AI",
    "text": "Early history of AI\n\n1930-1950\n\nThe Turing test\nManipulation of symbolic representations analogy for thought\nPsychology &lt;-&gt; Artifical intelligence"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#s-1960s",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#s-1960s",
    "title": "What is AI?",
    "section": "1950s & 1960s",
    "text": "1950s & 1960s\n\n1965, H. A. Simon: “machines will be capable, within twenty years, of doing any work a man can do.”\n\n\n1970, Marvin Minsky (in Life Magazine): “In from three to eight years we will have a machine with the general intelligence of an average human being.”\n\n\nWork with perceptrons (single layer neural networks)\nCompeting with symbolic representation work"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#the-first-ai-winter",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#the-first-ai-winter",
    "title": "What is AI?",
    "section": "The first AI winter",
    "text": "The first AI winter\n\nEarly successes didn’t continue\nLimited computer power- many examples were “toys”\nCombinatorial explosions- many problems could only be solved in exponential time\nRepresenting common sense reason and knowledge requires immense amounts of data"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#moravecs-paradox",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#moravecs-paradox",
    "title": "What is AI?",
    "section": "Moravec’s paradox",
    "text": "Moravec’s paradox\n\nEncoded in the large, highly evolved sensory and motor portions of the human brain is a billion years of experience about the nature of the world and how to survive in it… We are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy. Abstract thought, though, is a new trick, perhaps less than 100 thousand years old. We have not yet mastered it. It is not all that intrinsically difficult; it just seems so when we do it"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#the-ai-effect",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#the-ai-effect",
    "title": "What is AI?",
    "section": "The “AI effect”",
    "text": "The “AI effect”\n\n“The AI effect” refers to a phenomenon where either the definition of AI or the concept of intelligence is adjusted to exclude capabilities that AI systems have mastered. This often manifests as tasks that AI can now perform successfully no longer being considered part of AI, or as the notion of intelligence itself being redefined to exclude AI achievements (wikipedia)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#the-coffee-test",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#the-coffee-test",
    "title": "What is AI?",
    "section": "The coffee test",
    "text": "The coffee test\n“A machine is required to enter an average American home and figure out how to make coffee: find the coffee machine, find the coffee, add water, find a mug, and brew the coffee by pushing the proper buttons.” (source)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#a-narrower-definition-of-ai-for-us-to-use",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#a-narrower-definition-of-ai-for-us-to-use",
    "title": "What is AI?",
    "section": "A narrower definition of “AI” for us to use",
    "text": "A narrower definition of “AI” for us to use\n\nSearching the possibility space of the game of chess is not AI- because it doesn’t scale\nA robot coming to your house and making a cup of coffee is AI but it’s way more sophisticated than we need\nUseful AI systems today are basically something inbetween"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#machine-learning",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#machine-learning",
    "title": "What is AI?",
    "section": "Machine learning",
    "text": "Machine learning"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#right-so-really-what-is-ai",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#right-so-really-what-is-ai",
    "title": "What is AI?",
    "section": "Right, so, really, what is AI?",
    "text": "Right, so, really, what is AI?\n\nClassification is often called “AI”\nVarious types of classification problems:\n\nGiven a set of observations will this patient require hospitalisation?\nGiven a piece of patient feedback which of these 10 categories is it about?\nGiven an X ray is it probable that this patient has cancer?"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#no-really-tell-me-what-is-ai",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#no-really-tell-me-what-is-ai",
    "title": "What is AI?",
    "section": "No, really, tell me, what is AI?",
    "text": "No, really, tell me, what is AI?\n\nThere are two elements of a classification problem that can be “difficult”\n\nInput\nComputation\n\nI’d really like to avoid calling anything AI if one or both of those things is not complex"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#risk-score",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#risk-score",
    "title": "What is AI?",
    "section": "Risk score",
    "text": "Risk score\n\nSometimes you see any predictive algorithm as “AI”"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#deep-learning",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#deep-learning",
    "title": "What is AI?",
    "section": "Deep learning",
    "text": "Deep learning\n\nDeep learning is literally artificial intelligence\n\n\n(source)"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#deep-learning-predictions",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#deep-learning-predictions",
    "title": "What is AI?",
    "section": "Deep learning predictions",
    "text": "Deep learning predictions\n\nDeep learning models meet a reasonable standard for “AI”\nThey model complex non linear relationships without any prior knowledge, rules, or models (a little bit like a human baby, in fact)\nDeep learning models for prediction can absorb large numbers of variables and produce predictions based on highly complex relationships within the data, merely by being trained appropriately"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#classifying-patient-experience-feedback",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#classifying-patient-experience-feedback",
    "title": "What is AI?",
    "section": "Classifying patient experience feedback",
    "text": "Classifying patient experience feedback\n\nComplex input\nFor comment theme- non complex computation (logistic regression)\nFor sentiment- transfer learning\nTheme does not require intelligence- linear models of word counts suffice\nSentiment does require pretrained deep learning models of language\nWe can see intuitively that detecting sentiment is “harder” than theme"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#large-language-models",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#large-language-models",
    "title": "What is AI?",
    "section": "Large language models",
    "text": "Large language models\n\nPasses the Turing test (e.g. ChatGPT)\nCan not make you coffee\nThe fundamental training is done by “masking” words and asking the model to predict them\nChatGPT has been called “a stochastic parrot”\nDespite appearances, ChatGPT has no understanding other than a deep knowledge of the probabilistic structure of words in language"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-chris/index.html#summary",
    "href": "presentations/2024-10-10_what-is-ai-chris/index.html#summary",
    "title": "What is AI?",
    "section": "Summary",
    "text": "Summary\n\nAI is lots of things and has been called lots of things\nEven some apparently “intelligent” tasks are not really so intelligent when you know how they work\nMy own view is that a useful definition of AI includes:\n\nModelling complex nonlinear systems without an explicit model\nDeep learning predictive algorithms meet this criterion, as do LLMs"
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#tldr",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#tldr",
    "title": "GitHub as a team sport",
    "section": "tl;dr",
    "text": "tl;dr\n\n\n\nGitHub organises code\nGitHub can help organise people\nWe’re learning as we go\n\n\n\n\n\n\n‘Too long; didn’t read’.\nGitHub isn’t just a dumping ground for code and version histories.\nThere are features that can help with communication and collaboration.\nWe’ve been learning what works for us as our team continues to grow."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#the-data-science-team",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#the-data-science-team",
    "title": "GitHub as a team sport",
    "section": "The Data Science Team",
    "text": "The Data Science Team\n       \n\nExpanding to 8, all remote\nComplex New Hospital Programme (NHP)\nHow should we work together?\n\n\n\nWe’re a growing team (soon to be 8).\nWe’ve got different backgrounds and experiences.\nWe do modelling, data pipelines, apps, etc.\nWe work largely on a big, complicated project with lots of stakeholders and tasks.\nWe want to bring other teams in the SU along with us.\nAre there tools or approaches we can use to help us?"
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#the-dream",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#the-dream",
    "title": "GitHub as a team sport",
    "section": "The dream",
    "text": "The dream\n\n\n\nOrder from chaos\nGood communication\n‘Bus factor’ reduction\n\n\n\n\n\n\nWe have a big project with lots of repositories. We have lots of different tasks and goals.\nWe want to improve clarity and reduce the chance of misunderstanding and error.\nWe don’t want information locked up in one person’s brain."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#living-the-dream",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#living-the-dream",
    "title": "GitHub as a team sport",
    "section": "Living the dream",
    "text": "Living the dream\n\n\n\nThis works (for now)\nNew folks are joining\nThings can will change\n\n\n\n\n\n\nWe’ve been slowly changing how we work and the tools we use.\nOur standards will make it easier for new starters, but they should also have an influence on how we do things.\nNothing is set in stone. We’re continually thinking about what works and what doesn’t."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#github-projects",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#github-projects",
    "title": "GitHub as a team sport",
    "section": "GitHub Projects",
    "text": "GitHub Projects\n\n\n\nWe’re ‘agile’\nMany tasks/respositories\nWe want to show progress\n\n\n\n\n\n\nWe work in sprints.\nThere’s lots to keep track of: the model, a couple of apps, a documentation site, etc.\nWe want to show others how things are progressing.\nGitHub Projects helps us by arranging individual tasks from across lots of different repositories.\nWe can also add custom labelling to help us organise and track."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#section",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#section",
    "title": "GitHub as a team sport",
    "section": "",
    "text": "We can show the tasks in kanban style, or as a list or as a calendar.\nWe can filter down to show only certain labels, statuses or assigned people.\nThis is helps us find, organise and focus during sprint planning and weekly sprint catch-ups."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#division-of-labour",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#division-of-labour",
    "title": "GitHub as a team sport",
    "section": "Division of labour",
    "text": "Division of labour\n\n\n\nThe ‘scrum master’\nOwners and deputies (CODEOWNERS)\nIssue and pull-request assignees\n\n\n\n\n\n\nAt the level of the sprint, we have a scrum master that oversees the movement of tasks from the backlog and takes us through the GitHub Project in weekly sprint catch-ups.\nWithin each repository we have an owner and deputy on each repository, with the goal of keeping the it shipshape (e.g. good docs, no stale branches, PRs are reviewed).\nAnd we have people assigned to issues and PRs, which signals the tasks that people are working on.\nHaving an identifiable person in charge makes it easier to identify ownership and for others to talk to the right person."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#task-sorting",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#task-sorting",
    "title": "GitHub as a team sport",
    "section": "Task sorting",
    "text": "Task sorting\n\n\n\nMoSCoW method\nRelease-aligned milestones\nEfficient triage\n\n\n\n\n\n\nOrganising repositories at a higher level doesn’t preclude organisation at the repository level, which is foundational.\nWe typically include the labels Must, Should, Could, Won’t (MoSCoW) to filter tasks and to help assess importance.\nThe issues associated with the current sprint are added to a milestone with the upcoming version number. This makes it easier to focus, but also release the code with auto-generated notes.\nThese approaches signal intent and help the team to more efficiently decide what to do next."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#pull-requests-prs",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#pull-requests-prs",
    "title": "GitHub as a team sport",
    "section": "Pull requests (PRs)",
    "text": "Pull requests (PRs)\n\n\n\nTalk!\nUse suggestions\nThe assignee merges the PR\n\n\n\n\n\n\nRespect each others’ time. ‘Closes #10’ isn’t always enough.\nGitHub comments don’t replace talking. Discuss if unclear.\nSuggestions are efficient and respect the submitter.\nThe submitter owns the PR. They’re responsible for closing it."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#github-is-a-team-member",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#github-is-a-team-member",
    "title": "GitHub as a team sport",
    "section": "GitHub is a team member",
    "text": "GitHub is a team member\n\n\nAutomate with Actions\nIssue templates\nRepo templates\n\n\n\nMost of the team is human. GitHub itself has features that can automate away some boring things and help prevent accidents or forgetfulness.\nGitHub Actions for continuous integration. R-CMD check at least for R projects. Start with r-lib examples as a basis.\nWe’re looking towards things like templates at the issue and repo levels; again to remove drudgery."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#are-we-curling",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#are-we-curling",
    "title": "GitHub as a team sport",
    "section": "Are we curling? 🥌",
    "text": "Are we curling? 🥌\n\n\nWe:\n\nare a small team\nassume specialist roles\nwork in sync\n\n\n\n\n\n\nYou have been wondering: if this is a ‘team sport’, what sport is it?\nThis is a terrible metaphor. But think about it."
  },
  {
    "objectID": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#the-bottom-line-actually",
    "href": "presentations/2024-11-22_github-team-sport-rpysoc/index.html#the-bottom-line-actually",
    "title": "GitHub as a team sport",
    "section": "The bottom line, actually",
    "text": "The bottom line, actually\n\n\n\n\n\nCommunicate\nHelp each other\nBe kind\n\n\n\n\nThe features of GitHub should help you do the things you should already be doing.\nI am the guy falling over, the stones are tasks, my team mates are picking me up and dusting me off.\nWhat has your team been doing? What works for you?"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-is-data-science",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-is-data-science",
    "title": "Travels with R and Python",
    "section": "What is data science?",
    "text": "What is data science?\n\n“A data scientist knows more about computer science than the average statistician, and more about statistics than the average computer scientist”\n\n(Josh Wills, a former head of data engineering at Slack)"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#drew-conways-famous-venn-diagram",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#drew-conways-famous-venn-diagram",
    "title": "Travels with R and Python",
    "section": "Drew Conway’s famous Venn diagram",
    "text": "Drew Conway’s famous Venn diagram\n\nSource"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-are-the-skills-of-data-science",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-are-the-skills-of-data-science",
    "title": "Travels with R and Python",
    "section": "What are the skills of data science?",
    "text": "What are the skills of data science?\n\nAnalysis\n\nML\nStats\nData viz\n\nSoftware engineering\n\nProgramming\nSQL/ data\nDevOps\nRAP"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-are-the-skills-of-data-science-1",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-are-the-skills-of-data-science-1",
    "title": "Travels with R and Python",
    "section": "What are the skills of data science?",
    "text": "What are the skills of data science?\n\nDomain knowledge\n\nCommunication\nProblem formulation\nDashboards and reports"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#stats-and-data-viz",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#stats-and-data-viz",
    "title": "Travels with R and Python",
    "section": "Stats and data viz",
    "text": "Stats and data viz\n\nML leans a bit more towards atheoretical prediction\nStats leans a bit more towards inference (but they both do both)\nData scientists may use different visualisations\n\nInteractive web based tools\nDashboard based visualisers e.g. {stminsights}"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#software-engineering",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#software-engineering",
    "title": "Travels with R and Python",
    "section": "Software engineering",
    "text": "Software engineering\n\nProgramming\n\nNo/ low code data science?\n\nSQL/ data\n\nTend to use reproducible automated processes\n\nDevOps\n\nPlan, code, build, test, release, deploy, operate, monitor\n\nRAP\n\nI will come back to this"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#domain-knowledge",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#domain-knowledge",
    "title": "Travels with R and Python",
    "section": "Domain knowledge",
    "text": "Domain knowledge\n\nDo stuff that matters\n\nThe best minds of my generation are thinking about how to make people click ads. That sucks. Jeffrey Hammerbacher\n\nConvince other people that it matters\nThis is the hardest part of data science"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#rap",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#rap",
    "title": "Travels with R and Python",
    "section": "RAP",
    "text": "RAP\n\nData science isn’t RAP\nRAP isn’t data science\nThey are firm friends"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#reproducibility",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#reproducibility",
    "title": "Travels with R and Python",
    "section": "Reproducibility",
    "text": "Reproducibility\n\nReproducibility in science\nThe $6B spreadsheet error\nGeorge Osbourne’s austerity was based on a spreadsheet error\nFor us, reproducibility also means we can do the same analysis 50 times in one minute\n\nWhich is why I started down the road of data science"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-is-rap",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#what-is-rap",
    "title": "Travels with R and Python",
    "section": "What is RAP",
    "text": "What is RAP\n\na process in which code is used to minimise manual, undocumented steps, and a clear, properly documented process is produced in code which can reliably give the same result from the same dataset\nRAP should be:\n\n\nthe core working practice that must be supported by all platforms and teams; make this a core focus of NHS analyst training\n\n\nGoldacre review"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--baseline",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--baseline",
    "title": "Travels with R and Python",
    "section": "Levels of RAP- Baseline",
    "text": "Levels of RAP- Baseline\n\nData produced by code in an open-source language (e.g., Python, R, SQL)\nCode is version controlled\nRepository includes a README.md file that clearly details steps a user must follow to reproduce the code\nCode has been peer reviewed\nCode is published in the open and linked to & from accompanying publication (if relevant)\n\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--silver",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--silver",
    "title": "Travels with R and Python",
    "section": "Levels of RAP- Silver",
    "text": "Levels of RAP- Silver\n\nCode is well-documented…\nCode is well-organised following standard directory format\nReusable functions and/or classes are used where appropriate\nPipeline includes a testing framework\nRepository includes dependency information (e.g. requirements.txt, PipFile, environment.yml)\nData is handled and output in a Tidy data format\n\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--gold",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#levels-of-rap--gold",
    "title": "Travels with R and Python",
    "section": "Levels of RAP- Gold",
    "text": "Levels of RAP- Gold\n\nCode is fully packaged\nRepository automatically runs tests etc. via CI/CD or a different integration/deployment tool e.g. GitHub Actions\nProcess runs based on event-based triggers (e.g., new data in database) or on a schedule\nChanges to the RAP are clearly signposted. E.g. a changelog in the package, releases etc. (See gov.uk info on Semantic Versioning)\n\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#data-science-in-healthcare",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#data-science-in-healthcare",
    "title": "Travels with R and Python",
    "section": "Data science in healthcare",
    "text": "Data science in healthcare\n\nForecasting\n\nStats versus ML\n\nText mining\n\nR versus Python\n\nDemand modelling\n\nDevOps as a way of life"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#get-involved",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#get-involved",
    "title": "Travels with R and Python",
    "section": "Get involved!",
    "text": "Get involved!\n\nNHS-R community\n\nWebinars, training, conference, Slack\n\nNHS Pycom\n\nditto…\n\nMLCSU GitHub?\nBuild links with the other CSUs"
  },
  {
    "objectID": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#contact",
    "href": "presentations/2023-08-02_mlcsu-ksn-meeting/index.html#contact",
    "title": "Travels with R and Python",
    "section": "Contact",
    "text": "Contact\n\n\n\n\n strategy.unit@nhs.net\n The-Strategy-Unit\n\n\n\n\n\n chris.beeley1@nhs.net\n chrisbeeley"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#generative-ai",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#generative-ai",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Generative AI ✨",
    "text": "Generative AI ✨\n\nCreates new content\nTrained on lots of examples\nCan mimic creativity\n\n\n\nGenerative AI uses patterns from data (like text, images, or sound) to create new, similar content.\nIt’s trained on large datasets and then generates things based on what it has learned.\nCan write stories, make art, create music, or even simulate human conversations."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#modalities",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#modalities",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Modalities 🖼️",
    "text": "Modalities 🖼️\n\n\n\nImages (e.g. DALL-E)\nAudio/video (e.g. inVideo AI)\nText (e.g. ChatGPT)\n\n\n\n\n\n\nWritten content like stories, code or conversations.\nVisuals, from drawings to photorealistic images.\nMusic, voices, or even entire videos with sound and motion.\nThese slides are about that last one, text, via Large Language Models (LLMs)."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#text",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#text",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Text 🔤",
    "text": "Text 🔤\n\nSpam filters\nSentiment\nTopic detection\nWord prediction\n…\n\n\n\nBefore we get into LLMs, it’s worth recognising how many tools we use that involve some kind of text processing (perhaps AI driven).\nPattern detection, comparison to previous spam, keyword detection.\nSummarising customer reviews as positive or negative (sentiment analysis).\nAutomatically identify key themes or topics within large collections of unstructured text (e.g. topic modelling)."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#large-language-models-llms",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#large-language-models-llms",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Large Language Models (LLMs) 🦜",
    "text": "Large Language Models (LLMs) 🦜\n\nA (fancy?) parrot\nLearns from lots of text\nPredicts next word\n\n\n\n\nBreaks down text: the model processes input text by breaking it into smaller units (like words or subwords).\nFinds patterns: it learns relationships between the units.\nGenerates predictions: based on the input, it predicts the most likely next word or phrase to generate coherent responses or text."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#in-healthcare",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#in-healthcare",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "In healthcare 🏥",
    "text": "In healthcare 🏥\n\nOrganise medical documents\nDrug discovery research\nChatbots\n…\n\n\n\nAutomatically generate, organise, summarise patient notes.\nAnalyse medical literature and data to help identify new potential treatments or drug interactions.\nProvide 24/7 support by answering common medical questions and guiding patients through symptom checks."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#irl-models",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#irl-models",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "IRL models 🤖",
    "text": "IRL models 🤖\n\nHealthcare-focused models\nGoogle’s MedLM\nNature: LLMs in medicine and the future landscape\n\n\n\nSpecifically-developed LLMs for healthcare applications already exist, many are openly available.\nGoogle’s MedLM has a price tag.\nThere’s a couple of Nature articles on current applications and potential future landscape."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#case-study",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#case-study",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Case study 🧠",
    "text": "Case study 🧠\n\nNHS: chatbot for mental health referrals\nUsed Limbic ‘Access’ ‘e-triage’ chatbot\nAccording to Limbic:\n\n\n‘Nearly 40% of NHS Talking Therapies already trust Limbic to improve their services’\n\n\n\nThere’s an admin burden.\nTook place in the Surrey and Borders Partnership NHS Foundation Trust"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#case-study-1",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#case-study-1",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Case study 🧠",
    "text": "Case study 🧠\n\n\nPatient\n\n\nClinician\n\n\n\nFrom the NHS-E Transformation Directorate write-up:\n~99% of patients that left feedback said that Limbic was helpful\nthe service has seen a 30% increase in referrals and initial evidence indicates that Limbic improved out of hours access\non a pro-rata basis, a saving of 3000 hours (4 psychological wellbeing practitioners)\nnearly 20% of referrals were identified as ineligible and signposted to a more appropriate service"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#but",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#but",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "But… ⚠️",
    "text": "But… ⚠️\nThe effect of using a large language model to respond to patient messages (The Lancet)\n\n…raises the question of the extent to which LLM assistance is decision support versus LLM-based decision making\n\n\n…a minority of LLM drafts, if left unedited, could lead to severe harm or death\n\n\n\nBrigham and Women’s Hospital, USA.\nInvestigated how ‘LLM [GPT-4] assistance for electronic patient portal messaging in electronic health record systems (ie, using an LLM to draft a response for a clinician to edit) might impact subjective efficiency, clinical recommendations, and potential harms’ for cancer patients.\nFound safety errors, and in one instance the advice given to a patient could have been fatal."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#pros",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#pros",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Pros ➕",
    "text": "Pros ➕\n\nFor providers: could reduce pressure\nFor users: increases service accessibility\nCan be trained for domain specificity\n\n\n\nCould ease pressure by providing info/assistance to users anytime, making services like customer support available 24/7.\nHealthcare is very broad, but we can train on specific subsets of information to tailor the outcome. However, we do need plenty of data."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#cons",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#cons",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Cons ➖",
    "text": "Cons ➖\n\nEthical issues, like:\n\nbias\ncomputational cost\ndata origins\nprivacy\n\nNot human\nIt lies\n\n\n\nSometimes gives wrong or confusing information, especially on complex topics. May provide inaccurate medical advice or information, leading to potential misdiagnoses or harmful decisions.\n‘Hallucination’ is perhaps a weasel word.\nCan reflect biases found in the data it was trained on, leading to unfair responses that could be plain wrong for the target audience.\nLack of accountability: it can be unclear who is responsible, complicating patient care and trust issues.\nRequires significant computational power and resources, which can be expensive and environmentally taxing.\nDoesn’t understand emotions, context, or nuance, which are all important in clinical settings."
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#consider",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#consider",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Consider 🤔",
    "text": "Consider 🤔\n\nAre there legal issues?\nHave you considered user needs?\nShould you follow policies (e.g. HM gov, NHS, trust)?\n\n\n\nCould your product technically be a medical device, for example?\nHave you performed a data protection impact assessment?\nAre there more classic, better understood AI approaches you could use? NLP?\nTest the limits of the system; where is it likely to fail or give bad information?\nProvide an alternative to users in certain cases (e.g. non-chatbot given the chance of ‘conversation loops’).\nAre you thinking about the humans at the end of the process? The users of your service?"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#to-ponder",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#to-ponder",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "To ponder ❓",
    "text": "To ponder ❓\n\nIs this AI?\nAre LLMs an appropriate tool in healthcare?\nHow might you feel interacting with an LLM-driven service?\nHow can we protect patient privacy?\nHow do we deal with LLMs as tools for decision support vs decision making?\nWho is responsible for errors, or even death?"
  },
  {
    "objectID": "presentations/2024-10-10_what-is-ai-matt/index.html#further-reading",
    "href": "presentations/2024-10-10_what-is-ai-matt/index.html#further-reading",
    "title": "Large Language Models (LLMs): Is this AI?",
    "section": "Further reading 📚",
    "text": "Further reading 📚\n\nNHS Knowledge and Library Services: AI\n3 Blue 1 Brown (YouTube)\nComputerphile (YouTube)\nGOV.UK chatbot experiment"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#the-problem",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#the-problem",
    "title": "Package tour of sconn",
    "section": "The problem",
    "text": "The problem\n\n\n\nSome of us are very precious and need our familiar keyboard shortcuts and UI\nWorking in a Databricks notebook is fine but for more advanced work in R it’s easier to work in your local IDE.\nInitial attempts to connect using {sparklyr} alone didn’t work (for me)\n\n\n\n\n\nyum"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#how-does-sconn-help",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#how-does-sconn-help",
    "title": "Package tour of sconn",
    "section": "How does {sconn} help?",
    "text": "How does {sconn} help?\n\nCreates a convenience function to connect (and disconnect) from our databricks instance\nProvides documentation for new users to get set up\nProvides a place to track issues"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#brief-usage",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#brief-usage",
    "title": "Package tour of sconn",
    "section": "Brief usage",
    "text": "Brief usage\nlibrary(sconn)\nsc()\n\nsc_disconnect()"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#development-history",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#development-history",
    "title": "Package tour of sconn",
    "section": "Development history",
    "text": "Development history\n\n\n\nSimilar package used to connect to SQL Server databases\nConnection functions are lazily bound to a secret environment (and not run?)\nThe user-facing function gets the function and thus activates it\n\n\n\n\n\nenvironments, Hadley’s version"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#under-the-hood-1",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#under-the-hood-1",
    "title": "Package tour of sconn",
    "section": "Under the hood 1",
    "text": "Under the hood 1\n\n\nsc_conn &lt;- function() {\n  check_vars()\n  sparklyr::spark_connect(\n    master = Sys.getenv(\"DATABRICKS_HOST\"),\n    cluster_id = Sys.getenv(\"DATABRICKS_CLUSTER_ID\"),\n    token = Sys.getenv(\"DATABRICKS_TOKEN\"),\n    envname = Sys.getenv(\"DATABRICKS_VENV\"),\n    app_name = \"sconn_sparklyr\",\n    method = \"databricks_connect\"\n  )\n}\n\n.onLoad &lt;- function(...) {\n  .conns &lt;&lt;- rlang::new_environment()\n  rlang::env_bind_lazy(.conns, sc = sc_conn())\n}\n\n\n\n\nempty env"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#under-the-hood-2",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#under-the-hood-2",
    "title": "Package tour of sconn",
    "section": "Under the hood 2",
    "text": "Under the hood 2\nsc &lt;- function(hide_output = TRUE) {\n  if (!rlang::env_has(.conns, \"sc\")) {\n    rlang::env_bind_lazy(.conns, sc = sc_conn())\n  }\n  sc &lt;- rlang::env_get(.conns, \"sc\", default = NULL)\n  if (hide_output) invisible(sc) else sc\n}"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#lazy-binding",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#lazy-binding",
    "title": "Package tour of sconn",
    "section": "Lazy binding?",
    "text": "Lazy binding?\n\n\n\nOn load (library()/load_all()) the package should lazily bind a connection function to the .conns environment\nWhen the user calls sc(), this function is activated by rlang::env_get()\n\n\n\n\n\nMr. Lazy\n\n\n\nmrmen.fandom.com"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#remaining-questions",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#remaining-questions",
    "title": "Package tour of sconn",
    "section": "Remaining questions",
    "text": "Remaining questions\n\nDoes .onLoad work the way I expect it to?\nDoes env_get always have to activate the connection?\n(sparklyr’s spark_connection_is_open() function triggers it)\nConnection time-outs and reconnection? How best to handle?"
  },
  {
    "objectID": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#further-resources",
    "href": "presentations/2025-03-13_sconn-r-pkg-databricks/index.html#further-resources",
    "title": "Package tour of sconn",
    "section": "Further resources",
    "text": "Further resources\n\nbrickster\nsconn repo\ndatabricks docs\nunlocking databricks from R…"
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#the-new-hospital-programme-nhp-demand-model",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#the-new-hospital-programme-nhp-demand-model",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "The New Hospital Programme (NHP) Demand Model",
    "text": "The New Hospital Programme (NHP) Demand Model\n\nDiagram of model process\nA quick reminder of how the model works.\nThis is an updated image of the modelling process, showing the elements currently supported.\nThe NHP Demand model is A probabilistic Monte Carlo simulation that:\n\nTakes hospital activity from a baseline year, using NHS England’s Hospital Episode Statistics (HES) data\nApplies variables that:\n\nare outside of our control (e.g. population changes, using ONS projections),\n\nA choice of projections\nNon-demographic growth estimates Nationally, both of which will swell the baseline\n\nThen we apply elements that can reduce hospital activity (mitigators, e.g. virtual wards or teleappointments). So these components, given to us from your inputting your P10 and P90 confidence limits, can either remove some future activity, modify it or reduce length of stay.\n\nForecasts future demand based on these variables, outputting probabilistic predictive intervals.\nThe data needed include HES baseline, ONS populations, national NDG, COVID adjustement (no longer for 23/24), HSA, reference lookups.\nWe need the model to be able to preserve selections, results, version control the data and model Needs to be fast and reactive to user inputs.\nAdded difficulties are that\nHospitals are actively using the model while it is still in development, which can be tricky\nDataset is massive for each hospital - hundreds and thousands of rows - all activity for a hospital trust in one year\nModel can accommodate hundreds of different variables, and more are added over time.\nWe need the model to be backwards compatible, so that we can rerun any historic runs."
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#section",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#section",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "",
    "text": "Here’s an overview of the setup for the infrastructure.\nThis is quite an old diagram, there are more code repositories now, but the basics are the same Will explain more of some elements in a minute\n\n\nWe start with the HES data, which is processed using Databricks and stored on Azure, accessed with PySpark. The ONS data also lives here, and both are combined and wrangled by the python code in the nhp_data repo to produce the model-ready data version which lives in Azure.\nThe inputs data is pulled in to the inputs app, which lives on Posit Connect and which you should all be familiar with. This enables users to see historic trends for mitigator activity in their scheme, look at summary counts etc.\nEvery time to move a slider or make a selecion on the inputs app, those data are captured by a reactive JSON file that preserves all the metadata for your model run. This is also kept on Connect.\nWhen you click Run, the JSON is sent via the API to the model, whose code lives in the nhp_model repo and is written in python.\n\n\nThe model spins up a new Docker image on the Azure Container Registry in which to run the model as I described previously.\nThe output is the results parquet file, which is saved onto the Azure storage.\nFinally, the outputs app, also on Connect, picks up this results file and uses it to display all the charts and summaries you’re used to, as well as allowing you to download the results.\n\nAll these datasets, the inputs, results and JSON files are preserved for the future."
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#section-1",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#section-1",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "",
    "text": "Here’s a simplified, zoomed-in version.\nHES data from the database is used to make the inputs data, which is pulled in to the inputs app.\nThat sends information to the model, which creates results that are sent back to storage, and from which the ourputs app does its thing.\nBy why did we choose this setup? It seems complicated and busy."
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#principles",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#principles",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "Principles",
    "text": "Principles\n\nDeploy alongside develop\nReproducible analytical pipelines\nTransparent\nOpen (FOSS where possible)\nTeam skills and work management\n\n\n\nSome of these choices stem from original familiarity of staff with those languages, for example, or from institutional restrictions\nBut as a general rule, we will choose the best tool for the job, being open to upskilling across the board and sharing our learning.\nWe operate in a fast-paced environment where the model is in active use Nationally and we must maintain that status while developing new functionality to provide a seamless improvement arc for users\nWe operate based on the principles of RAP and have opened the code base\nWhich improves transparency for users, developers and the public, since its all paid for with public funds.\nFree and open source software is always used where we can, and we try to keep up with the latest advances\nThe happiness, satisfaction and development of our team is the cornerstone of everything\nWe foster a culture of continuous learning, sharing, openness and lack of blame."
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#tools-and-platforms",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#tools-and-platforms",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "Tools and platforms",
    "text": "Tools and platforms\n\nData pipelines: , parquet, CSV\nModel: Python , Docker \nApps: {shiny} and {golem} , Posit Connect \nInfrastructure and storage: Azure \nDocumentation: Quarto \nVersion control and collaboration: Git , GitHub \n\n\n\nOur data pipelines leverage the speed of pyspark and Databricks.\nThe model is built in Python and involves a lot of pandas DataFrame manipulations.\nWe use Azure for storage of model input data and JSON files of parameters and parquet for results.\nUsers input model paramters in one Shiny app and view results in another. This uses modules and {golem} for its package focus, as well as {bs4Dash}. We have development and production environments.\nWe have a deployed Quarto website that contains the documentation for the whole project."
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#data-for-nhp",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#data-for-nhp",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "Data for NHP",
    "text": "Data for NHP\n\n\n\nHES data stored on Azure\nDatabricks and PySpark\nCSVs and TSVs for reference data\nparameters derived and stored in JSON\n\n\n\n\n\n\nWe use a Data lake on Azure using Databricks and parquet files. Apache Parquet = columnar data file format that is super fast and efficient Free and open source, contains metadata.\nIts also just as simple to wrangle as CSVs or other more familiar filetypes so swapping from, for example Rds files to parquet, as we did when the files started getting too large, was easy.\nWe used store results data in JSON (JavaScript Object Notation) but they got too big so moved to parquet But we maintain the parameters files in JSON format, a snapshot of which you can see here. Its a nice readable dictionary-type file format that is great for handling lots of key:value pairs as we do here."
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#generating-inputs-data",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#generating-inputs-data",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "Generating inputs data",
    "text": "Generating inputs data\n\n\nThis is a diagram of the Databricks workflows used to create the inputs data (which is a subset of the model data).\n\nIf an upstream step fails, all downstream ones will. We used to handle this with {targets} so we’d only rerun the parts we needed, but now in DataBricks and using Parquet, the process is so fast this is moot. Targets didn’t play nicely with pyspark which is why we changed over."
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#model-running",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#model-running",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "Model running",
    "text": "Model running\nParameters JSON passed to model via API\n\nDocker image stored on Azure Container Registry\nRuns in Azure Container Instance\nBuilt-in paralellisation features of Python language\n\n\n\n\nDocker is for safety - need to run compute on protected data Also for environment handling. Again, Docker is FOSS."
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#interfaces",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#interfaces",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "Interfaces",
    "text": "Interfaces\n\n\nCoded in R - Shiny Open repos: nhp_inputs and nhp_outputs\nModular design\n\nEach function is in its own .R\nEach module is separate\nUIs and servers are separate\nPackaged using {golem} to use R CMD check(), devtools::document() etc\n\n\n\n\n\n\nIn theory we could take in all the user’s required model run settings via a big spreadsheet, but the number of variables needed doesn’t have to get too big before this becomes problematic. Using apps is a visually appealing way of gathering that info while forcing users to provide inputs in the formats needed by the model.\nShiny is better than any spreadsheet thanks to this input verification, and it empowers users to understand their own data and choices.\nOur apps are coded in Shiny, because its a great tool for creating production-grade apps, and we have the in-house expertise to do so.\nThe repositories that contain the code for the apps are open so you can go on our GitHub and see it all.\nWe follow good practice convention on structure, where by each element of logic is kept in a separate .R file, and UI and server code are separate. This helps with debugging, human-readability and makes extending the functionality easier.\nGolem is another R package that helps create Shiny apps as a package - meaning that we can leverage the benefits of CMD check and documentation to help QA our processes during the build."
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#section-2",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#section-2",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "",
    "text": "Taking the inputs app you’ll be familiar with as an example, here we see the landing page for the selection app. This app, stored on Connect, interacts with the Azure storage where those JSON parameters files are kept, so that it can pick up one of your previous models if you ask it to.\nOnce you’ve made those decisions, you are taken to the inputs app - which is actually a separate app hosted on Connect."
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#section-3",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#section-3",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "",
    "text": "Here is the front page of the inputs app that you will have interacted with if you managed to run a model yourself.\nWe have the metadata centrally, showing the choices you have made on the previous screen. These data are stored in the JSON file and be retrievable in future.\nOn the left you can see various modules like the baseline adjustment, and the COVID adjustment (no longer needed in the 23/24 baseline).\nAll parts of this app are coded in separate modules, two modules for each ‘part’.\nLets see how this looks in the code, stored on GitHub"
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#section-4",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#section-4",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "",
    "text": "This is the open code repository for the inputs app on GitHub within the Strategy Unit organisation. We’ve clicked into the folder called simply ‘R’ here, as that’s conventionally where all the code logic that powers the functionality is held.\nYou can see that in this nhp_inputs repo, within the R folder, we have separate R files for each function (prefix “fct”) and two per module - one server and one UI, like for the baseline adjustment module.\nThis file structure should be familiar to those of you who have built or interacted with the bones of an R package, where the R folder is where the active code is stored in a modular format.\n\n** Open GitHub\n\nBecause this app is built with {golem} the rest of the package structure is also followed, including having NAMESPACE, DESCRIPTION, LICENSE and README files, and separate folders for data, documentation and what to do on installation etc. As I said, because this app is structured like a package, and functions held in R files are documented with Roxygen skeletons etc, we can use the powerful package building tools like devtools::document() to ensure that any changes are picked up, giving a faster, smoother, cleaner build experience.\nWe also have some extras that are not mandatory, like a CODEOWNERS file, which I’ll come on to discuss later in this session.\nThe app can be run locally provided you give it the keys needed to access the data storage, which can be held in an Renviron file. Being able to run the app locally assists with debugging and development tasks."
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#hosting",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#hosting",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "Hosting",
    "text": "Hosting\nPosit Connect\n\nManage user access \nRStudio, VSCode integration ( and )\nCan handle secrets \nNo limits on views \nWe deploy a separate app per model version\n\nUnambiguous\nBackwards compatability\n\n\n\n\nThere are various options for where to host Shiny apps so that they are available to end users through the browser. shinyapps.io for example. They vary a lot. We had a number of specific requirements for the hosting of the NHP model, including\nUser access\nbeing compatible with different languages and GUIs that we use in the team\nElegantly and safely handling secrets\nNo limits on views"
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#outputs",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#outputs",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "Outputs",
    "text": "Outputs\nMany formats:\n\nCSV\nJSON\nMS Word (using {officeR})\nOutputs app\nProject information Quarto website\n\nWe use a _brand.yml to help easily maintain style format of outputs (Quarto and Shiny).\n\n\nThe main way information flows out of the NHP model is via the outputs app, which also provides the functionality to download results into a spreadsheet software of your choice through CSV files.\nThe JSON files we’ve discussed are also produced and stored and users can ask us for them if useful\nWe produce templated reports in Microsoft Word for schemes, allowing them the comfort and accessibility of familiar software without greatly sacrificing the programmatic and RAP-compliant nature of our work.\nAnd our Quarto website holding the project information is a key tool for dissemination of info. Its repo is open, so you can go and look at all the code used to create it.\nIn essence we will aim to provide outputs that best meet the needs of our model users, to maximise the impact of the modelling and communicate the uncertainty around estimates as clearly as we can.\nWe also make use of ‘brand.yml’ to be consistent in the appearance of our NHP outputs. You can see this yaml file on our github, with an informative README that contains useful links to guidance around branding and usage."
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#reproducible-analytical-pipeline-rap-principles",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#reproducible-analytical-pipeline-rap-principles",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "Reproducible Analytical Pipeline (RAP) principles",
    "text": "Reproducible Analytical Pipeline (RAP) principles\nData versioning\nData and model follow the same semantic versioning1.\n\n\n\nNow we’re moving into ever-mode nerdy territory. You’ll all be familiar with RAP, what it means and why it matters.\nWe as a team are fully on board with those principles of transparency, quality, sharing and continuous improvement.\nOne way we do this is through version control - not only of the code, but of the data too.\nWe keep a public log of all changes to the model, viewable at the link below on our project information site, which is a Quarto website whose deployment is handled by GitHub Actions.\nThe code is versioned semantically, like usual software is, following the major-minor-patch naming convention. We are currently on version 3.4 of the model\n\n\n\n\nData Log\n\n\nOn another page of the Quarto website, we have a data log. As you can see, the data are named the same as the model versions, and we have history here going right back to the begining of the model lifespan.\nRelease notes are linked to and held on the nhp_data repository, which is also open source. Lets follow one of these release notes links to see the kinds of information stored.\n\n\n\n\nThe Strategy Unit GitHub\n\n\nHere in GitHub you can see the release notes for the current version of the model, v3.4.0. These notes are partly auto-generated by GitHub through collating the pull request comments on the elements merged to create this release, but we also might overwrite or simplify some things.\nFor instance we can see that Tom merged PR number 89 that replace the AEC mitigators with the new SDEC ones. I can click on this link to go to that merged PR and see all the details, timeline and conversations around it.\nThe full changelog also contains all the commits relevant to this new model version.\n\n\nModel Updates"
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#rap",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#rap",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "RAP",
    "text": "RAP\n\nModularised code\nStyling and Linting\nOPEN source code\nVersion control\nEnvironment management:\nDocker\n{renv}\nconda\n\n\n\nThere’s endless approaches and tooling you can use to adhere to RAP principles.\nThe selections we use are not static, we consciously improve and adapt over time\nModularising code we’ve spoken about, we try to take this approach wherever it makes sense (almost everywhere)\nWe use stylers like Air, which can be set to run every time you submit a PR to ensure that you are conforming to internally consistent style rules\nSimilarly with linting, this helps us be consistent in our code syntax and semantics.\nAs you’ve seen a lot already today, we code in the open. This can be scary to start with, but if your team follow advice around keeping secrets secret etc, and feel supported, its not too difficult to do.\nVersion control we’ve talked a lot about- handled using Git.\nReproducibility requires environment management. With the model this means using Docker to spin up a fully-resourced environment every time\nWe were using {renv} for some repos, but having shifted to building these as packages we no longer need to, and instead rely on the DESCRIPTION file.\nwith everything else we use conda to manage package dependencies."
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#rap-continued",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#rap-continued",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "RAP continued",
    "text": "RAP continued\n\nDocumentation\n\nProject Information\nREADMEs on GitHub\nDocstrings, packaged apps (golem)\n\nJSON schema {✔❌} for validating the parameters passed to the model\n\n\n\nKeeping clear, concise, up to date documentation is essential.\nAll the information about NHP is on our project information website.\nWe adhere to guidelines around having comprehensive README files\nOur apps, built as packages, have documentation kept up to date, which is easily done using devtools::document() function as you would when building a package. This means that all functions whose logic has been extrapolated out to separate R files should be automatically documented by picking up the Roxygen comments.\nAnother RAP element that we’ve very recently added is JSON schema - a way of validating the content of a JSON file before it is run through the model - this was deemed a good idea as on some test runs we had mistakenly put nonsensical values in the JSON, which didn’t throw an error when run through the model, but caused silly results. Having a check against a schema prevents this."
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#deployment-cicd",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#deployment-cicd",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "Deployment (CI/CD)",
    "text": "Deployment (CI/CD)\nHow do we maintain clean, safe, working code, centrally, when we have open repositories and up to 10 people collaborating on maintaining that code alongside its active deployment?\n\n\nContinuous Integration\nAutomated checks, tests when merging code into main\n\nOn pull request submission\nOn merge to main\n\n\n\nContinuous Deployment\nAutomated checks and tests when deploying (to dev or to prod)\n\nOn merge to main\nOn release"
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#github-actions",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#github-actions",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "GitHub Actions",
    "text": "GitHub Actions\nActions like:\n\nFormatting ({Air}) to pick up stylistic inconsistencies\nLinting - for logical, syntactic and stylistic issues\nRendering the README.Rmd\nPackage checks (benefits of creating a Shiny app as a package)\nDeploys to a ‘preview’ site\nAssess code coverage ({codecov})"
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#break-time",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#break-time",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "Break time! ",
    "text": "Break time! \n\n\n\n\n\n\nChris Beeley\n\n\n\n\n\n\n\nClaire Welsh\n\n\n\n\n\n\n\nNatasha Stephenson\n\n\n\n\n\n\n\nTom Jemmett\n\n\n\n\n\n\n\n\n\nMatt Dray\n\n\n\n\n\n\n\nYiWen Hon\n\n\n\n\n\n\n\nOzayr Mohammed\n\n\n\n\n\n\n\nRhian Davies"
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#how-we-work",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#how-we-work",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "How we work",
    "text": "How we work\nWe are AGILE, and use Scrum (light)\n\n3 week sprints with 1 week fallow\nWeekly sprint catch-up meetings\nKickoff and retro\nPromote T-shape expertise while reducing ‘bus-factor’\nTransparent prioritisation processes\nDistinct team roles\n\n\n\nScrum is a software development project management approach. We pick and choose the elements of these approaches as we see fit.\nThe reason for using it is to help us coordinate our activities across all these codebases and through all this complexity. It also helps us to push back on work if needed.\nWe have recently shortened the length of our sprints to release code more frequently and prevent large, tricky pieces of work becoming blocked for too long. We’re being agile about trying this out!\nAt the start of each sprint we hold a meeting to go through all the work elements to be delivered in that sprint, decide who is doing what, and ensure that all issues are clear and complete enough to avoid ambiguity. We need to have sub-issues that accurately reflect the quantity and distribution of work coming down the line to help us prioritise.\nEvery week on Monday we catch-up around the sprint and talk about whether we are on course, what needs to happen next and ensure we are sticking to deadlines.\nAt the end of every sprint we have an open discussion about what went well or otherwise.\nThe team needs to have deep expertise, but each person’s expertise covers different areas. The ‘T-shape’ indicates a broad understanding/skillset that enables work sharing and reduces bus-factor, and the downwards element shows that depth of expertise that we call upon - things like data engineering, statistics, documentation, app building, project management.\nHow we decide what we work on when is an evolving area. We aim to have logical, transparent tools for prioritising our work plan, which is sufficiently disseminated that stakeholders feel ‘in-the-loop’ and our staff do not burn out.\nWe also use Scrum principles around nominated roles within the team, I’ll go through the main ones now."
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#section-8",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#section-8",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "",
    "text": "Scrum master : keeping the sprint on track\nProduct owner : steer work towards the goals\nProject director : overall responsibility for delivery\nDevelopment board : define the goals and priorities\nQA : oversee quality\n\n\n\n\n\n\n\nTip\n\n\nRoles have enough specificity to provide clarity, but are also shared. Flat management structure.\n\n\n\n\n\nThese roles are fundamental to being able to operate across all the different elements discussed, while continuously upskilling and making sure everyone is happy and healthy.\nBeing agile means - We welcome input at any stage and can act on it because of the attitude and setup we have - We are not thrown off course by even late-stage changes (although these should be rare if our prioritisation frameworks are working as planned) - We collaborate with anyone that it makes sense to do so, drawing on expertise that benefits the product - Continuous improvement in both the work output and the team’s skills are emphasised.\n\nLets look at some of the actual processes we use to achieve all this."
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#agile-and-scrum-in-github",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#agile-and-scrum-in-github",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "Agile and Scrum in GitHub",
    "text": "Agile and Scrum in GitHub\nLeverage a LOT of GitHub’s excellent tooling\n\n\n\nProjects\nIssues with bespoke labelling\nBranch protection rules\nCODEOWNERS\nClear and consistent collaboration guidance\nChecklists\n\n\n\n\n\nBespoke labelling is a very active area for development for us working on NHP.\nWe have a LOT of opinions, sets of priorities and dependencies to incorporate, organise and understand\n\nCreating, modifying and revising labelling strategies is key to team understanding and clarity.\nThey avoid clashing of scheduled priorities and help resolve conflicts\nWe create label sets to denote things like\n\nOur priorities within DS\nOur dev board’s priorities\nStages of production for new elements\nSign-off status\nWhether we could/should/must/won’t\nTo get an idea of the scope of any issue\nUrgency versus importance"
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#codeowners",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#codeowners",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "CODEOWNERS",
    "text": "CODEOWNERS\nA simple but powerful idea!\n\n\n\nThe more code and repos you have, the more likely you are to miss things, for issues to go stale, and for best practice to fall by the wayside.\nTo avoid this, each repo has two nominated CODEOWNERS, whose responsibility it is to know about the content, understand its purpose, be able to debug it when necessary, and to keep on top of issues related to it - prioritising these as makes sense and delegating that work if necessary.\nThese people are automatically chosen as reviewers for any PR seeking to merge to main.\nAlthough you can nominate a team to be a CODEOWNER, we’ve opted not to do this to avoid everybody getting ‘spammed’ with emails whenever a PR is submitted to one of our repos - our inboxes are full enough!"
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#product-team",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#product-team",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "Product Team",
    "text": "Product Team\n‘The model’ is a product - it has current and potential use cases and user groups.\nWe need a team responsible for understanding the software business  as well as the software product 🚀.\n“What should we build next and why?”"
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#product-team-members",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#product-team-members",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "Product Team Members",
    "text": "Product Team Members\n\nProduct Owner\nProduct Manager\nCustomer Engagement\nProject Director"
  },
  {
    "objectID": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#information-management",
    "href": "presentations/2025-05-13_NHP_DS_deep_dive/index.html#information-management",
    "title": "NHP Demand and Capacity (D&C) Model",
    "section": "Information management",
    "text": "Information management\n🗣 Info IN\nInbox, user feedback forms, button on apps, videoed interviews, roundtable, ad hoc.\n📣 Info OUT\nProject information site, GitHub, how-to videos, conference and meeting presentations, blogs.\nTo ensure that:\n\nUser needs guide what we build\nWe help (current and future) users know what we have built and are building"
  },
  {
    "objectID": "presentations/2025-07-17_deploy-to-connect/index.html#notes",
    "href": "presentations/2025-07-17_deploy-to-connect/index.html#notes",
    "title": "Deploy to Posit Connect",
    "section": "Notes",
    "text": "Notes\n\nThis is a brief overview\nLearn more in Posit Connect docs\nPress the ‘S’ key to see speaker notes"
  },
  {
    "objectID": "presentations/2025-07-17_deploy-to-connect/index.html#what-and-why",
    "href": "presentations/2025-07-17_deploy-to-connect/index.html#what-and-why",
    "title": "Deploy to Posit Connect",
    "section": "What and why?",
    "text": "What and why?\n\nHost ‘content’ on ‘someone else’s computer’\nServer address: https://connect.strategyunitwm.nhs.uk\nDocs, apps, data, APIs, notebooks\nControl access, run on schedule\nAll users see the same up-to-date content\n\n\n\nWhen you generate some ‘product’, where can you put it so others can see it?\nAt the SU we have a Posit Connect server to host materials.\nYou can put your work on the web, with admin features like access control and scheduled re-runs.\nEveryone can look at the same version and there’s no need to email files around."
  },
  {
    "objectID": "presentations/2025-07-17_deploy-to-connect/index.html#su-examples",
    "href": "presentations/2025-07-17_deploy-to-connect/index.html#su-examples",
    "title": "Deploy to Posit Connect",
    "section": "SU examples",
    "text": "SU examples\n\nNHP model project information (Quarto site)\nNHP tagged-runs table (scheduled Rmd) 🔒\nRenal evidence map (Shiny app)\nEvidence map data (Pin) 🔒\nRandomised coffee trials (API)\n\n🔒 Login required.\n\n\nHere’s some examples of the wide-ranging content already on the SU’s Posit Connect server.\nIt hosts static sites and documents, apps, pins and APIs.\nWe’re using it for important work, like the New Hospital Programme.\nYou can read more about {pins} in a previous presentation and blog."
  },
  {
    "objectID": "presentations/2025-07-17_deploy-to-connect/index.html#process-overview",
    "href": "presentations/2025-07-17_deploy-to-connect/index.html#process-overview",
    "title": "Deploy to Posit Connect",
    "section": "Process overview",
    "text": "Process overview\n\nGet a user account for our Posit Connect server.\nWrite your code (push to GitHub ideally).\nSend file ‘bundle’ to the server.\nThe server generates the content.\nAdjust content settings in the web interface (optional).\nUpdate your code and redeploy.\n\n\n\nThis is the basic high-level steps to put content onto the server.\nFirst, you need a Posit Connect account (let us know if you don’t have one).\nYou send to the server all the files needed to generate your output (e.g. to create your static HTML report, you need to include your Quarto file, R scripts and possibly data, although you can also send the rendered HTML).\nThe server will take those files and generate the output (or just display the rendered file).\nYou can then change your code and redeploy to the server."
  },
  {
    "objectID": "presentations/2025-07-17_deploy-to-connect/index.html#r-demo",
    "href": "presentations/2025-07-17_deploy-to-connect/index.html#r-demo",
    "title": "Deploy to Posit Connect",
    "section": "R demo",
    "text": "R demo\nSimplified first-time approach (preferred):\n\nAuthorise in RStudio: Tools &gt; Global options &gt; Publishing.\nAdd deploy.R with rsconnect::deploy*() call.\nRun deploy.R without appId argument.\nAdd deployed content ID as appId in deploy.R.\nUpdate code, re-run deploy.R.\n\nGo to the source repo for this demo.\n\n\nWhat’s the actual process of getting content onto the server using R?\nNote that there’s a few ways to do this, but this is our preferred suggestion within the SU.\nFirst: authorise with Connect in RStudio through settings.\nIf not in RStudio, set credentials with connect::setAccountInfo().\nOur preferred method: create a deploy.R script in your directory that contains a call to deploy*() (deployDoc(), deployApp() are most likely) that specifies information about your app. This file allows your colleagues to more easily/consistently deploy your content.\nFirst time: run the deploy function once without appId.\nThe ID will be generated and shown in the deployed app under ⚙️ Settings &gt; Info &gt; Content ID’.\nAdd the content ID as the appID into the deploy function in deploy.R."
  },
  {
    "objectID": "presentations/2025-07-17_deploy-to-connect/index.html#r-demo-1",
    "href": "presentations/2025-07-17_deploy-to-connect/index.html#r-demo-1",
    "title": "Deploy to Posit Connect",
    "section": "R demo",
    "text": "R demo\nExample deploy.R for a Shiny app:\n\nrsconnect::deployApp(\n  appName = \"coffee-test-app\",\n  appTitle = \"Test app: Coffee and Coding\",\n  appFiles = c(\"R/\", \"ui.R\", \"server.R\"),\n  server = \"connect.strategyunitwm.nhs.uk\",\n  # appId = &lt;your deployed app's content ID&gt;,\n  lint = FALSE,\n  forceUpdate = TRUE\n)\n\nFind appID in ⚙️ Settings &gt; Info &gt; Content ID after it’s deployed for the first time.\n\n\nThis is an example of the content to put in deploy.R if you’re deployng a Shiny app.\nNote appFiles to specify the minimal set of files needed for the server to render your app."
  },
  {
    "objectID": "presentations/2025-07-17_deploy-to-connect/index.html#r-demo-2",
    "href": "presentations/2025-07-17_deploy-to-connect/index.html#r-demo-2",
    "title": "Deploy to Posit Connect",
    "section": "R demo",
    "text": "R demo\nExample deploy.R for a Quarto doc:\n\nrsconnect::deployDoc(\n  appName = \"coffee-test-doc\",\n  appTitle = \"Test doc: Coffee and Coding\",\n  server = \"connect.strategyunitwm.nhs.uk\",\n  # appId = &lt;your deployed app's content ID&gt;,\n  lint = FALSE,\n  forceUpdate = TRUE\n)\n\nOr deployApp() to specify undetected appFiles.\n\n\nThis is an example of the content to put in deploy.R if you’re deployng a Shiny app.\nNote appFiles to specify the minimal set of files needed for the server to render your app.\ndeployDoc() detects the required files, so the appFiles argument isn’t needed.\nHowever, sometimes files can be missed. You can instead use deployApp() with appFiles. Alternatively, you can specify resources in the YAML: header of your Quarto file with the resource_files key."
  },
  {
    "objectID": "presentations/2025-07-17_deploy-to-connect/index.html#python-demo",
    "href": "presentations/2025-07-17_deploy-to-connect/index.html#python-demo",
    "title": "Deploy to Posit Connect",
    "section": "Python demo",
    "text": "Python demo\n\nInstall rsconnect-python package\nLogin to Connect server with rsconnect-python in your terminal with rsconnect add --server https://connect.strategyunitwm.nhs.uk --api-key YOUR-CONNECT-API-KEY --name SU\nDeploy your app, following Posit Connect documentation.\n\nYou will want a requirements.txt in the root of your folder capturing the package requirements - rsconnect gets annoyed if you don’t have one."
  },
  {
    "objectID": "presentations/2025-07-17_deploy-to-connect/index.html#python-demo-2",
    "href": "presentations/2025-07-17_deploy-to-connect/index.html#python-demo-2",
    "title": "Deploy to Posit Connect",
    "section": "Python demo (2)",
    "text": "Python demo (2)\nrsconnect deploy streamlit --name SU --entrypoint app.py .\n\nstreamlit: Type of app I am deploying - there are other options\n--name: Nickname that you gave the server when you logged in with the Connect server. You can check with rsconnect list in your terminal\n--entrypoint: Name of the file containing your app. Slightly more complicated with the APIs\n. : Files to include. I’m including all the files in my repo, otherwise the app will not run\n\nOptional arguments:\n\n--app-id: App ID on the Connect server, if you want to overwrite an existing deployment. --new if a fresh one"
  },
  {
    "objectID": "presentations/2025-07-17_deploy-to-connect/index.html#web-interface",
    "href": "presentations/2025-07-17_deploy-to-connect/index.html#web-interface",
    "title": "Deploy to Posit Connect",
    "section": "Web interface",
    "text": "Web interface\nThere’s ‘open solo’, ‘logs’ and ‘more options’ buttons.\nPanels under the ‘settings’ button are:\n\nInfo: e.g. adjust name, get content (app) ID\nAccess: protect with login, set vanity URL\nSchedule: set automated re-run schedule\nVars: set environmental variables\n\n⚠️ Share the direct URL, not the ‘admin view’ URL!"
  },
  {
    "objectID": "presentations/2025-07-17_deploy-to-connect/index.html#web-interface-1",
    "href": "presentations/2025-07-17_deploy-to-connect/index.html#web-interface-1",
    "title": "Deploy to Posit Connect",
    "section": "Web interface",
    "text": "Web interface"
  },
  {
    "objectID": "presentations/2025-07-17_deploy-to-connect/index.html#web-interface-2",
    "href": "presentations/2025-07-17_deploy-to-connect/index.html#web-interface-2",
    "title": "Deploy to Posit Connect",
    "section": "Web interface",
    "text": "Web interface\n\n\n\nOnce on Posit Connect, you can login and adjust settings through the app.\nThe vanity URL means you can put your URL in the form ‘https://connect.strategyunitwm.nhs.uk/’.\nConnect can re-render your document at specified times, which can be useful if your datasets continually update and you don’t want to keep having to manually deploy.\nIf accessing your content through Posit Connect, you’ll get some ‘admin panels’ around the content. This is so that you as a developer can adjust settings. When sharing with users, do not share the URL to this view! Instead, share the direct content URL (Access &gt; Content URL) or click the ‘open solo’ button (icon is the four corners of a square) the top-right to open without the admin panels."
  },
  {
    "objectID": "presentations/2025-07-17_deploy-to-connect/index.html#bonus-round",
    "href": "presentations/2025-07-17_deploy-to-connect/index.html#bonus-round",
    "title": "Deploy to Posit Connect",
    "section": "Bonus round",
    "text": "Bonus round\n\nDeploy via GitHub Action\nDeploy by button-press in RStudio\nOur recent server switch\nYour stories, questions, gripes\n\n\n\nYou can use a GitHub Action to deploy when releasing your code, for example. We do this for NHP Inputs app, for example, which has workflows to release to our development environment on merge, to our production environment on release, or to deploy to either location manually.\nThere is a button in the GUI of RStudio that you can click to deploy. We’re suggesting you use the deploy.R method for consistency. The push-button method relies on an rsconnect/ folder that gets autogenerated on your local machine, which is not obvious to other potential deployers of your app.\nRecently we changed from one server to another. The new server has taken the same address as the old one so that all of our URLs stay consistent from the users’ perspectives. We spent some time deploying apps to both servers, but only need to deploy now to https://connect.strategyunitwm.nhs.uk."
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#welcome-to-coffee-and-coding",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#welcome-to-coffee-and-coding",
    "title": "Coffee and coding",
    "section": "Welcome to coffee and coding",
    "text": "Welcome to coffee and coding\n\nProject demos, showcasing work from a particular project\nMethod demos, showcasing how to use a particular method/tool/package\nSurgery and problem solving sessions\nDefining code standards and SOP"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#what-are-we-trying-to-achieve",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#what-are-we-trying-to-achieve",
    "title": "Coffee and coding",
    "section": "What are we trying to achieve?",
    "text": "What are we trying to achieve?\n\nLegibility\nReproducibility\nAccuracy\nLaziness"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#what-are-some-of-the-fundamental-principles",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#what-are-some-of-the-fundamental-principles",
    "title": "Coffee and coding",
    "section": "What are some of the fundamental principles?",
    "text": "What are some of the fundamental principles?\n\nPredictability, reducing mental load, and reducing truck factor\nMaking it easy to collaborate with yourself and others on different computers, in the cloud, in six months’ time…\nDRY"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#what-is-rap",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#what-is-rap",
    "title": "Coffee and coding",
    "section": "What is RAP",
    "text": "What is RAP\n\na process in which code is used to minimise manual, undocumented steps, and a clear, properly documented process is produced in code which can reliably give the same result from the same dataset\nRAP should be:\n\n\nthe core working practice that must be supported by all platforms and teams; make this a core focus of NHS analyst training\n\nGoldacre review"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#the-road-to-rap",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#the-road-to-rap",
    "title": "Coffee and coding",
    "section": "The road to RAP",
    "text": "The road to RAP\n\nWe’re roughly using NHS Digital’s RAP stages\nThere is an incredibly large amount to learn!\nConfession time! (everything I do not know…)\nYou don’t need to do it all at once\nYou don’t need to do it all at all ever\nEach thing you learn will incrementally help you\nRemember- that’s why we learnt this stuff. Because it helped us. And it can help you too"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--baseline",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--baseline",
    "title": "Coffee and coding",
    "section": "Levels of RAP- Baseline",
    "text": "Levels of RAP- Baseline\n\nData produced by code in an open-source language (e.g., Python, R, SQL).\nCode is version controlled (see Git basics and using Git collaboratively guides).\nRepository includes a README.md file (or equivalent) that clearly details steps a user must follow to reproduce the code\nCode has been peer reviewed.\nCode is published in the open and linked to & from accompanying publication (if relevant).\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--silver",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--silver",
    "title": "Coffee and coding",
    "section": "Levels of RAP- Silver",
    "text": "Levels of RAP- Silver\n\nCode is well-documented…\nCode is well-organised following standard directory format\nReusable functions and/or classes are used where appropriate\nPipeline includes a testing framework\nRepository includes dependency information (e.g. requirements.txt, PipFile, environment.yml\nData is handled and output in a Tidy data format\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--gold",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#levels-of-rap--gold",
    "title": "Coffee and coding",
    "section": "Levels of RAP- Gold",
    "text": "Levels of RAP- Gold\n\nCode is fully packaged\nRepository automatically runs tests etc. via CI/CD or a different integration/deployment tool e.g. GitHub Actions\nProcess runs based on event-based triggers (e.g., new data in database) or on a schedule\nChanges to the RAP are clearly signposted. E.g. a changelog in the package, releases etc. (See gov.uk info on Semantic Versioning)\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#a-learning-journey-to-get-us-there",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#a-learning-journey-to-get-us-there",
    "title": "Coffee and coding",
    "section": "A learning journey to get us there",
    "text": "A learning journey to get us there\n\nCode style, organising your files\nFunctions and iteration\nGit and GitHub\nPackaging your code\nTesting\nPackage management and versioning"
  },
  {
    "objectID": "presentations/2023-02-23_coffee-and-coding/index.html#how-we-can-help-each-other-get-there",
    "href": "presentations/2023-02-23_coffee-and-coding/index.html#how-we-can-help-each-other-get-there",
    "title": "Coffee and coding",
    "section": "How we can help each other get there",
    "text": "How we can help each other get there\n\nWork as a team!\nCoffee and coding!\nAsk for help!\nDo pair coding!\nGet your code reviewed!\nJoin the NHS-R/ NHSPycom communities"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#text-vectorisation-turning-words-into-numbers",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#text-vectorisation-turning-words-into-numbers",
    "title": "Text mining",
    "section": "Text vectorisation: Turning words into numbers",
    "text": "Text vectorisation: Turning words into numbers\n\nComputers cannot do statistics with words as raw text\nThe basic foundation of all Natural Language Processing!\nHuge range in complexity"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#bag-of-words-each-word-is-represented-by-1-number",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#bag-of-words-each-word-is-represented-by-1-number",
    "title": "Text mining",
    "section": "Bag of words: Each word is represented by 1 number",
    "text": "Bag of words: Each word is represented by 1 number\n\n‘I love to run’\n‘the cat does not eat fruit’\n‘run to the cat’\n‘I love to eat fruit. fruit fruit fruit fruit’"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#word-embeddings-50-300-numbers-per-word",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#word-embeddings-50-300-numbers-per-word",
    "title": "Text mining",
    "section": "Word embeddings (50-300 numbers per word)",
    "text": "Word embeddings (50-300 numbers per word)\n1"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#attention-mechanism-768-3-numbers-per-word",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#attention-mechanism-768-3-numbers-per-word",
    "title": "Text mining",
    "section": "Attention mechanism: 768 * 3 numbers per word",
    "text": "Attention mechanism: 768 * 3 numbers per word\n\nBasis of Large Language Models!\nAttempts to capture the relationship between words\nHuge computational power required"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#text-classification",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#text-classification",
    "title": "Text mining",
    "section": "Text classification",
    "text": "Text classification\n\nSupervised learning - we need examples that have already been labelled\nSentiment analysis - whether a review is positive or negative\n\nOver to the code!"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#how-do-we-know-how-good-a-model-is",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#how-do-we-know-how-good-a-model-is",
    "title": "Text mining",
    "section": "How do we know how good a model is?",
    "text": "How do we know how good a model is?\n\n\nWe use performance metrics like:\n\nAccuracy\nPrecision\nRecall\n\n\n2"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#whats-the-accuracy-for-this-model",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#whats-the-accuracy-for-this-model",
    "title": "Text mining",
    "section": "What’s the accuracy for this model?",
    "text": "What’s the accuracy for this model?\n\n\n\n\n\nThe model’s accuracy is 91%!\nBut is there something wrong with this model?"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#different-metrics-for-different-purposes",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#different-metrics-for-different-purposes",
    "title": "Text mining",
    "section": "🐟 Different metrics for different purposes3",
    "text": "🐟 Different metrics for different purposes3\n\n\nRecall\n\nA model for cancer screening (positive = potential cancer)\nCost of false negative is higher than cost of false positive\n🥅 Fishing with a net (more fish, some rocks are ok)\n\n\nPrecision\n\nA model for identifying safe seatbelts (positive = safe)\nCost of false positive is higher than cost of false negative\n🎣 Fishing with a spear (fewer fish, but fewer rocks too)"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#topic-modelling",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#topic-modelling",
    "title": "Text mining",
    "section": "Topic Modelling",
    "text": "Topic Modelling\n\nUnsupervised learning - the model has no examples to learn from\n\nOver to the code!"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#topic-modelling-pros-and-cons",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#topic-modelling-pros-and-cons",
    "title": "Text mining",
    "section": "Topic Modelling pros and cons",
    "text": "Topic Modelling pros and cons\n\nHow do you evaluate the performance of a topic model?\nCan work well sometimes\nBlack box"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#conclusion",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#conclusion",
    "title": "Text mining",
    "section": "Conclusion",
    "text": "Conclusion\n\nOur code examples today were very basic\nText mining is not magic\nFancier models = Fancier tasks!"
  },
  {
    "objectID": "presentations/2024-11-26_text-mining-yh/index.html#footnotes",
    "href": "presentations/2024-11-26_text-mining-yh/index.html#footnotes",
    "title": "Text mining",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOpenclassrooms.com\nEvidently AI\npxtextmining"
  },
  {
    "objectID": "presentations/2025-08-29_adruk-nhp/index.html#the-new-hospital-programme-nhp-demand-model",
    "href": "presentations/2025-08-29_adruk-nhp/index.html#the-new-hospital-programme-nhp-demand-model",
    "title": "Building a model by the NHS, for the NHS",
    "section": "The New Hospital Programme (NHP) Demand Model",
    "text": "The New Hospital Programme (NHP) Demand Model\n\nAn award-winning, fully open source model for estimating future hospital activity1\nUsed for local hospital planning and to inform national policy"
  },
  {
    "objectID": "presentations/2025-08-29_adruk-nhp/index.html#the-model-in-a-nutshell",
    "href": "presentations/2025-08-29_adruk-nhp/index.html#the-model-in-a-nutshell",
    "title": "Building a model by the NHS, for the NHS",
    "section": "The model in a nutshell",
    "text": "The model in a nutshell\n\n\n\n\n\n\n\n\n\n\n\n\nProbabilistic Monte Carlo simulation model, providing hundreds of possible futures and an average forecast"
  },
  {
    "objectID": "presentations/2025-08-29_adruk-nhp/index.html#hes-a-powerful-dataset",
    "href": "presentations/2025-08-29_adruk-nhp/index.html#hes-a-powerful-dataset",
    "title": "Building a model by the NHS, for the NHS",
    "section": "HES: a powerful dataset",
    "text": "HES: a powerful dataset\n\nModel data: 500 million anonymised patient records\nSignificant data wrangling and infrastructure changes required to process data on this scale\nAll code for data processing is openly available - this ensures we’re all using the same definitions2\n\n\n\nWhat are the pros and cons of working with NHS data"
  },
  {
    "objectID": "presentations/2025-08-29_adruk-nhp/index.html#categories-of-potentially-mitigable-activity",
    "href": "presentations/2025-08-29_adruk-nhp/index.html#categories-of-potentially-mitigable-activity",
    "title": "Building a model by the NHS, for the NHS",
    "section": "92 Categories of potentially mitigable activity",
    "text": "92 Categories of potentially mitigable activity\n\n\n\nCategory\nExample\n\n\n\n\nPrevention and public health\nSmoking Related Admissions\n\n\nRedirection of activity\nA&E Frequent Attenders\n\n\nDe-adoption of activities\nInterventions with Limited Evidence\n\n\nEfficiencies\nOutpatient Convert to Tele-Attendance\n\n\n\n\n\nNote that some of these are only possible because we have a national picture of activity, e.g. with A&E frequent attenders"
  },
  {
    "objectID": "presentations/2025-08-29_adruk-nhp/index.html#creation-of-new-datasets",
    "href": "presentations/2025-08-29_adruk-nhp/index.html#creation-of-new-datasets",
    "title": "Building a model by the NHS, for the NHS",
    "section": "Creation of new datasets",
    "text": "Creation of new datasets\n\nElicitation exercise showing what activity mitigation experts think might be possible in the future3\nModel users’ inputs are a helpful insight into what providers think is possible in the future\nCounts of activity that is potentially mitigable at a local/regional/national level4\nInequalities in planned hospital procedures5"
  },
  {
    "objectID": "presentations/2025-08-29_adruk-nhp/index.html#new-datasets-in-use",
    "href": "presentations/2025-08-29_adruk-nhp/index.html#new-datasets-in-use",
    "title": "Building a model by the NHS, for the NHS",
    "section": "New datasets in use",
    "text": "New datasets in use"
  },
  {
    "objectID": "presentations/2025-08-29_adruk-nhp/index.html#future-ambitions",
    "href": "presentations/2025-08-29_adruk-nhp/index.html#future-ambitions",
    "title": "Building a model by the NHS, for the NHS",
    "section": "Future ambitions",
    "text": "Future ambitions\n\n🔓 Sharing our datasets\n🗺️ Interactive apps to help users explore and understand our datasets\n🔭 Continuing to support local hospital planning and national policy"
  },
  {
    "objectID": "presentations/2025-08-29_adruk-nhp/index.html#footnotes",
    "href": "presentations/2025-08-29_adruk-nhp/index.html#footnotes",
    "title": "Building a model by the NHS, for the NHS",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.strategyunitwm.nhs.uk/new-hospital-programme-demand-model\nhttps://github.com/The-Strategy-Unit/nhp_data\nhttps://bmjopen.bmj.com/content/14/10/e084632\nhttps://connect.strategyunitwm.nhs.uk/nhp/community_mitigators/\nhttps://www.strategyunitwm.nhs.uk/publications/strategies-reduce-inequalities-access-planned-hospital-procedures"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#prologue",
    "href": "presentations/2025-09-04_demand-presentation/index.html#prologue",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "Prologue",
    "text": "Prologue\n\nThere is much to say"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#intro",
    "href": "presentations/2025-09-04_demand-presentation/index.html#intro",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "Intro",
    "text": "Intro\n\nWhy did we build this model\nWhat does it do\nHow was it created\nWhat did we learn when we were building it\nWho else might use the model in the future\n(Sorry for the last two I saw a pattern and dug in)"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#why",
    "href": "presentations/2025-09-04_demand-presentation/index.html#why",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "Why?",
    "text": "Why?\n\nNew Hospital Programme came to the Strategy Unit c.2020\nPredict demand for the future of the hospitals c.2041\nWe built on existing work and knowledge in the SU as well as the literature\nI was not here!"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#the-landscape",
    "href": "presentations/2025-09-04_demand-presentation/index.html#the-landscape",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "The landscape",
    "text": "The landscape\n\nLots of models\nLots of consultancy support\nLots of repetition / duplication\nBUT no consistency about definitions\nMethodological progress is slow\nProprietary models means progress is not shared"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#common-issues-with-models",
    "href": "presentations/2025-09-04_demand-presentation/index.html#common-issues-with-models",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "Common issues with models",
    "text": "Common issues with models\n\nHandling uncertainty\nUnnecessary aggregation\nPoor coverage of some changes\nSystematic bias in model assumptions\nLack of ownership and auditability of assumptions"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#the-principles",
    "href": "presentations/2025-09-04_demand-presentation/index.html#the-principles",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "The principles",
    "text": "The principles\n\nProbabilistic\nTransparent and open source\nCollaborative\nReproducible\nBy the NHS, for the NHS"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#so-what-does-it-do",
    "href": "presentations/2025-09-04_demand-presentation/index.html#so-what-does-it-do",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "So what does it do?",
    "text": "So what does it do?"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#the-big-picture",
    "href": "presentations/2025-09-04_demand-presentation/index.html#the-big-picture",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "The big picture",
    "text": "The big picture\n\nDemographic change\nNon-demographic change\nCategories of potentially mitigable activity\nMore to come"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#demographic-change",
    "href": "presentations/2025-09-04_demand-presentation/index.html#demographic-change",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "Demographic change",
    "text": "Demographic change\n\nPopulation size\nPopulation age profile\nAge specific health status"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#non-demographic-change",
    "href": "presentations/2025-09-04_demand-presentation/index.html#non-demographic-change",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "Non-demographic change",
    "text": "Non-demographic change\n\nMedical interventions\nClinical standards\nPatient expectations"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#categories-of-potentially-mitigable-activity",
    "href": "presentations/2025-09-04_demand-presentation/index.html#categories-of-potentially-mitigable-activity",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "Categories of potentially mitigable activity",
    "text": "Categories of potentially mitigable activity\n\nAdmission avoidance\nLength of stay reduction\nConversion to day procedure/ tele\nNinety two and counting (will have a look later)"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#the-products",
    "href": "presentations/2025-09-04_demand-presentation/index.html#the-products",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "The products",
    "text": "The products\n\nThe model!\nParameter setting and model run interface\nInteractive results explorer\nDocumentation\nCategories of potentially mitigable activity (CPMA- no link but more on this later)\nA set of people who have successfully used the model in a business case\nThe ability to run the model at regional/ national level"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#the-model",
    "href": "presentations/2025-09-04_demand-presentation/index.html#the-model",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "The model",
    "text": "The model\n\nSample the parameters (assume normal)\nCalculate demand at IP, OP, A&E level\nDo this 256 times and plot the distribution\nThe results are conceptually at row level, but not in practice"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#the-national-elicitation-exercise",
    "href": "presentations/2025-09-04_demand-presentation/index.html#the-national-elicitation-exercise",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "The national elicitation exercise",
    "text": "The national elicitation exercise"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#its-a-whole-separate-talk",
    "href": "presentations/2025-09-04_demand-presentation/index.html#its-a-whole-separate-talk",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "It’s a whole separate talk…",
    "text": "It’s a whole separate talk…\n\nWe asked experts to predict likely levels of mitigation in the future, in a structured way\nWe also made some whizzy data science tools to do it with- which ended up being really important and useful\nIt’s not my area so I won’t say any more- this was Prof Mohammed and team\nWe show these values to trusts to help them make better guesses about the future"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#inputs",
    "href": "presentations/2025-09-04_demand-presentation/index.html#inputs",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "Inputs",
    "text": "Inputs\n\nLet’s break a rule"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#outputs",
    "href": "presentations/2025-09-04_demand-presentation/index.html#outputs",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "Outputs",
    "text": "Outputs\n\nWe’ve come this far, let’s break the rule again\nNote to self- if the vibe seems right kick off an “Analysis versus dashboards” bunfight"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#now-for-the-data-science",
    "href": "presentations/2025-09-04_demand-presentation/index.html#now-for-the-data-science",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "Now for the (data) science",
    "text": "Now for the (data) science"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#big-list-of-technical-sounding-words-coming-up",
    "href": "presentations/2025-09-04_demand-presentation/index.html#big-list-of-technical-sounding-words-coming-up",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "Big list of technical sounding words coming up…",
    "text": "Big list of technical sounding words coming up…\n\nSQL -&gt; databricks\nAzure compute (Docker)\nAzure BLOB storage\nPython for the model\nR for reporting/ dashboards\nQuarto for documentation"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#thats-what-we-did--how-did-we-do-it",
    "href": "presentations/2025-09-04_demand-presentation/index.html#thats-what-we-did--how-did-we-do-it",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "That’s what we did- how did we do it?",
    "text": "That’s what we did- how did we do it?"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#agile",
    "href": "presentations/2025-09-04_demand-presentation/index.html#agile",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "Agile!",
    "text": "Agile!\n\nWe are not doing “proper” scrum\nProduct owner, scrum master, everyone else\nFive week sprints with a one week recovery run between each one\nSprint planning, sprint catchup, sprint retro"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#sprint-retro",
    "href": "presentations/2025-09-04_demand-presentation/index.html#sprint-retro",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "Sprint retro",
    "text": "Sprint retro\n\nWhat went well, what could have gone better, and what to improve next time\nLooking at process, not blaming individuals\nRequires maturity and trust to bring up issues, and to respond to them in a constructive way\nShould agree at the end on one process improvement which goes in the next sprint\nWe’ve had some really, really good retros and I think it’s a really important process for a team"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#what-did-scrum-give-the-team",
    "href": "presentations/2025-09-04_demand-presentation/index.html#what-did-scrum-give-the-team",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "What did scrum give the team?",
    "text": "What did scrum give the team?\n\nSimultaneous releases of linked repos\nThe team works autonomously in the sprint\nBetter conversations about “no”\nThe planning and retro process improves the team’s processes, not just the code"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#product-owner",
    "href": "presentations/2025-09-04_demand-presentation/index.html#product-owner",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "Product owner",
    "text": "Product owner\n\nMy lessson- get out the way\nA better connection between high level and low level planning\nClear release dates and responsibilities\nClear what I should be doing"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#agility",
    "href": "presentations/2025-09-04_demand-presentation/index.html#agility",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "Agility",
    "text": "Agility\n\nThis project was agile whether we liked it or not!\nMy 2022 agile definition:\n\nCustomers can’t make up their minds\nIt’s hard to design software all at once\nContinuous delivery keeps customers happy"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#some-highlights-from-the-agile-manifesto",
    "href": "presentations/2025-09-04_demand-presentation/index.html#some-highlights-from-the-agile-manifesto",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "Some highlights from The agile manifesto",
    "text": "Some highlights from The agile manifesto\n\n“Welcome changing requirements, even late in development”\n“At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly”\n“Continuous attention to technical excellence and good design enhances agility”\n“Simplicity- the art of maximizing the amount of work not done- is essential”"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#how-scrum-helped",
    "href": "presentations/2025-09-04_demand-presentation/index.html#how-scrum-helped",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "How scrum helped",
    "text": "How scrum helped\n\nThose shaping the project wanted to be able to make quick changes- and see the long term plan\nAgility is a mindset, a mode of practice\nIf anything we were actually too agile\nBeing agile is all about being able to review and make decisions frequently\nBut it isn’t about changing what you’re doing all the time\nGood code and good teams are ready to change direction- whether they change or not"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#product",
    "href": "presentations/2025-09-04_demand-presentation/index.html#product",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "Product!",
    "text": "Product!"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#what-do-we-want",
    "href": "presentations/2025-09-04_demand-presentation/index.html#what-do-we-want",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "What do we want?",
    "text": "What do we want?\n\nPrioritised roadmap with appropriate stakeholder engagement!…\n(Fairly) early days\nWe need to have structured and useful conversations with our users\nWe need to imagine other users for the products we already have and for the products we might make\nWe need to think of what we do not as a set of analytical tools or products but as a service, something that help people in the real world do their real job"
  },
  {
    "objectID": "presentations/2025-09-04_demand-presentation/index.html#the-future",
    "href": "presentations/2025-09-04_demand-presentation/index.html#the-future",
    "title": "Transforming Hospital Planning with an Open-Source Demand Model",
    "section": "The future",
    "text": "The future\n\nNational and regional model runs\nBring your own data (FDP?)\nUnderstanding more about categories of potentially mitigable activity, who thinks what’s possible, and why it matters\nIncreasing understanding of the shift from hospital to the community"
  },
  {
    "objectID": "presentations/index.html",
    "href": "presentations/index.html",
    "title": "Presentations",
    "section": "",
    "text": "Title\nAuthor\nDate\n\n\n\n\nTransforming Hospital Planning with an Open-Source Demand Model: Thanks to Steven Wyatt, Jake Parsons, Tom Jemmett and others too numerous to name for materials/ inspiration\nChris Beeley\n2025-09-04\n\n\nBuilding a model by the NHS, for the NHS: Forecasting future hospital activity and the left shift of care ADRUK 2025\nYiWen Hon\n2025-08-29\n\n\nDeploy to Posit Connect: Coffee and Coding\nMatt Dray, YiWen Hon\n2025-07-17\n\n\nData science in the UK NHS: Building a model and building a community\nChris Beeley, Claire Welsh\n2025-06-06\n\n\nNHP Demand and Capacity (D&C) Model: A Technical Deep Dive for Data Scientists\nClaire Welsh\n2025-05-07\n\n\nPackage tour of sconn: An R package to connect to databricks\nFran Barton\n2025-03-13\n\n\nDeployment of a probabilistic simulation model for predicting demand for acute healthcare in the NHS\nChris Beeley\n2025-02-27\n\n\nWord is better than Quarto?: Mail merge is back, baby!\nJac Grout, Matt Dray\n2025-02-27\n\n\nA gentle introduction to databricks: What the heck is databricks?\nChris Beeley\n2025-01-16\n\n\nWhat the heck do I do all day?: Transitioning to management from a technical analytical role\nChris Beeley\n2024-12-04\n\n\nDHSC presentation: NHS-R and data science at the Strategy Unit\nChris Beeley\n2024-12-02\n\n\nText mining: What it can and can't do, and what it could do\nChris Beeley\n2024-11-27\n\n\nText mining: Some practical examples\nYiWen Hon\n2024-11-26\n\n\nForged in fire: Project management lessons from the frontline\nChris Beeley\n2024-11-22\n\n\nGitHub as a team sport: NHS RPySOC 2024\nMatt Dray\n2024-11-22\n\n\nWhat is AI?\nData science team, Strategy Unit\n2024-10-10\n\n\nLarge Language Models (LLMs): Is this AI?\nData Science team, Strategy Unit\n2024-10-10\n\n\nComputer Vision: Is this AI?\nTom Jemmett\n2024-10-10\n\n\nIdentifying patients at risk: Is this AI?\nYiWen Hon\n2024-10-10\n\n\nRAP: A cautionary tale\nNA\n2024-10-09\n\n\nUsing R and Python to model future hospital activity: EARL Conference 2024\nYiWen Hon, Matt Dray, Tom Jemmett\n2024-09-05\n\n\nAgile and scrum working\nChris Beeley\n2024-08-22\n\n\nOpen source licensing: Or: how I learned to stop worrying and love openness\nChris Beeley\n2024-05-30\n\n\nGitHub as a team sport: DfT QA Month\nMatt Dray\n2024-05-23\n\n\nStore Data Safely: Coffee & Coding\nYiWen Hon, Matt Dray\n2024-05-16\n\n\nCoffee and Coding: Making my analytical workflow more reproducible with {targets}\nJacqueline Grout\n2024-01-25\n\n\nConference Check-in App: NHS-R/NHS.pycom 2023\nTom Jemmett\n2023-10-17\n\n\nSystem Dynamics in health and care: fitting square data into round models\nSally Thompson\n2023-10-09\n\n\nRepeating Yourself with Functions: Coffee and Coding\nSally Thompson\n2023-09-07\n\n\nCoffee and Coding: Working with Geospatial Data in R\nTom Jemmett\n2023-08-24\n\n\nUnit testing in R: NHS-R Community Webinar\nTom Jemmett\n2023-08-23\n\n\nEverything you ever wanted to know about data science: but were too afraid to ask\nChris Beeley\n2023-08-02\n\n\nTravels with R and Python: the power of data science in healthcare\nChris Beeley\n2023-08-02\n\n\nAn Introduction to the New Hospital Programme Demand Model: HACA 2023\nTom Jemmett\n2023-07-11\n\n\nWhat good data science looks like\nChris Beeley\n2023-05-23\n\n\nText mining of patient experience data\nChris Beeley\n2023-05-15\n\n\nCoffee and Coding: {targets}\nTom Jemmett\n2023-03-23\n\n\nCollaborative working\nChris Beeley\n2023-03-23\n\n\nCoffee and Coding: Good Coding Practices\nTom Jemmett\n2023-03-09\n\n\nRAP: what is it and how can my team start using it effectively?\nChris Beeley\n2023-03-09\n\n\nCoffee and coding: Intro session\nChris Beeley\n2023-02-23"
  },
  {
    "objectID": "presentations/2025-02-27_word-not-quarto/index.html#backstory",
    "href": "presentations/2025-02-27_word-not-quarto/index.html#backstory",
    "title": "Word is better than Quarto?",
    "section": "Backstory 📖",
    "text": "Backstory 📖\n\nNHP outputs reports: need automation/reproducibility\nQuarto was the answer… until it wasn’t!\nSolution: meet users where they are (Microsoft Word)\nProblem: how?\n\n\n\nWe had what sounded like an obvious Reproducible Analytical Pipeline (RAP).\nIdeal solution: create a Quarto document and populate it for a named organisation with images and calculations derived from data read from Azure.\nHowever, the customer’s pre-existing Word template was complicated and was frequently being changed.\nIt turned out to be easier to use the Word template as an input and insert our figures and values into it instead."
  },
  {
    "objectID": "presentations/2025-02-27_word-not-quarto/index.html#the-word-template",
    "href": "presentations/2025-02-27_word-not-quarto/index.html#the-word-template",
    "title": "Word is better than Quarto?",
    "section": "The Word template 📄",
    "text": "The Word template 📄\n\n\n\nThis zoomed-out view of part of the Word document template highlights the complexity.\nThe document uses a pre-existing theme; contains many specifically-formatted text chunks, tables and page orientations; and uses colour-coded highlighting to indicate who needs to edit which parts of the document.\nThe document was being changed and updated quite a lot. The customers were used to amending Word documents quickly, as and when required.\nA more ‘static’ Quarto document wasn’t going to cut the mustard here."
  },
  {
    "objectID": "presentations/2025-02-27_word-not-quarto/index.html#process",
    "href": "presentations/2025-02-27_word-not-quarto/index.html#process",
    "title": "Word is better than Quarto?",
    "section": "Process ✏️",
    "text": "Process ✏️\n\nRecord a request with a GitHub issue template.\nTag model-run JSONs on Azure, update site-selections file.\nRead SharePoint template with {Microsoft365R}.\nInsert content with {officer}.\nWrite timestamped folder with doc, results, log.\nCheck output, return to recipient.\n\n\n\nHow do we identify chosen runs? On Aure, with run_stage metadata on results files and a list of chosen sites.\nHow to confirm with model relationship managers? Expose tagged scenarios and sites via a scheduled Quarto-report.\nBasic interface: supply a scheme code, at simplest level.\nUnderlying functions save figures and values independently.\nImages inserted by moving the imaginary ‘cursor’ to a known string of text in the report.\nValue-insertion is more complicated: insert ‘field’ codes into the report, insert the values to custom document properties, refresh to insert these into the fields.\nThe output is a timestamped directory with the report, standalone images and values, and a log."
  },
  {
    "objectID": "presentations/2025-02-27_word-not-quarto/index.html#generate-a-report",
    "href": "presentations/2025-02-27_word-not-quarto/index.html#generate-a-report",
    "title": "Word is better than Quarto?",
    "section": "Generate a report 🖨️",
    "text": "Generate a report 🖨️\nIn R:\npopulate_template(\n  scheme_code = \"XYZ\",                  # create a report for this scheme\n  result_sets = get_nhp_result_sets(),  # fetch results metadata from Azure\n  run_stages = list(                    # list the tagged model runs\n    primary = \"final_report_ndg2\",      # main data source for the report\n    secondary = \"final_report_ndg1\"     # used as a comparator\n  )\n)\nOutput file structure:\nYYYY-MM-DD-HHMMSS_scheme/\n ├──YYYY-MM-DD-HHMMSS_scheme.log                        # log of printed metadata\n ├──YYYY-MM-DD-HHMMSS_scheme_outputs-report_draft.docx  # populated report\n ├──figures/                                            # standalone PNG files\n └──values/                                             # CSVs of calculated values\n    \n\n\nThe interface for us is quite simple: a single function where we set the scheme code and identify the metadata ‘tag’ for the results that will form the main basis of the report and a secondary set used for comparative purposes.\nA bunch of sub-functions then read the data, calculate values, generate figures, read the Word template from SharePoint and then insert the content.\nThe output is a timestamped folder that contains the populated Word doc, a subfolder of the charts and calculated values, and a log file that contains information printed to the console (details thigns like the scheme name and site selections for the run, as well as warning and error messages if present)."
  },
  {
    "objectID": "presentations/2025-02-27_word-not-quarto/index.html#insert-content-with-officer",
    "href": "presentations/2025-02-27_word-not-quarto/index.html#insert-content-with-officer",
    "title": "Word is better than Quarto?",
    "section": "Insert content with {officer} 👮",
    "text": "Insert content with {officer} 👮\nFor calculated values:\n\nAdd doc-property fields, e.g. { DOCPROPERTY item_01 }.\nAdd values to custom doc properties.\nUpdate fields with corresponding property values.\n\nFor generated figures:\n\nAdd unique target strings, e.g. ‘[Insert Figure 8.2]’.\nFind/replace with corresponding PNG.\n\n\n\nThe {officer} package is crucial for inserting our content into the template.\nWe use different methods to insert values and images.\nFor calculated values, first add document-property fields to the Word doc (ctrl+F9); then write your calculated values to the custom document properties with officer::set_doc_properties(); then open and refresh the document (F9) to replace the fields with the corresponding value for the named property.\nTo insert images, programmatically find unique target strings with {officer}’s cursor_reach() and then overwrite with images using body_add_img(); we use and a custom insert_figure_on_cursor() to do this."
  },
  {
    "objectID": "presentations/2025-02-27_word-not-quarto/index.html#doing-it-right",
    "href": "presentations/2025-02-27_word-not-quarto/index.html#doing-it-right",
    "title": "Word is better than Quarto?",
    "section": "‘Doing it right’ 💯",
    "text": "‘Doing it right’ 💯\n\nGithub: ‘learn by doing’, challenge and improve things\nDocumentation/version history have saved us!\nThe repo is a springboard for parallel/future work"
  },
  {
    "objectID": "presentations/2025-02-27_word-not-quarto/index.html#reflections",
    "href": "presentations/2025-02-27_word-not-quarto/index.html#reflections",
    "title": "Word is better than Quarto?",
    "section": "Reflections 🪞",
    "text": "Reflections 🪞\n\nChallenge your thinking, be adaptable to user needs\nEmbrace agility and acknowledge fragility\n‘The power of friendship’ keeps things working\n\n\n\nThe default mode of ‘make a Quarto doc’ wasn’t the right fit here, but we recognised that and pivoted quickly.\nWe know the solution isn’t perfect, but it is good enough.\nThe process has some frailties, but we recognise them. For example, the report path being changed on SharePoint means we can’t retrieve it!\nTalking between us on the code side, but also with the user, is really important to handle changing needs and to keep the process and outputs fir for purpose."
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#what-is-targets",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#what-is-targets",
    "title": "Coffee and Coding",
    "section": "What is {targets}?",
    "text": "What is {targets}?\n\nThe targets package is a Make-like pipeline tool for Statistics and data science in R. With targets, you can maintain a reproducible workflow without repeating yourself. targets learns how your pipeline fits together, skips costly runtime for tasks that are already up to date, runs only the necessary computation, supports implicit parallel computing, abstracts files as R objects, and shows tangible evidence that the results match the underlying code and data.\n\n\n\nData analysis can be slow. A round of scientific computation can take several minutes, hours, or even days to complete. After it finishes, if you update your code or data, your hard-earned results may no longer be valid. Unchecked, this invalidation creates chronic Sisyphean loop:\n\n\nLaunch the code.\nWait while it runs.\nDiscover an issue.\nRestart from scratch.\n\n\nsource: The {targets} R package user manual"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#what-is-it-actually-trying-to-do",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#what-is-it-actually-trying-to-do",
    "title": "Coffee and Coding",
    "section": "What is it actually trying to do?",
    "text": "What is it actually trying to do?\n\nYour analysis is built up of a number of steps that build one on top of another\nbut these steps need to run in a particular order\nsome of these steps may take a long time to run\nso you only want to run the steps that have changed"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#typical-solution",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#typical-solution",
    "title": "Coffee and Coding",
    "section": "Typical solution",
    "text": "Typical solution\n\n\nSteps\nYou have a folder with numbered scripts, such as:\n\n1. get data.R\n2. process data.R\n3. produce charts.R\n4. build model.R\n5. report.qmd\n\n\nDownsides\n\nit’s easy to accidentally skip a step: what happens if you went from 1 to 3?\nperforming one of the steps may take a long time, so you may want to skip it if it’s already been run… but how do you know that it’s already been run?\nperhaps step 4 doesn’t depend on step 3, but is this obvious that you could skip step 4 if step 3 is updated?\nwhat if someone labels the files terribly, or doesn’t number them at all?\nwhat if the numbers become out of date and are in the wrong order?\ndo you need to create a procedure document that describes what to do, step-by-step?"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#targets-to-the-rescue",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#targets-to-the-rescue",
    "title": "Coffee and Coding",
    "section": "{targets} to the rescue?",
    "text": "{targets} to the rescue?\n\n\nUsing the previous example, if we were to create functions for each of the steps (all saved in the folder R/), we can start using targets using the function use_targets() which will create a file called _targets.R.\nWe can then modify the file to match our pipeline, for example:\nNote that:\n\nprocessed_data depends upon raw_data,\nchart and model depend upon processed_data,\nreport depends upon chart and model.\n\nWe can visualise our pipeline using tar_visnetwork().\n\n\nlibrary(targets)\n\ntar_option_set(\n  packages = c(\"tibble\", \"dplyr\", \"ggplot2\"),\n)\n\ntar_source()\n\nlist(\n  tar_target(raw_data, get_data()),\n  tar_target(processed_data, process_data(raw_data)),\n  tar_target(chart, produce_chart(processed_data)),\n  tar_target(model, build_model(processed_data)),\n  tar_target(report, generate_report(chart, model))\n)"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#running-the-pipeline",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#running-the-pipeline",
    "title": "Coffee and Coding",
    "section": "Running the pipeline",
    "text": "Running the pipeline\nRunning this pipeline is as simple as: tar_make().\nThis will output the following:\n• start target raw_data\n• built target raw_data [1.05 seconds]\n• start target processed_data\n• built target processed_data [0.03 seconds]\n• start target chart\n• built target chart [0.02 seconds]\n• start target model\n• built target model [0.01 seconds]\n• start target report\n• built target report [0 seconds]\n• end pipeline [1.75 seconds]\n\nRunning tar_make() again will show these step’s being skipped:\n✔ skip target raw_data\n✔ skip target processed_data\n✔ skip target chart\n✔ skip target model\n✔ skip target report\n✔ skip pipeline [0.12 seconds]"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#changing-one-of-the-files",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#changing-one-of-the-files",
    "title": "Coffee and Coding",
    "section": "Changing one of the files",
    "text": "Changing one of the files\nIf we change produce_chart.R slightly, this will cause chart and report to be invalidated, but it will skip over the other steps.\n\n\n&gt; targets::tar_make()\n\n✔ skip target raw_data\n✔ skip target processed_data\n• start target chart\n• built target chart [0.03 seconds]\n✔ skip target model\n• start target report\n• built target report [0 seconds]\n• end pipeline [1.71 seconds]"
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#using-the-results-of-our-pipeline",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#using-the-results-of-our-pipeline",
    "title": "Coffee and Coding",
    "section": "Using the results of our pipeline",
    "text": "Using the results of our pipeline\nWe can view the results of any step using tar_read() and tar_load(). These will either directly give you the results of a step, or load that step into your environment (as a variable with the same name as the step).\nThis allows us to view intermediate steps as well as the final outputs of our pipelines.\nOne thing you may want to consider doing is as a final step in a pipeline is to generate a quarto document, or save call a function like saveRDS to generate more useful outputs."
  },
  {
    "objectID": "presentations/2023-03-23_coffee-and-coding/index.html#current-examples-of-targets-in-action",
    "href": "presentations/2023-03-23_coffee-and-coding/index.html#current-examples-of-targets-in-action",
    "title": "Coffee and Coding",
    "section": "Current examples of {targets} in action",
    "text": "Current examples of {targets} in action\n\ncode used in this presentation\nNHP Inputs (all of the data processing steps are a targets pipeline)\nNHP Strategies (runs Sql scripts to update tables in the data warehouse)\nNHP Model (all of the data extraction, processing, and uploading for the model is a targets pipeline)\nMacmillan on NCDR - Jacqueline has been using {targets} for her current project\n\nThe {targets} documentation is exceptionally detailed and easy to follow, and goes into more complex examples (such as dynamic branching of steps in a pipeline and high performance computing setups)"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#patient-experience",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#patient-experience",
    "title": "What good data science looks like",
    "section": "Patient experience",
    "text": "Patient experience\n\nThe NHS collects a lot of patient experience data\nRate the service 1-5 (Very poor… Excellent) but also give written feedback\n\n“Parking was difficult”\n“Doctor was rude”\n“You saved my life”\n\nMany organisations lack the staffing to read all of the feedback in a systematic way\nProduce an algorithm to rate theme and “criticality”"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#help-people-to-do-their-jobs",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#help-people-to-do-their-jobs",
    "title": "What good data science looks like",
    "section": "Help people to do their jobs",
    "text": "Help people to do their jobs\n\nText based data is complex and built on human experience\nThe tool should enhance, not replace, human understanding\nEnhancing search and filtering\n\nIf they read 100 comments today, which should they read?\n\n“A recommendation engine for feedback data”"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#reflect-what-users-want",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#reflect-what-users-want",
    "title": "What good data science looks like",
    "section": "Reflect what users want",
    "text": "Reflect what users want\n\nI have worked with this data since before it existed\nI came to realise that people were struggling to read all of their data\nFits alongside other work happening within NHSE\n\nA framework for understanding patient experience"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#useful",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#useful",
    "title": "What good data science looks like",
    "section": "Useful",
    "text": "Useful\n\nA fundamental principle is that everyone can use\nIf you can run the code, run it\nIf you can use the API, use it\nIf you just want the dashboard, use it\nCredit to the growth charts API"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#understandable",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#understandable",
    "title": "What good data science looks like",
    "section": "Understandable",
    "text": "Understandable\n\nTuned to the users needs\nNot simply tuning accuracy scores\nLook at the type of mistake the model is making\nLook at the category it’s predicting\n\nWe can lose a few of common unimportant categories\nWe need to get every rare and important category"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#iterative",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#iterative",
    "title": "What good data science looks like",
    "section": "Iterative",
    "text": "Iterative\n\nYear one\n\n10 categories\nModerate criticality performance\nNo deep learning\nWeak dashboard\nPositive evaluation"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#iterative-1",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#iterative-1",
    "title": "What good data science looks like",
    "section": "Iterative",
    "text": "Iterative\n\nYear two\n\n30-50 categories\nStrong criticality performance\nDeep learning\nImproved dashboard\nWIP\n\nOverall five minor versions of algorithm and seven of dashboard"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#documented",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#documented",
    "title": "What good data science looks like",
    "section": "Documented",
    "text": "Documented\n\nWe’ve documented in the way you usually would\nWe were asked in year 1 to provide plain English documentation\nWe made a website with all the product details"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#develop-skills-of-the-staff-technical-and-otherwise",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#develop-skills-of-the-staff-technical-and-otherwise",
    "title": "What good data science looks like",
    "section": "Develop skills of the staff, technical and otherwise",
    "text": "Develop skills of the staff, technical and otherwise\n\nYear one created a Python programmer\nYear two created an R/ Shiny programmer\nThe team has learned:\n\nStatic website generation\nText cleaning/ searching/ mining\nCollaborative coding practices\nWorking with and communicating with users\nLinux, databases, APIs…"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#benefits-from-and-benefits-the-community",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#benefits-from-and-benefits-the-community",
    "title": "What good data science looks like",
    "section": "Benefits from, and benefits, the community",
    "text": "Benefits from, and benefits, the community\n\nNHSBSA R Shiny template"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#benefits-from-and-benefits-the-community-1",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#benefits-from-and-benefits-the-community-1",
    "title": "What good data science looks like",
    "section": "Benefits from, and benefits, the community",
    "text": "Benefits from, and benefits, the community\n\nWe benefit and benefit from\n\nNHS-R\nNHS-Pycom\nGovernment Digital Service\nColleagues and friends"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#open-and-reproducible",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#open-and-reproducible",
    "title": "What good data science looks like",
    "section": "Open and reproducible",
    "text": "Open and reproducible\n\nOff the shelf, proprietary data collection systems dominate\nThey often offer bundled analytic products of low quality\nThe DS time can’t and doesn’t want to offer a complete data system\nHow can we best contribute to improving patient experience for patients in the NHS?\n\nIf the patient experience data won’t come to the mountain…"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#open-source-ftw",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#open-source-ftw",
    "title": "What good data science looks like",
    "section": "Open source FTW!",
    "text": "Open source FTW!\n\nOften individuals in the NHS don’t want private companies to “benefit” from open code\nBut if they make their products better with open code the patients win\nBest practice as code"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#fun",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#fun",
    "title": "What good data science looks like",
    "section": "Fun!",
    "text": "Fun!\n\nCombing through spreadsheets looking for one comment is not fun\nDoing things the same way you did them last year is not fun\nTrying to implement a project that is too complicated is not fun\n\n \n\nWorking with a diverse team with different skills is fun\nAccessing high quality documentation to understand a project better is fun*"
  },
  {
    "objectID": "presentations/2023-05-23_data-science-for-good/index.html#team-and-code",
    "href": "presentations/2023-05-23_data-science-for-good/index.html#team-and-code",
    "title": "What good data science looks like",
    "section": "Team and code",
    "text": "Team and code\n\nAndreas Soteriades (Y1)\nYiWen Hon, Oluwasegun Apejoye (Y2)\n\n \n\npxtextmining\nexperiencesdashboard\nDocumentation\n\n\n\nchris.beeley1@nhs.net\nhttps://fosstodon.org/@chrisbeeley"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#the-history-of-nhs-r",
    "href": "presentations/2024-12-02_dhsc/index.html#the-history-of-nhs-r",
    "title": "DHSC presentation",
    "section": "The history of NHS-R",
    "text": "The history of NHS-R\n\nFounded in 2018\nFirst conference was pretty meagre but very exciting\nNHS-R has gone from strength to strength, to…"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#goldacre",
    "href": "presentations/2024-12-02_dhsc/index.html#goldacre",
    "title": "DHSC presentation",
    "section": "Goldacre",
    "text": "Goldacre\n\nCreate and maintain a curated national open library of NHS analyst code\nEnsure all training is open by default\nSupport an NHS analyst community\nOversee funding and delivery of training, both open online and one-to-one"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#everything-open-all-the-time",
    "href": "presentations/2024-12-02_dhsc/index.html#everything-open-all-the-time",
    "title": "DHSC presentation",
    "section": "Everything open, all the time",
    "text": "Everything open, all the time\n\nEverything NHS-R does is open\n\n(except Slack 🙁)\n\nEverything NHS-R produces has an open licence\nWe teach the skills of open\n\nGit, GitHub, how to write reusable code\n\n(our other value- we love beginners)"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#what-does-nhs-r-do-already",
    "href": "presentations/2024-12-02_dhsc/index.html#what-does-nhs-r-do-already",
    "title": "DHSC presentation",
    "section": "What does NHS-R do already?",
    "text": "What does NHS-R do already?\n\nTraining\nNHS-R academy\nNHS-R solutions\nConference\nWebinars\nSlack\nBlogs\nGitHub and collaboration"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#how-can-nhs-r-help-you",
    "href": "presentations/2024-12-02_dhsc/index.html#how-can-nhs-r-help-you",
    "title": "DHSC presentation",
    "section": "How can NHS-R help you?",
    "text": "How can NHS-R help you?\n\nTraining, and open training resources\nOnline, (often instant) help\nJoin the academy\nCommunity and belonging\nHear about (and copy 😉) best practice\nPermission and visibility"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#a-couple-of-examples-of-nhs-r-at-its-best",
    "href": "presentations/2024-12-02_dhsc/index.html#a-couple-of-examples-of-nhs-r-at-its-best",
    "title": "DHSC presentation",
    "section": "A couple of examples of NHS-R at its best",
    "text": "A couple of examples of NHS-R at its best\n\nNHSRplotthedots- in production across the NHS\nNHSRwaitinglists\nIntro to R and intermediate R\nAdvent of code is a yearly event on the Slack- happening now 🎄"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#ask-not-what-nhs-r-can-do-for-you",
    "href": "presentations/2024-12-02_dhsc/index.html#ask-not-what-nhs-r-can-do-for-you",
    "title": "DHSC presentation",
    "section": "Ask not what NHS-R can do for you…",
    "text": "Ask not what NHS-R can do for you…\n\nWe need you!\n\nTrain analysts (intro, reporting, Shiny…)\nWork on NHS-R solutions (like NHSRplotthedots)\nHost and run webinars\nContribute to shared policy and practice documents\nGive help to others\nBlog"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#conclusion",
    "href": "presentations/2024-12-02_dhsc/index.html#conclusion",
    "title": "DHSC presentation",
    "section": "Conclusion",
    "text": "Conclusion\n\nIf you want to go fast, go alone\nIf you want to go far, go together\n\nAfrican Proverb"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#the-strategy-unit",
    "href": "presentations/2024-12-02_dhsc/index.html#the-strategy-unit",
    "title": "DHSC presentation",
    "section": "The Strategy Unit",
    "text": "The Strategy Unit\n\n“Leading research, analysis and change from within the NHS”\n70+ person unit, data science being the newest addition\nSpecialist qualitative and quantitative analysis\nThe data science team works closely with the other teams"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#meet-the-team",
    "href": "presentations/2024-12-02_dhsc/index.html#meet-the-team",
    "title": "DHSC presentation",
    "section": "Meet the team",
    "text": "Meet the team\n       \n\nWe have:\n\nPsychology, Maths/ computing, Librarian-ing, Statistics, Entomology, Shiny, Horses, and Maternity data"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#projects",
    "href": "presentations/2024-12-02_dhsc/index.html#projects",
    "title": "DHSC presentation",
    "section": "Projects",
    "text": "Projects\n\nNew Hospital Programme\nOne project, lots of components"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#the-model",
    "href": "presentations/2024-12-02_dhsc/index.html#the-model",
    "title": "DHSC presentation",
    "section": "The model",
    "text": "The model\n\n\n\n\n\nflowchart LR\n  classDef orange fill:#f9bf07,stroke:#2c2825,color:#2c2825;\n  classDef lightslate fill:#b2b7b9,stroke:#2c2825,color:#2c2825;\n\n  A[Data Extraction]\n  B[Inputs App]\n  C[Model]\n  D[Outputs App]\n\n\n  SB[(input app data)]\n  SC[(model data)]\n  SD[(results data)]\n\n  A ---&gt; SB\n  A ---&gt; SC\n  \n  SB ---&gt; B\n  SC ---&gt; C\n\n  B ---&gt; C\n\n  C ---&gt; SD\n  SD ---&gt; D\n\n  B -.-&gt; D\n\n  class A,B,C,D orange\n  class SB,SC,SD lightslate"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#the-model-itself",
    "href": "presentations/2024-12-02_dhsc/index.html#the-model-itself",
    "title": "DHSC presentation",
    "section": "The model itself",
    "text": "The model itself\n\nProbabilistic inputs and outputs\nMonte carlo simulation\nAccounts for:\n\nDemographic growth\nNon demographic growth\n“Mitigation”"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#the-work",
    "href": "presentations/2024-12-02_dhsc/index.html#the-work",
    "title": "DHSC presentation",
    "section": "The work",
    "text": "The work\n\nRollout with NHP schemes\nUpdating the model and interface in place\nReporting on and analysing the model results and inputs"
  },
  {
    "objectID": "presentations/2024-12-02_dhsc/index.html#our-interests",
    "href": "presentations/2024-12-02_dhsc/index.html#our-interests",
    "title": "DHSC presentation",
    "section": "Our interests",
    "text": "Our interests\n\nProductionisation of models for healthcare\nData science approaches to evidence review\nData science approaches to text\nLLMs"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#section",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#section",
    "title": "Conference Check-in App",
    "section": "",
    "text": "digital.library.unt.edu/ark:/67531/metadc1039451/m1/1/\n\n\nClark, Junebug. [Registration Desk for the LPC Conference], photograph, 2016-03-17/2016-03-19; (https://digital.library.unt.edu/ark:/67531/metadc1039451/m1/1/: accessed October 16, 2023), University of North Texas Libraries, UNT Digital Library, https://digital.library.unt.edu; crediting UNT Libraries Special Collections."
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#qr-codes-are-great",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#qr-codes-are-great",
    "title": "Conference Check-in App",
    "section": "QR codes are great",
    "text": "QR codes are great"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#and-can-be-easily-generated-in-r",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#and-can-be-easily-generated-in-r",
    "title": "Conference Check-in App",
    "section": "and can be easily generated in R",
    "text": "and can be easily generated in R\ninstall.packages(\"qrcode\")\nlibrary(qrcode)\n\nqr_code(\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\")"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#why-not",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#why-not",
    "title": "Conference Check-in App",
    "section": "Why not?",
    "text": "Why not?\n\n{shiny} would be doing all the processing on the server side\nwe would need to read from a camera client side\nthen stream video to the server for {shiny} to detect and decode the QR codes"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work",
    "title": "Conference Check-in App",
    "section": "How does this work?",
    "text": "How does this work?\n\n\nFront-end\n\nuses the React JavaScript framework\n@yidel/react-qr-scanner\nApp scan’s a QR code, then sends this to our backend\nA window pops up to say who has checked in, or shows an error message"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work-1",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work-1",
    "title": "Conference Check-in App",
    "section": "How does this work?",
    "text": "How does this work?\nBack-end\nUses the {plumber} R package to build the API, with endpoints for\n\ngetting the list of all of the attendees for that day\nuploading a list of attendees in bulk\nadding an attendee individually\ngetting an attendee\nchecking the attendee in"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work-2",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#how-does-this-work-2",
    "title": "Conference Check-in App",
    "section": "How does this work?",
    "text": "How does this work?\nMore Back-end Stuff\n\nuses a simple SQLite DB that will be thrown away at the end of the conference\nwe send personalised emails using {blastula} to the attendees with their QR codes\nthe QR codes are just random ids (UUIDs) that identify each attendee\nuses websockets to update all of the clients when a user checks in (to update the list of attendees)"
  },
  {
    "objectID": "presentations/2023-10-17_conference-check-in-app/index.html#learning-different-tools-can-show-you-the-light",
    "href": "presentations/2023-10-17_conference-check-in-app/index.html#learning-different-tools-can-show-you-the-light",
    "title": "Conference Check-in App",
    "section": "Learning different tools can show you the light",
    "text": "Learning different tools can show you the light\n\nunsplash.com/photos/tMGMINwFOtI"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#intro",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#intro",
    "title": "A gentle introduction to databricks",
    "section": "Intro",
    "text": "Intro\n\nI insisted on having this slot in C&C\nI think some people want to know what a thing does; others want to know what it is\nThis is the is part of the session"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-is-spark",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-is-spark",
    "title": "A gentle introduction to databricks",
    "section": "What the heck is Spark?",
    "text": "What the heck is Spark?\n\nDatabricks is everything now, and confusingly so\nLet’s look at the story of databricks- which starts with Spark\nSpark was an attempt to improve on MapReduce (primarily Hadoop)"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-is-mapreduce",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-is-mapreduce",
    "title": "A gentle introduction to databricks",
    "section": "What the heck is MapReduce",
    "text": "What the heck is MapReduce\n\nMapReduce is a less analytically specific version of Split, apply, combine\nWhat the heck is Split, apply, combine? (last layer of the onion I promise!)\nHadley Wickham wrote about Split, apply, combine in the intro to {plyr}\n\n(For the young people plyr is what we had in the olden days before dplyr- which is Dataframe plyr- dplyr)"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-is-split-apply-combine",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-is-split-apply-combine",
    "title": "A gentle introduction to databricks",
    "section": "What the heck is Split, apply, combine?",
    "text": "What the heck is Split, apply, combine?\n\nVery often in an analysis you want to do the same thing to different groups\nSplit: divide a dataset up by age group\nApply: find the mean number of A&E attendances for 2023/4 (e.g.) for each group\nCombine: bring the results back together and put them in a table"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-was-i-talking-about-again",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-was-i-talking-about-again",
    "title": "A gentle introduction to databricks",
    "section": "What the heck was I talking about again?",
    "text": "What the heck was I talking about again?\n\nMapReduce is essentially an algorithm that relies on massive parallelisation to get jobs done quickly\nSpark was a proposed improvement"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#spark-hadoop",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#spark-hadoop",
    "title": "A gentle introduction to databricks",
    "section": "Spark > Hadoop",
    "text": "Spark &gt; Hadoop\n\nIn-memory processing- this is much faster, especially for certain data science applications\nMore tools and toys- APIs, built in modules for SQL, ML…\nFault tolerance- maintains all the fault tolerance of Hadoop, but works in-memory\nMuch greater flexibility on the way computation is done"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#the-advent-of-databricks",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#the-advent-of-databricks",
    "title": "A gentle introduction to databricks",
    "section": "The advent of databricks",
    "text": "The advent of databricks\n\nSpark was open sourced in 2010 and moved to Apache Foundation in 2013 (Apache Spark)\nDatabricks was set up as the commercial version of Apache Spark (databricks still contributes to the open source version)"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#commercial-spark",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#commercial-spark",
    "title": "A gentle introduction to databricks",
    "section": "Commercial spark",
    "text": "Commercial spark\n\nDatabricks does the enterprise-y stuff you’d expect (think Posit)\n\nProvides support to enterprises\nCurates, manages, and verifies the code in a commercial version of Spark\nProvide a platform to deploy and manage Spark, which is not simple"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#the-advent-of-delta-lake",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#the-advent-of-delta-lake",
    "title": "A gentle introduction to databricks",
    "section": "The advent of Delta Lake",
    "text": "The advent of Delta Lake\n\nThe other important thing to know about databricks is Delta Lake\nDelta lake is open source and was developed by databricks to improve on existing data lakes"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-is-a-data-lake",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-is-a-data-lake",
    "title": "A gentle introduction to databricks",
    "section": "What the heck is a data lake?",
    "text": "What the heck is a data lake?\n\nOkay, one more\nLike a data warehouse, but less structured\nWidely used in data science and analytics\n\nAs opposed to data warehouses which are more for BI\n\nNot either/ or- often orgs have both"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#what-does-delta-lake-bring",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#what-does-delta-lake-bring",
    "title": "A gentle introduction to databricks",
    "section": "What does Delta lake bring?",
    "text": "What does Delta lake bring?\n\nScalability (particularly around simultaneous processing)\nACID transactions\nWhat the heck is ACID? (Some will know- for those who don’t)"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-are-acid-transactions",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#what-the-heck-are-acid-transactions",
    "title": "A gentle introduction to databricks",
    "section": "What the heck are ACID transactions?",
    "text": "What the heck are ACID transactions?\n\nAtomicity - each statement in a transaction (to read, write, update or delete data) is treated as a single unit\nConsistency - ensures that transactions only make changes to tables in predefined, predictable ways\nIsolation - isolation of user transactions ensures that concurrent transactions don’t interfere with or affect one another\nDurability - ensures that changes to your data made by successfully executed transactions will be saved, even in the event of system failure\n\n(Source)"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#build-a-little-lakehouse-in-your-soul",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#build-a-little-lakehouse-in-your-soul",
    "title": "A gentle introduction to databricks",
    "section": "Build a little lakehouse in your soul",
    "text": "Build a little lakehouse in your soul\n\nDatabricks enables a “lakehouse”- warehouse and lake together\nLots of whizzy toys are available on databricks\n\nThere are so many now that it’s just confusing- “Generative AI”?\n“you can search and discover data by asking a question in your own words”\n\nEqually you can just write SQL against it"
  },
  {
    "objectID": "presentations/2025-01-16_c-and-c-databricks/index.html#why-might-we-as-an-su-want-databricks",
    "href": "presentations/2025-01-16_c-and-c-databricks/index.html#why-might-we-as-an-su-want-databricks",
    "title": "A gentle introduction to databricks",
    "section": "Why might we as an SU want databricks?",
    "text": "Why might we as an SU want databricks?\n\nWe started using it because it was fast\nWe can use databricks on UDAL\nIt provides a way that we can jointly organise and share data and data architecture in a RAP compliant way"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#the-team",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#the-team",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "The team",
    "text": "The team"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#a-hospital-is-a-place-where-you-can-find-people",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#a-hospital-is-a-place-where-you-can-find-people",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "A hospital is a place where you can find people…",
    "text": "A hospital is a place where you can find people…\n\nhaving the best day of their life,\nthe worst day of their life,\nthe first day of their life,\nand the last day of their life."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#planning-is-hard",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#planning-is-hard",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Planning is hard",
    "text": "Planning is hard\n\n\n\n\n\nbuilt with enough capacity to replace the existing school\nfailed to take into account a new housing estate\nlikely needs double the number of spaces within the next decade\n\nBBC article"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#review-of-existing-models",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#review-of-existing-models",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Review of existing models",
    "text": "Review of existing models\n\nSteven Wyatt - NHS-R 2022"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#review-of-existing-models-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#review-of-existing-models-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Review of existing models",
    "text": "Review of existing models\n\nlots of models\nlots of external consultancies\nlots of similarities\n\n\n\nlots of repetition/duplication\nsufficiently different that comparing results is difficult\nmethodological progress slow\nno base to build from\n\n\n\nconsultancies don’t tend to offer products, but services\ndifficult to compare different models to understand if differences are methodological or due to assumptions\nsame issues seen 20/30 years ago\nlearning and expertise gathered tends to be trapped within trusts, or kept secret by consultancies"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#common-issues",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#common-issues",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Common issues",
    "text": "Common issues\n\nhandling uncertainty\nunnecessary/early aggregation\npoor coverage of some changes\nlack of ownership & auditability of assumptions\nconflating demand forecasting with affordability\n\n\n\nmost models handle changes like demographic changes and the impact of changes in occupancy rates\nbut few try to handle addressing inequities, health status adjustment"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#our-model",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#our-model",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Our model",
    "text": "Our model\n\nopen source (not quite yet…)\nuses standard, well-known datasets (e.g. HES, ONS population projections)\ncurrently handles Inpatient admissions, Outpatient attendances, and A&E arrivals\nextensible and adaptable\ncovering all of the change factors\nstochastic Monte-Carlo model to handle uncertainty"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#project-structure",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#project-structure",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Project Structure",
    "text": "Project Structure\n\n\n\nData Extraction (R + {targets} & Sql)\nInputs App (R + {shiny})\nOutputs App (R + {shiny})\nModel Engine (Python & Docker)\nAzure Infrastructure (VM/ACR/ACI/Storage Accounts)\nAll of the code is stored on GitHub (currently, private repos 😔)\n\n\n\n\n\n\n\nflowchart TB\n  classDef orange fill:#f9bf07,stroke:#2c2825,color:#2c2825;\n  classDef lightslate fill:#b2b7b9,stroke:#2c2825,color:#2c2825;\n\n  A[Data Extraction]\n  B[Inputs App]\n  C[Model]\n  D[Outputs App]\n\n\n  SB[(input app data)]\n  SC[(model data)]\n  SD[(results data)]\n\n  A ---&gt; SB\n  A ---&gt; SC\n  \n  SB ---&gt; B\n  SC ---&gt; C\n\n  B ---&gt; C\n\n  C ---&gt; SD\n  SD ---&gt; D\n\n  B -.-&gt; D\n\n  class A,B,C,D orange\n  class SB,SC,SD lightslate"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-overview",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-overview",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Overview",
    "text": "Model Overview\n\nthe baseline data is a year worth of a provider’s HES data\neach row in the baseline data is run through a series of steps\neach step creates a factor that says how many times (on average) to sample that row\nthe factors are multiplied together and used to create a random Poisson value\nwe resample the rows using this random values\nefficiencies are then applied, e.g. LoS reductions, type conversions\n\n\n\nIP/OP/A&E data\ncomplex, but not complicated"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-diagram",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-diagram",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Diagram",
    "text": "Model Diagram\n\n\n\n\n\nflowchart TB\n    classDef blue fill:#5881c1,stroke:#2c2825,color:#2c2825;\n    classDef orange fill:#f9bf07,stroke:#2c2825,color:#2c2825;\n    classDef red fill:#ec6555,stroke:#2c2825,color:#2c2825;\n    classDef lightslate fill:#b2b7b9,stroke:#2c2825,color:#2c2825;\n    classDef slate fill:#e0e2e3,stroke:#2c2825,color:#2c2825;\n\n    S[Baseline Activity]\n    T[Future Activity]\n\n    class S,T red\n\n    subgraph rr[Row Resampling]\n        direction LR\n\n        subgraph pop[Population Changes]\n            direction TB\n            pop_p[Population Growth]\n            pop_a[Age/Sex Structure]\n            pop_h[Population Specific Health Status]\n\n            class pop_p,pop_a,pop_h orange\n\n            pop_p --- pop_a --- pop_h\n        end\n\n        subgraph dsi[Demand Supply Imbalances]\n            direction TB\n            dsi_w[Waiting List Adjustment]\n            dsi_r[Repatriation/Expatriation]\n            dsi_p[Private Healthcare Dynamics]\n\n            class dsi_w,dsi_r,dsi_p orange\n\n            dsi_w --- dsi_r --- dsi_p\n        end\n\n        subgraph nsi[Need Supply Imbalances]\n            direction TB\n            nsi_g[Gaps in Care]\n            nsi_i[Inequalities]\n            nsi_t[Threshold Imbalances]\n\n            class nsi_g,nsi_i,nsi_t orange\n\n            nsi_g --- nsi_i --- nsi_t\n        end\n\n        subgraph nda [Non-Demographic Adjustment]\n            direction TB\n            nda_m[Medical Interventions]\n            nda_c[Changes to National Standards]\n            nda_p[Patient Expectations]\n\n            class nda_m,nda_c,nda_p orange\n\n            nda_m --- nda_c --- nda_p\n        end\n\n        subgraph mit[Activity Mitigators]\n            direction TB\n            mit_a[Activity Avoidance]\n            mit_t[Type Conversion]\n            mit_e[Efficiencies]\n            \n            class mit_a,mit_t,mit_e orange\n\n            mit_a --- mit_t --- mit_e\n        end\n\n        pop --- dsi --- nsi --- nda --- mit\n\n        class dsi,nsi,pop,nda,mit lightslate\n    end\n\n    class rr slate\n    \n    S --&gt; rr --&gt; T\n\n\n\n\n\n\n\n\nuses either patient-level data, or minimal aggregation\nrow resampling grouped into 5 broad groups\n\npopulation changes address the changes to the structure of the population and health status over the medium term\ndemand supply imbalances: hospitals are currently struggling to keep pace with demand, so we correct for this to not carry forwards these into the future\nneed supply imbalance: addressing gaps in care that currently exist\nnon-demographic: such as the development of new medical technologies\nactivity mitigators: strategies trusts adopt for reducing activity, or delivering activity more efficiently\n\nsome assumptions set nationally, such as population growth via ONS population projections\nother assumptions set locally, with support from a Shiny app"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-diagram-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-diagram-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Diagram",
    "text": "Model Diagram\n\n\n\n\n\nflowchart TB\n    classDef blue fill:#5881c1,stroke:#2c2825,color:#2c2825;\n    classDef orange fill:#f9bf07,stroke:#2c2825,color:#2c2825;\n    classDef red fill:#ec6555,stroke:#2c2825,color:#2c2825;\n    classDef lightslate fill:#b2b7b9,stroke:#2c2825,color:#2c2825;\n    classDef slate fill:#e0e2e3,stroke:#2c2825,color:#2c2825;\n\n    S[Baseline Activity]\n    T[Future Activity]\n\n    ORANGE[Implemented]\n    BLUE[Not yet implemented]\n\n    class ORANGE orange\n    class BLUE blue\n\n    class S,T red\n\n    subgraph rr[Row Resampling]\n        direction LR\n\n        subgraph pop[Population Changes]\n            direction TB\n            pop_p[Population Growth]\n            pop_a[Age/Sex Structure]\n            pop_h[Population Specific Health Status]\n\n            class pop_p,pop_a,pop_h orange\n\n            pop_p --- pop_a --- pop_h\n        end\n\n        subgraph dsi[Demand Supply Imbalances]\n            direction TB\n            dsi_w[Waiting List Adjustment]\n            dsi_r[Repatriation/Expatriation]\n            dsi_p[Private Healthcare Dynamics]\n\n            class dsi_w,dsi_r orange\n            class dsi_p blue\n\n            dsi_w --- dsi_r --- dsi_p\n        end\n\n        subgraph nsi[Need Supply Imbalances]\n            direction TB\n            nsi_g[Gaps in Care]\n            nsi_i[Inequalities]\n            nsi_t[Threshold Imbalances]\n\n            class nsi_g,nsi_i,nsi_t blue\n\n            nsi_g --- nsi_i --- nsi_t\n        end\n\n        subgraph nda [Non-Demographic Adjustment]\n            direction TB\n            nda_m[Medical Interventions]\n            nda_c[Changes to National Standards]\n            nda_p[Patient Expectations]\n\n            class nda_m,nda_c,nda_p blue\n\n            nda_m --- nda_c --- nda_p\n        end\n\n        subgraph mit[Activity Mitigators]\n            direction TB\n            mit_a[Activity Avoidance]\n            mit_t[Type Conversion]\n            mit_e[Efficiencies]\n            \n            class mit_a,mit_t,mit_e orange\n\n            mit_a --- mit_t --- mit_e\n        end\n\n        pop --- dsi --- nsi --- nda --- mit\n\n        class dsi,nsi,pop,nda,mit lightslate\n    end\n\n    class rr slate\n    \n    S --&gt; rr --&gt; T"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#monte-carlo-simulation",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#monte-carlo-simulation",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Monte Carlo Simulation",
    "text": "Monte Carlo Simulation\n\n\n\nWe run the model N times, varying the input parameters each time slightly to handle the uncertainty.\nThe results of the model are aggregated at the end of each model run\nThe aggregated results are combined at the end into a single file\n\n\n\n\n\n\n\nflowchart LR\n  classDef orange fill:#f9bf07,stroke:#2c2825,color:#2c2825;\n  classDef red fill:#ec6555,stroke:#2c2825,color:#2c2825;\n  \n  A[Baseline Activity]\n  Ba[Model Run 0]\n  Bb[Model Run 1]\n  Bc[Model Run 2]\n  Bd[Model Run 3]\n  Bn[Model Run n]\n  C[Results]\n\n  A ---&gt; Ba ---&gt; C\n  A ---&gt; Bb ---&gt; C\n  A ---&gt; Bc ---&gt; C\n  A ---&gt; Bd ---&gt; C\n  A ---&gt; Bn ---&gt; C\n  \n  class A,C red\n  class Ba,Bb,Bc,Bd,Bn orange\n  \n\n\n\n\n\n\n\nInspired by\n\nMapReduce (Google, 2004)\nSplit, Apply, Combine (H. Wickham, 2011)"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Parameters",
    "text": "Model Parameters\n\nWe ask users to provide parameters in the form of 90% confidence intervals\nWe can then convert these confidence intervals into distributions\nDuring the model we sample values from these distributions for each model parameter\nAll of the parameters represent the average rate to sample a row of data from the baseline"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Parameters",
    "text": "Model Parameters\n\n“We expect in the future to see between a 25% reduction and a 25% increase in this activity”\n\n\n\n\ngrey highlighted section: 90% confidence intervals\nblack line: confidence intervals into distributions\nyellow points: sampled parameter for a model run"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-2",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-2",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Parameters",
    "text": "Model Parameters\n\n“We expect in the future to see between a 20% reduction and a 90% reduction in this activity”\n\n\n\n\ngrey highlighted section: 90% confidence intervals\nblack line: confidence intervals into distributions\nyellow points: sampled parameter for a model run"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-3",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-parameters-3",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Parameters",
    "text": "Model Parameters\n\n“We expect in the future to see between a 2% reduction and an 18% reduction in this activity”\n\n\n\n\ngrey highlighted section: 90% confidence intervals\nblack line: confidence intervals into distributions\nyellow points: sampled parameter for a model run"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Run Example (1)",
    "text": "Model Run Example (1)\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\nf\n\n\n\n\n1\n50\nm\n100\n4\n1.00\n\n\n2\n50\nm\n110\n3\n1.00\n\n\n3\n51\nm\n120\n5\n1.00\n\n\n4\n50\nf\n100\n1\n1.00\n\n\n5\n50\nf\n110\n2\n1.00\n\n\n6\n52\nf\n120\n0\n1.00\n\n\n\n\n\n\n\n\n\nStart with baseline data - we are going to sample each row exactly once (column f)."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-2",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-2",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Run Example (2)",
    "text": "Model Run Example (2)\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\nf\n\n\n\n\n1\n50\nm\n100\n4\n1.00\n\n\n2\n50\nm\n110\n3\n1.00\n\n\n3\n51\nm\n120\n5\n1.00\n\n\n4\n50\nf\n100\n1\n1.00\n\n\n5\n50\nf\n110\n2\n1.00\n\n\n6\n52\nf\n120\n0\n1.00\n\n\n\n\n\n\n\nage\nsex\nf\n\n\n\n\n50\nm\n0.90\n\n\n51\nm\n1.10\n\n\n52\nm\n1.20\n\n\n50\nf\n0.80\n\n\n51\nf\n0.70\n\n\n52\nf\n1.30\n\n\n\n\n\n\n\nf\n\n\n\n\n1.00 × 0.90 = 0.90\n\n\n1.00 × 0.90 = 0.90\n\n\n1.00 × 1.10 = 1.10\n\n\n1.00 × 0.80 = 0.80\n\n\n1.00 × 0.80 = 0.80\n\n\n1.00 × 1.30 = 1.30\n\n\n\n\n\nWe perform a step where we join based on age and sex, then update the f column."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-3",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-3",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Run Example (3)",
    "text": "Model Run Example (3)\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\nf\n\n\n\n\n1\n50\nm\n100\n4\n0.90\n\n\n2\n50\nm\n110\n3\n0.90\n\n\n3\n51\nm\n120\n5\n1.10\n\n\n4\n50\nf\n100\n1\n0.80\n\n\n5\n50\nf\n110\n2\n0.80\n\n\n6\n52\nf\n120\n0\n1.30\n\n\n\n\n\n\n\nspecialty\nf\n\n\n\n\n100\n0.90\n\n\n110\n1.10\n\n\n\n\n\n\n\nf\n\n\n\n\n0.90 × 0.90 = 0.81\n\n\n0.90 × 1.10 = 0.99\n\n\n1.10 × 1.00 = 1.10\n\n\n0.80 × 0.90 = 0.72\n\n\n0.80 × 1.10 = 0.88\n\n\n1.30 × 1.00 = 1.30\n\n\n\n\n\nThe next step joins on the specialty column, again updating f. Note, if there is no value to join on, then we multiply by 1."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-4",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-4",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Run Example (4)",
    "text": "Model Run Example (4)\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\nf\nn\n\n\n\n\n1\n50\nm\n100\n4\n0.90\n1\n\n\n2\n50\nm\n110\n3\n0.90\n0\n\n\n3\n51\nm\n120\n5\n1.10\n2\n\n\n4\n50\nf\n100\n1\n0.80\n1\n\n\n5\n50\nf\n110\n2\n0.80\n0\n\n\n6\n52\nf\n120\n0\n1.30\n3\n\n\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\n\n\n\n\n1\n50\nm\n100\n4\n\n\n3\n51\nm\n120\n5\n\n\n3\n51\nm\n120\n5\n\n\n4\n50\nf\n100\n1\n\n\n6\n52\nf\n120\n0\n\n\n6\n52\nf\n120\n0\n\n\n6\n52\nf\n120\n0\n\n\n\n\n\nOnce all of the steps are performed, sample a random value n from a Poisson distribution with λ=f, then we select each row n times."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-5",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#model-run-example-5",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Model Run Example (5)",
    "text": "Model Run Example (5)\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\ng\n\n\n\n\n1\n50\nm\n100\n4\n0.75\n\n\n3\n51\nm\n120\n5\n0.50\n\n\n3\n51\nm\n120\n5\n1.00\n\n\n4\n50\nf\n100\n1\n0.90\n\n\n6\n52\nf\n120\n0\n0.80\n\n\n6\n52\nf\n120\n0\n0.80\n\n\n6\n52\nf\n120\n0\n0.80\n\n\n\n\n\n\n\nid\nage\nsex\nspecialty\nlos\n\n\n\n\n1\n50\nm\n100\n2\n\n\n3\n51\nm\n120\n1\n\n\n3\n51\nm\n120\n5\n\n\n4\n50\nf\n100\n0\n\n\n6\n52\nf\n120\n0\n\n\n6\n52\nf\n120\n0\n\n\n6\n52\nf\n120\n0\n\n\n\n\n\nAfter resampling, we apply efficiency steps. E.g., similar joins are used to create column g, which is then used to sample a new LOS from a binomial distribution."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-built",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-built",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "How the model is built",
    "text": "How the model is built\n\nThe model is built in Python and can be run on any machine you can install Python on\nUses various packages, such as numpy and pandas\nReads data in .parquet format for efficiency\nReturns aggregated results as a .json file\nCould also output full row level results if needed"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-built-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-built-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "How the model is built",
    "text": "How the model is built\n\nCode is built in a modular approach\nEach activity type (Inpatients/Outpatients/A&E) has its own model code\nCode is reused where possible (e.g. all three models share the code for demographic adjustment)"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-deployed",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#how-the-model-is-deployed",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "How the model is deployed",
    "text": "How the model is deployed\n\nDeployed as a Docker Container\nRuns in Azure Container Instances\nEach model run creates a new container, and the container is destroyed when the model run completes"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#data-extraction",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#data-extraction",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Data Extraction",
    "text": "Data Extraction\n\nUses principles of RAP, using R + {targets} and Sql\nAll of the data required to run the model\nData is extracted from various sources\n\nSql Datawarehouse (HES data)\nONS population projections + life expectancy tables\nCentral returns, e.g. KH03\nODS data (organisation names, successors)\n\nExtracted data is uploaded to Azure storage containers"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#inputs-app",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#inputs-app",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Inputs App",
    "text": "Inputs App\nA {shiny} app that allows the user to set parameters, and submit as a job to run the model with those values."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#inputs-app-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#inputs-app-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Inputs App",
    "text": "Inputs App"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#outputs-app",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#outputs-app",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Outputs App",
    "text": "Outputs App\nA {shiny} app that allows the user to view the results of model runs."
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#outputs-app-1",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#outputs-app-1",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Outputs App",
    "text": "Outputs App"
  },
  {
    "objectID": "presentations/2023-07-11_haca-nhp-demand-model/index.html#questions",
    "href": "presentations/2023-07-11_haca-nhp-demand-model/index.html#questions",
    "title": "An Introduction to the New Hospital Programme Demand Model",
    "section": "Questions?",
    "text": "Questions?\n\nContact The Strategy Unit\n\n\n strategy.unit@nhs.net\n The-Strategy-Unit\n\n\nContact Me\n\n\n thomas.jemmett@nhs.net\n tomjemmett"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#what-is-data-science",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#what-is-data-science",
    "title": "Everything you ever wanted to know about data science",
    "section": "What is data science?",
    "text": "What is data science?\n\n“A data scientist knows more about computer science than the average statistician, and more about statistics than the average computer scientist”"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#drew-conways-famous-venn-diagram",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#drew-conways-famous-venn-diagram",
    "title": "Everything you ever wanted to know about data science",
    "section": "Drew Conway’s famous Venn diagram",
    "text": "Drew Conway’s famous Venn diagram\n\nSource"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#around-the-web",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#around-the-web",
    "title": "Everything you ever wanted to know about data science",
    "section": "Around the web…",
    "text": "Around the web…\n\n\n\nThe difference between a statitician and a data scientist? About $30,000\n… an actual definition of data science. Taking a database and making it do something else. (warning: this quote is me! :wink:)\nStatistics done on a Mac"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#what-are-the-skills-of-data-science",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#what-are-the-skills-of-data-science",
    "title": "Everything you ever wanted to know about data science",
    "section": "What are the skills of data science?",
    "text": "What are the skills of data science?\n\nAnalysis\n\nML\nStats\nData viz\n\nSoftware engineering\n\nProgramming\nSQL/ data\nDevOps\nRAP"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#what-are-the-skills-of-data-science-1",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#what-are-the-skills-of-data-science-1",
    "title": "Everything you ever wanted to know about data science",
    "section": "What are the skills of data science?",
    "text": "What are the skills of data science?\n\nDomain knowledge\n\nCommunication\nProblem formulation\nDashboards and reports"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#ml",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#ml",
    "title": "Everything you ever wanted to know about data science",
    "section": "ML",
    "text": "ML\n\nSource"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#inevitable-xkcd",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#inevitable-xkcd",
    "title": "Everything you ever wanted to know about data science",
    "section": "Inevitable XKCD",
    "text": "Inevitable XKCD\n\n\n\nSource\n\n\nGoogle flu trends"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#stats-and-data-viz",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#stats-and-data-viz",
    "title": "Everything you ever wanted to know about data science",
    "section": "Stats and data viz",
    "text": "Stats and data viz\n\nML leans a bit more towards atheoretical prediction\nStats leans a bit more towards inference (but they both do both)\nData scientists may use different visualisations\n\nInteractive web based tools\nDashboard based visualisers e.g. {stminsights}"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#software-engineering",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#software-engineering",
    "title": "Everything you ever wanted to know about data science",
    "section": "Software engineering",
    "text": "Software engineering\n\nProgramming\n\nNo/ low code data science?\n\nSQL/ data\n\nTend to use reproducible automated processes\n\nDevOps\n\nPlan, code, build, test, release, deploy, operate, monitor\n\nRAP\n\nI will come back to this"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#domain-knowledge",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#domain-knowledge",
    "title": "Everything you ever wanted to know about data science",
    "section": "Domain knowledge",
    "text": "Domain knowledge\n\nDo stuff that matters\n\nThe best minds of my generation are thinking about how to make people click ads. That sucks. Jeffrey Hammerbacher\n\nConvince other people that it matters\nThis is the hardest part of data science\nCommunicate, communicate, communicate!\nMany of you are expert at this"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#reproducibility",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#reproducibility",
    "title": "Everything you ever wanted to know about data science",
    "section": "Reproducibility",
    "text": "Reproducibility\n\nReproducibility in science\nThe $6B spreadsheet error\nGeorge Osbourne’s austerity was based on a spreadsheet error\nFor us, reproducibility also means we can do the same analysis 50 times in one minute\n\nWhich is why I started down the road of data science"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#what-is-rap",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#what-is-rap",
    "title": "Everything you ever wanted to know about data science",
    "section": "What is RAP",
    "text": "What is RAP\n\na process in which code is used to minimise manual, undocumented steps, and a clear, properly documented process is produced in code which can reliably give the same result from the same dataset\nRAP should be:\n\n\nthe core working practice that must be supported by all platforms and teams; make this a core focus of NHS analyst training\n\n\nGoldacre review"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--baseline",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--baseline",
    "title": "Everything you ever wanted to know about data science",
    "section": "Levels of RAP- Baseline",
    "text": "Levels of RAP- Baseline\n\nData produced by code in an open-source language (e.g., Python, R, SQL).\nCode is version controlled (see Git basics and using Git collaboratively guides).\nRepository includes a README.md file (or equivalent) that clearly details steps a user must follow to reproduce the code\nCode has been peer reviewed.\nCode is published in the open and linked to & from accompanying publication (if relevant).\n\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--silver",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--silver",
    "title": "Everything you ever wanted to know about data science",
    "section": "Levels of RAP- Silver",
    "text": "Levels of RAP- Silver\n\nCode is well-documented…\nCode is well-organised following standard directory format\nReusable functions and/or classes are used where appropriate\nPipeline includes a testing framework\nRepository includes dependency information (e.g. requirements.txt, PipFile, environment.yml\nData is handled and output in a Tidy data format\n\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--gold",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#levels-of-rap--gold",
    "title": "Everything you ever wanted to know about data science",
    "section": "Levels of RAP- Gold",
    "text": "Levels of RAP- Gold\n\nCode is fully packaged\nRepository automatically runs tests etc. via CI/CD or a different integration/deployment tool e.g. GitHub Actions\nProcess runs based on event-based triggers (e.g., new data in database) or on a schedule\nChanges to the RAP are clearly signposted. E.g. a changelog in the package, releases etc. (See gov.uk info on Semantic Versioning)\n\nSource: NHS Digital RAP community of practice"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#the-data-science-unicorn",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#the-data-science-unicorn",
    "title": "Everything you ever wanted to know about data science",
    "section": "The data science “Unicorn”",
    "text": "The data science “Unicorn”\n\nThe maybe-mythical data science “Unicorn” has mastered:\n\nDomain knowledge\nStats and ML\nSoftware engineering"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-a-team-sport",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-a-team-sport",
    "title": "Everything you ever wanted to know about data science",
    "section": "Data science is a team sport",
    "text": "Data science is a team sport\n\nIn my extended DS team I have:\nStats and DevOps (and rabble rousing) [this one is me :wink:]\nSQL, data, and training\nDevOps and programming\nText mining, Python, and APIs\nBilingual R/ Python, Shiny dashboards"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-an-mmo",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-an-mmo",
    "title": "Everything you ever wanted to know about data science",
    "section": "Data science is an MMO",
    "text": "Data science is an MMO\n\nData scientists need help with:\n\nStakeholder communication and engagement\nQualitative analysis\nTranslating models and prediction into the real world\nEvidence review and problem definition"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-an-mmo-1",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#data-science-is-an-mmo-1",
    "title": "Everything you ever wanted to know about data science",
    "section": "Data science is an MMO",
    "text": "Data science is an MMO\n\nData scientists are an excellent help when you:\n\nNeed a lot of pretty graphs\nNeed the same analysis done 50+ times with different data\nHave too much text and not enough time to analyse it\nWant to carefully document your analysis and make it reproducible\nHave a hideously messy, large dataset that you can’t hack together yourself"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#the-team",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#the-team",
    "title": "Everything you ever wanted to know about data science",
    "section": "The team",
    "text": "The team\n\nWe will be organising code review and pair coding sessions\nWe will be running coffee and coding sessions\nWe can be relied on to get very excited about thorny data problems, especially if they involve:\n\nDrawing pretty graphs\nNHS-R and other communities and events\nSpending long hours in a bunker writing open source code\nProcessing text\nDocumenting and version controlling analyses"
  },
  {
    "objectID": "presentations/2023-02-01_what-is-data-science/index.html#note",
    "href": "presentations/2023-02-01_what-is-data-science/index.html#note",
    "title": "Everything you ever wanted to know about data science",
    "section": "Note",
    "text": "Note\nAll copyrighted material is reused under Fair Dealing"
  }
]